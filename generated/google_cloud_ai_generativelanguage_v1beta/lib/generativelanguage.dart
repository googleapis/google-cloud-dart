// Copyright 2025 Google LLC
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     https://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
// Code generated by sidekick. DO NOT EDIT.

/// The Google Cloud client for the Generative Language API.
///
/// The Gemini API allows developers to build generative AI applications using
/// Gemini models. Gemini is our most capable model, built from the ground up
/// to be multimodal. It can generalize and seamlessly understand, operate
/// across, and combine different types of information including language,
/// images, audio, video, and code. You can use the Gemini API for use cases
/// like reasoning across text and images, content generation, dialogue
/// agents, summarization and classification systems, and more.
library;

// ignore_for_file: avoid_unused_constructor_parameters
// ignore_for_file: camel_case_types
// ignore_for_file: comment_references
// ignore_for_file: implementation_imports
// ignore_for_file: lines_longer_than_80_chars
// ignore_for_file: unintended_html_in_doc_comment

import 'package:google_cloud_longrunning/longrunning.dart';
import 'package:google_cloud_protobuf/protobuf.dart';
import 'package:google_cloud_protobuf/src/encoding.dart';
import 'package:google_cloud_rpc/rpc.dart';
import 'package:google_cloud_rpc/service_client.dart';
import 'package:google_cloud_type/type.dart';
import 'package:http/http.dart' as http;

const _apiKeys = ['GOOGLE_API_KEY', 'GEMINI_API_KEY'];

/// API for managing cache of content (CachedContent resources) that can be used
/// in GenerativeService requests. This way generate content requests can benefit
/// from preprocessing work being done earlier, possibly lowering their
/// computational cost. It is intended to be used with large contexts.
final class CacheService {
  static const _host = 'generativelanguage.googleapis.com';

  final ServiceClient _client;

  /// Creates a `CacheService` using [client] for transport.
  ///
  /// The provided [http.Client] must be configured to provide whatever
  /// authentication is required by `CacheService`. You can do that using
  /// [`package:googleapis_auth`](https://pub.dev/packages/googleapis_auth).
  CacheService({required http.Client client})
    : _client = ServiceClient(client: client);

  /// Creates a `CacheService` that does authentication through an API key.
  ///
  /// If called without arguments, the API key is taken from these environment
  /// variables:
  ///
  /// - `GOOGLE_API_KEY`
  /// - `GEMINI_API_KEY`
  ///
  /// Throws [ConfigurationException] if called without arguments and none of
  /// the above environment variables are set. On the web,
  /// always throws [ConfigurationException] if called without arguments.
  ///
  /// See [API Keys Overview](https://cloud.google.com/api-keys/docs/overview).
  factory CacheService.fromApiKey([String? apiKey]) =>
      CacheService(client: httpClientFromApiKey(apiKey, _apiKeys));

  /// Lists CachedContents.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<ListCachedContentsResponse> listCachedContents(
    ListCachedContentsRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/cachedContents', {
      if (request.pageSize case final $1 when $1.isNotDefault)
        'pageSize': '${$1}',
      if (request.pageToken case final $1 when $1.isNotDefault) 'pageToken': $1,
    });
    final response = await _client.get(url);
    return ListCachedContentsResponse.fromJson(response);
  }

  /// Creates CachedContent resource.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<CachedContent> createCachedContent(
    CreateCachedContentRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/cachedContents');
    final response = await _client.post(url, body: request.cachedContent);
    return CachedContent.fromJson(response);
  }

  /// Reads CachedContent resource.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<CachedContent> getCachedContent(
    GetCachedContentRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    final response = await _client.get(url);
    return CachedContent.fromJson(response);
  }

  /// Updates CachedContent resource (only expiration is updatable).
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<CachedContent> updateCachedContent(
    UpdateCachedContentRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.cachedContent!.name}', {
      if (request.updateMask case final $1?) 'updateMask': $1.toJson(),
    });
    final response = await _client.patch(url, body: request.cachedContent);
    return CachedContent.fromJson(response);
  }

  /// Deletes CachedContent resource.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> deleteCachedContent(DeleteCachedContentRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    await _client.delete(url);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<ListOperationsResponse> listOperations(
    ListOperationsRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.name}/operations', {
      if (request.filter case final $1 when $1.isNotDefault) 'filter': $1,
      if (request.pageSize case final $1 when $1.isNotDefault)
        'pageSize': '${$1}',
      if (request.pageToken case final $1 when $1.isNotDefault) 'pageToken': $1,
      if (request.returnPartialSuccess case final $1 when $1.isNotDefault)
        'returnPartialSuccess': '${$1}',
    });
    final response = await _client.get(url);
    return ListOperationsResponse.fromJson(response);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  ///
  /// This method can be used to get the current status of a long-running
  /// operation.
  Future<Operation<T, S>> getOperation<
    T extends ProtoMessage,
    S extends ProtoMessage
  >(Operation<T, S> request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    final response = await _client.get(url);
    return Operation.fromJson(response, request.operationHelper);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> deleteOperation(DeleteOperationRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    await _client.delete(url);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> cancelOperation(CancelOperationRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}:cancel');
    await _client.post(url);
  }

  /// Closes the client and cleans up any resources associated with it.
  ///
  /// Once [close] is called, no other methods should be called.
  void close() => _client.close();
}

/// An API for using Generative Language Models (GLMs) in dialog applications.
///
/// Also known as large language models (LLMs), this API provides models that
/// are trained for multi-turn dialog.
final class DiscussService {
  static const _host = 'generativelanguage.googleapis.com';

  final ServiceClient _client;

  /// Creates a `DiscussService` using [client] for transport.
  ///
  /// The provided [http.Client] must be configured to provide whatever
  /// authentication is required by `DiscussService`. You can do that using
  /// [`package:googleapis_auth`](https://pub.dev/packages/googleapis_auth).
  DiscussService({required http.Client client})
    : _client = ServiceClient(client: client);

  /// Creates a `DiscussService` that does authentication through an API key.
  ///
  /// If called without arguments, the API key is taken from these environment
  /// variables:
  ///
  /// - `GOOGLE_API_KEY`
  /// - `GEMINI_API_KEY`
  ///
  /// Throws [ConfigurationException] if called without arguments and none of
  /// the above environment variables are set. On the web,
  /// always throws [ConfigurationException] if called without arguments.
  ///
  /// See [API Keys Overview](https://cloud.google.com/api-keys/docs/overview).
  factory DiscussService.fromApiKey([String? apiKey]) =>
      DiscussService(client: httpClientFromApiKey(apiKey, _apiKeys));

  /// Generates a response from the model given an input `MessagePrompt`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<GenerateMessageResponse> generateMessage(
    GenerateMessageRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.model}:generateMessage');
    final response = await _client.post(url, body: request);
    return GenerateMessageResponse.fromJson(response);
  }

  /// Runs a model's tokenizer on a string and returns the token count.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<CountMessageTokensResponse> countMessageTokens(
    CountMessageTokensRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.model}:countMessageTokens');
    final response = await _client.post(url, body: request);
    return CountMessageTokensResponse.fromJson(response);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<ListOperationsResponse> listOperations(
    ListOperationsRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.name}/operations', {
      if (request.filter case final $1 when $1.isNotDefault) 'filter': $1,
      if (request.pageSize case final $1 when $1.isNotDefault)
        'pageSize': '${$1}',
      if (request.pageToken case final $1 when $1.isNotDefault) 'pageToken': $1,
      if (request.returnPartialSuccess case final $1 when $1.isNotDefault)
        'returnPartialSuccess': '${$1}',
    });
    final response = await _client.get(url);
    return ListOperationsResponse.fromJson(response);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  ///
  /// This method can be used to get the current status of a long-running
  /// operation.
  Future<Operation<T, S>> getOperation<
    T extends ProtoMessage,
    S extends ProtoMessage
  >(Operation<T, S> request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    final response = await _client.get(url);
    return Operation.fromJson(response, request.operationHelper);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> deleteOperation(DeleteOperationRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    await _client.delete(url);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> cancelOperation(CancelOperationRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}:cancel');
    await _client.post(url);
  }

  /// Closes the client and cleans up any resources associated with it.
  ///
  /// Once [close] is called, no other methods should be called.
  void close() => _client.close();
}

/// An API for uploading and managing files.
final class FileService {
  static const _host = 'generativelanguage.googleapis.com';

  final ServiceClient _client;

  /// Creates a `FileService` using [client] for transport.
  ///
  /// The provided [http.Client] must be configured to provide whatever
  /// authentication is required by `FileService`. You can do that using
  /// [`package:googleapis_auth`](https://pub.dev/packages/googleapis_auth).
  FileService({required http.Client client})
    : _client = ServiceClient(client: client);

  /// Creates a `FileService` that does authentication through an API key.
  ///
  /// If called without arguments, the API key is taken from these environment
  /// variables:
  ///
  /// - `GOOGLE_API_KEY`
  /// - `GEMINI_API_KEY`
  ///
  /// Throws [ConfigurationException] if called without arguments and none of
  /// the above environment variables are set. On the web,
  /// always throws [ConfigurationException] if called without arguments.
  ///
  /// See [API Keys Overview](https://cloud.google.com/api-keys/docs/overview).
  factory FileService.fromApiKey([String? apiKey]) =>
      FileService(client: httpClientFromApiKey(apiKey, _apiKeys));

  /// Creates a `File`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<CreateFileResponse> createFile(CreateFileRequest request) async {
    final url = Uri.https(_host, '/v1beta/files');
    final response = await _client.post(url, body: request);
    return CreateFileResponse.fromJson(response);
  }

  /// Lists the metadata for `File`s owned by the requesting project.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<ListFilesResponse> listFiles(ListFilesRequest request) async {
    final url = Uri.https(_host, '/v1beta/files', {
      if (request.pageSize case final $1 when $1.isNotDefault)
        'pageSize': '${$1}',
      if (request.pageToken case final $1 when $1.isNotDefault) 'pageToken': $1,
    });
    final response = await _client.get(url);
    return ListFilesResponse.fromJson(response);
  }

  /// Gets the metadata for the given `File`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<File> getFile(GetFileRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    final response = await _client.get(url);
    return File.fromJson(response);
  }

  /// Deletes the `File`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> deleteFile(DeleteFileRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    await _client.delete(url);
  }

  /// Download the `File`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<DownloadFileResponse> downloadFile(DownloadFileRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}:download');
    final response = await _client.get(url);
    return DownloadFileResponse.fromJson(response);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<ListOperationsResponse> listOperations(
    ListOperationsRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.name}/operations', {
      if (request.filter case final $1 when $1.isNotDefault) 'filter': $1,
      if (request.pageSize case final $1 when $1.isNotDefault)
        'pageSize': '${$1}',
      if (request.pageToken case final $1 when $1.isNotDefault) 'pageToken': $1,
      if (request.returnPartialSuccess case final $1 when $1.isNotDefault)
        'returnPartialSuccess': '${$1}',
    });
    final response = await _client.get(url);
    return ListOperationsResponse.fromJson(response);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  ///
  /// This method can be used to get the current status of a long-running
  /// operation.
  Future<Operation<T, S>> getOperation<
    T extends ProtoMessage,
    S extends ProtoMessage
  >(Operation<T, S> request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    final response = await _client.get(url);
    return Operation.fromJson(response, request.operationHelper);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> deleteOperation(DeleteOperationRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    await _client.delete(url);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> cancelOperation(CancelOperationRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}:cancel');
    await _client.post(url);
  }

  /// Closes the client and cleans up any resources associated with it.
  ///
  /// Once [close] is called, no other methods should be called.
  void close() => _client.close();
}

/// API for using Large Models that generate multimodal content and have
/// additional capabilities beyond text generation.
final class GenerativeService {
  static const _host = 'generativelanguage.googleapis.com';

  final ServiceClient _client;

  /// Creates a `GenerativeService` using [client] for transport.
  ///
  /// The provided [http.Client] must be configured to provide whatever
  /// authentication is required by `GenerativeService`. You can do that using
  /// [`package:googleapis_auth`](https://pub.dev/packages/googleapis_auth).
  GenerativeService({required http.Client client})
    : _client = ServiceClient(client: client);

  /// Creates a `GenerativeService` that does authentication through an API key.
  ///
  /// If called without arguments, the API key is taken from these environment
  /// variables:
  ///
  /// - `GOOGLE_API_KEY`
  /// - `GEMINI_API_KEY`
  ///
  /// Throws [ConfigurationException] if called without arguments and none of
  /// the above environment variables are set. On the web,
  /// always throws [ConfigurationException] if called without arguments.
  ///
  /// See [API Keys Overview](https://cloud.google.com/api-keys/docs/overview).
  factory GenerativeService.fromApiKey([String? apiKey]) =>
      GenerativeService(client: httpClientFromApiKey(apiKey, _apiKeys));

  /// Generates a model response given an input `GenerateContentRequest`.
  /// Refer to the [text generation
  /// guide](https://ai.google.dev/gemini-api/docs/text-generation) for detailed
  /// usage information. Input capabilities differ between models, including
  /// tuned models. Refer to the [model
  /// guide](https://ai.google.dev/gemini-api/docs/models/gemini) and [tuning
  /// guide](https://ai.google.dev/gemini-api/docs/model-tuning) for details.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<GenerateContentResponse> generateContent(
    GenerateContentRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.model}:generateContent');
    final response = await _client.post(url, body: request);
    return GenerateContentResponse.fromJson(response);
  }

  /// Generates a grounded answer from the model given an input
  /// `GenerateAnswerRequest`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<GenerateAnswerResponse> generateAnswer(
    GenerateAnswerRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.model}:generateAnswer');
    final response = await _client.post(url, body: request);
    return GenerateAnswerResponse.fromJson(response);
  }

  /// Generates a [streamed
  /// response](https://ai.google.dev/gemini-api/docs/text-generation?lang=python#generate-a-text-stream)
  /// from the model given an input `GenerateContentRequest`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Stream<GenerateContentResponse> streamGenerateContent(
    GenerateContentRequest request,
  ) {
    final url = Uri.https(
      _host,
      '/v1beta/${request.model}:streamGenerateContent',
    );
    return _client
        .postStreaming(url, body: request)
        .map(GenerateContentResponse.fromJson);
  }

  /// Generates a text embedding vector from the input `Content` using the
  /// specified [Gemini Embedding
  /// model](https://ai.google.dev/gemini-api/docs/models/gemini#text-embedding).
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<EmbedContentResponse> embedContent(EmbedContentRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.model}:embedContent');
    final response = await _client.post(url, body: request);
    return EmbedContentResponse.fromJson(response);
  }

  /// Generates multiple embedding vectors from the input `Content` which
  /// consists of a batch of strings represented as `EmbedContentRequest`
  /// objects.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<BatchEmbedContentsResponse> batchEmbedContents(
    BatchEmbedContentsRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.model}:batchEmbedContents');
    final response = await _client.post(url, body: request);
    return BatchEmbedContentsResponse.fromJson(response);
  }

  /// Runs a model's tokenizer on input `Content` and returns the token count.
  /// Refer to the [tokens guide](https://ai.google.dev/gemini-api/docs/tokens)
  /// to learn more about tokens.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<CountTokensResponse> countTokens(CountTokensRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.model}:countTokens');
    final response = await _client.post(url, body: request);
    return CountTokensResponse.fromJson(response);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<ListOperationsResponse> listOperations(
    ListOperationsRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.name}/operations', {
      if (request.filter case final $1 when $1.isNotDefault) 'filter': $1,
      if (request.pageSize case final $1 when $1.isNotDefault)
        'pageSize': '${$1}',
      if (request.pageToken case final $1 when $1.isNotDefault) 'pageToken': $1,
      if (request.returnPartialSuccess case final $1 when $1.isNotDefault)
        'returnPartialSuccess': '${$1}',
    });
    final response = await _client.get(url);
    return ListOperationsResponse.fromJson(response);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  ///
  /// This method can be used to get the current status of a long-running
  /// operation.
  Future<Operation<T, S>> getOperation<
    T extends ProtoMessage,
    S extends ProtoMessage
  >(Operation<T, S> request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    final response = await _client.get(url);
    return Operation.fromJson(response, request.operationHelper);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> deleteOperation(DeleteOperationRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    await _client.delete(url);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> cancelOperation(CancelOperationRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}:cancel');
    await _client.post(url);
  }

  /// Closes the client and cleans up any resources associated with it.
  ///
  /// Once [close] is called, no other methods should be called.
  void close() => _client.close();
}

/// Provides methods for getting metadata information about Generative Models.
final class ModelService {
  static const _host = 'generativelanguage.googleapis.com';

  final ServiceClient _client;

  /// Creates a `ModelService` using [client] for transport.
  ///
  /// The provided [http.Client] must be configured to provide whatever
  /// authentication is required by `ModelService`. You can do that using
  /// [`package:googleapis_auth`](https://pub.dev/packages/googleapis_auth).
  ModelService({required http.Client client})
    : _client = ServiceClient(client: client);

  /// Creates a `ModelService` that does authentication through an API key.
  ///
  /// If called without arguments, the API key is taken from these environment
  /// variables:
  ///
  /// - `GOOGLE_API_KEY`
  /// - `GEMINI_API_KEY`
  ///
  /// Throws [ConfigurationException] if called without arguments and none of
  /// the above environment variables are set. On the web,
  /// always throws [ConfigurationException] if called without arguments.
  ///
  /// See [API Keys Overview](https://cloud.google.com/api-keys/docs/overview).
  factory ModelService.fromApiKey([String? apiKey]) =>
      ModelService(client: httpClientFromApiKey(apiKey, _apiKeys));

  /// Gets information about a specific `Model` such as its version number, token
  /// limits,
  /// [parameters](https://ai.google.dev/gemini-api/docs/models/generative-models#model-parameters)
  /// and other metadata. Refer to the [Gemini models
  /// guide](https://ai.google.dev/gemini-api/docs/models/gemini) for detailed
  /// model information.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<Model> getModel(GetModelRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    final response = await _client.get(url);
    return Model.fromJson(response);
  }

  /// Lists the [`Model`s](https://ai.google.dev/gemini-api/docs/models/gemini)
  /// available through the Gemini API.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<ListModelsResponse> listModels(ListModelsRequest request) async {
    final url = Uri.https(_host, '/v1beta/models', {
      if (request.pageSize case final $1 when $1.isNotDefault)
        'pageSize': '${$1}',
      if (request.pageToken case final $1 when $1.isNotDefault) 'pageToken': $1,
    });
    final response = await _client.get(url);
    return ListModelsResponse.fromJson(response);
  }

  /// Gets information about a specific TunedModel.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<TunedModel> getTunedModel(GetTunedModelRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    final response = await _client.get(url);
    return TunedModel.fromJson(response);
  }

  /// Lists created tuned models.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<ListTunedModelsResponse> listTunedModels(
    ListTunedModelsRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/tunedModels', {
      if (request.pageSize case final $1 when $1.isNotDefault)
        'pageSize': '${$1}',
      if (request.pageToken case final $1 when $1.isNotDefault) 'pageToken': $1,
      if (request.filter case final $1 when $1.isNotDefault) 'filter': $1,
    });
    final response = await _client.get(url);
    return ListTunedModelsResponse.fromJson(response);
  }

  /// Creates a tuned model.
  /// Check intermediate tuning progress (if any) through the
  /// [google.longrunning.Operations] service.
  ///
  /// Access status and results through the Operations service.
  /// Example:
  ///   GET /v1/tunedModels/az2mb0bpw6i/operations/000-111-222
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  ///
  /// Returns an [Operation] representing the status of the long-running
  /// operation.
  ///
  /// When complete, [Operation.done] will be `true`. If successful,
  /// [Operation.responseAsMessage] will contain the operation's result.
  Future<Operation<TunedModel, CreateTunedModelMetadata>> createTunedModel(
    CreateTunedModelRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/tunedModels', {
      if (request.tunedModelId case final $1?) 'tunedModelId': $1,
    });
    final response = await _client.post(url, body: request.tunedModel);
    return Operation.fromJson(
      response,
      OperationHelper(TunedModel.fromJson, CreateTunedModelMetadata.fromJson),
    );
  }

  /// Updates a tuned model.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<TunedModel> updateTunedModel(UpdateTunedModelRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.tunedModel!.name}', {
      if (request.updateMask case final $1?) 'updateMask': $1.toJson(),
    });
    final response = await _client.patch(url, body: request.tunedModel);
    return TunedModel.fromJson(response);
  }

  /// Deletes a tuned model.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> deleteTunedModel(DeleteTunedModelRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    await _client.delete(url);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<ListOperationsResponse> listOperations(
    ListOperationsRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.name}/operations', {
      if (request.filter case final $1 when $1.isNotDefault) 'filter': $1,
      if (request.pageSize case final $1 when $1.isNotDefault)
        'pageSize': '${$1}',
      if (request.pageToken case final $1 when $1.isNotDefault) 'pageToken': $1,
      if (request.returnPartialSuccess case final $1 when $1.isNotDefault)
        'returnPartialSuccess': '${$1}',
    });
    final response = await _client.get(url);
    return ListOperationsResponse.fromJson(response);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  ///
  /// This method can be used to get the current status of a long-running
  /// operation.
  Future<Operation<T, S>> getOperation<
    T extends ProtoMessage,
    S extends ProtoMessage
  >(Operation<T, S> request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    final response = await _client.get(url);
    return Operation.fromJson(response, request.operationHelper);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> deleteOperation(DeleteOperationRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    await _client.delete(url);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> cancelOperation(CancelOperationRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}:cancel');
    await _client.post(url);
  }

  /// Closes the client and cleans up any resources associated with it.
  ///
  /// Once [close] is called, no other methods should be called.
  void close() => _client.close();
}

/// Provides methods for managing permissions to PaLM API resources.
final class PermissionService {
  static const _host = 'generativelanguage.googleapis.com';

  final ServiceClient _client;

  /// Creates a `PermissionService` using [client] for transport.
  ///
  /// The provided [http.Client] must be configured to provide whatever
  /// authentication is required by `PermissionService`. You can do that using
  /// [`package:googleapis_auth`](https://pub.dev/packages/googleapis_auth).
  PermissionService({required http.Client client})
    : _client = ServiceClient(client: client);

  /// Creates a `PermissionService` that does authentication through an API key.
  ///
  /// If called without arguments, the API key is taken from these environment
  /// variables:
  ///
  /// - `GOOGLE_API_KEY`
  /// - `GEMINI_API_KEY`
  ///
  /// Throws [ConfigurationException] if called without arguments and none of
  /// the above environment variables are set. On the web,
  /// always throws [ConfigurationException] if called without arguments.
  ///
  /// See [API Keys Overview](https://cloud.google.com/api-keys/docs/overview).
  factory PermissionService.fromApiKey([String? apiKey]) =>
      PermissionService(client: httpClientFromApiKey(apiKey, _apiKeys));

  /// Create a permission to a specific resource.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<Permission> createPermission(CreatePermissionRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.parent}/permissions');
    final response = await _client.post(url, body: request.permission);
    return Permission.fromJson(response);
  }

  /// Gets information about a specific Permission.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<Permission> getPermission(GetPermissionRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    final response = await _client.get(url);
    return Permission.fromJson(response);
  }

  /// Lists permissions for the specific resource.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<ListPermissionsResponse> listPermissions(
    ListPermissionsRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.parent}/permissions', {
      if (request.pageSize case final $1 when $1.isNotDefault)
        'pageSize': '${$1}',
      if (request.pageToken case final $1 when $1.isNotDefault) 'pageToken': $1,
    });
    final response = await _client.get(url);
    return ListPermissionsResponse.fromJson(response);
  }

  /// Updates the permission.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<Permission> updatePermission(UpdatePermissionRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.permission!.name}', {
      if (request.updateMask case final $1?) 'updateMask': $1.toJson(),
    });
    final response = await _client.patch(url, body: request.permission);
    return Permission.fromJson(response);
  }

  /// Deletes the permission.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> deletePermission(DeletePermissionRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    await _client.delete(url);
  }

  /// Transfers ownership of the tuned model.
  /// This is the only way to change ownership of the tuned model.
  /// The current owner will be downgraded to writer role.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<TransferOwnershipResponse> transferOwnership(
    TransferOwnershipRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.name}:transferOwnership');
    final response = await _client.post(url, body: request);
    return TransferOwnershipResponse.fromJson(response);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<ListOperationsResponse> listOperations(
    ListOperationsRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.name}/operations', {
      if (request.filter case final $1 when $1.isNotDefault) 'filter': $1,
      if (request.pageSize case final $1 when $1.isNotDefault)
        'pageSize': '${$1}',
      if (request.pageToken case final $1 when $1.isNotDefault) 'pageToken': $1,
      if (request.returnPartialSuccess case final $1 when $1.isNotDefault)
        'returnPartialSuccess': '${$1}',
    });
    final response = await _client.get(url);
    return ListOperationsResponse.fromJson(response);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  ///
  /// This method can be used to get the current status of a long-running
  /// operation.
  Future<Operation<T, S>> getOperation<
    T extends ProtoMessage,
    S extends ProtoMessage
  >(Operation<T, S> request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    final response = await _client.get(url);
    return Operation.fromJson(response, request.operationHelper);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> deleteOperation(DeleteOperationRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    await _client.delete(url);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> cancelOperation(CancelOperationRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}:cancel');
    await _client.post(url);
  }

  /// Closes the client and cleans up any resources associated with it.
  ///
  /// Once [close] is called, no other methods should be called.
  void close() => _client.close();
}

/// A service for online predictions and explanations.
final class PredictionService {
  static const _host = 'generativelanguage.googleapis.com';

  final ServiceClient _client;

  /// Creates a `PredictionService` using [client] for transport.
  ///
  /// The provided [http.Client] must be configured to provide whatever
  /// authentication is required by `PredictionService`. You can do that using
  /// [`package:googleapis_auth`](https://pub.dev/packages/googleapis_auth).
  PredictionService({required http.Client client})
    : _client = ServiceClient(client: client);

  /// Creates a `PredictionService` that does authentication through an API key.
  ///
  /// If called without arguments, the API key is taken from these environment
  /// variables:
  ///
  /// - `GOOGLE_API_KEY`
  /// - `GEMINI_API_KEY`
  ///
  /// Throws [ConfigurationException] if called without arguments and none of
  /// the above environment variables are set. On the web,
  /// always throws [ConfigurationException] if called without arguments.
  ///
  /// See [API Keys Overview](https://cloud.google.com/api-keys/docs/overview).
  factory PredictionService.fromApiKey([String? apiKey]) =>
      PredictionService(client: httpClientFromApiKey(apiKey, _apiKeys));

  /// Performs a prediction request.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<PredictResponse> predict(PredictRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.model}:predict');
    final response = await _client.post(url, body: request);
    return PredictResponse.fromJson(response);
  }

  /// Same as Predict but returns an LRO.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  ///
  /// Returns an [Operation] representing the status of the long-running
  /// operation.
  ///
  /// When complete, [Operation.done] will be `true`. If successful,
  /// [Operation.responseAsMessage] will contain the operation's result.
  Future<Operation<PredictLongRunningResponse, PredictLongRunningMetadata>>
  predictLongRunning(PredictLongRunningRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.model}:predictLongRunning');
    final response = await _client.post(url, body: request);
    return Operation.fromJson(
      response,
      OperationHelper(
        PredictLongRunningResponse.fromJson,
        PredictLongRunningMetadata.fromJson,
      ),
    );
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<ListOperationsResponse> listOperations(
    ListOperationsRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.name}/operations', {
      if (request.filter case final $1 when $1.isNotDefault) 'filter': $1,
      if (request.pageSize case final $1 when $1.isNotDefault)
        'pageSize': '${$1}',
      if (request.pageToken case final $1 when $1.isNotDefault) 'pageToken': $1,
      if (request.returnPartialSuccess case final $1 when $1.isNotDefault)
        'returnPartialSuccess': '${$1}',
    });
    final response = await _client.get(url);
    return ListOperationsResponse.fromJson(response);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  ///
  /// This method can be used to get the current status of a long-running
  /// operation.
  Future<Operation<T, S>> getOperation<
    T extends ProtoMessage,
    S extends ProtoMessage
  >(Operation<T, S> request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    final response = await _client.get(url);
    return Operation.fromJson(response, request.operationHelper);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> deleteOperation(DeleteOperationRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    await _client.delete(url);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> cancelOperation(CancelOperationRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}:cancel');
    await _client.post(url);
  }

  /// Closes the client and cleans up any resources associated with it.
  ///
  /// Once [close] is called, no other methods should be called.
  void close() => _client.close();
}

/// An API for semantic search over a corpus of user uploaded content.
final class RetrieverService {
  static const _host = 'generativelanguage.googleapis.com';

  final ServiceClient _client;

  /// Creates a `RetrieverService` using [client] for transport.
  ///
  /// The provided [http.Client] must be configured to provide whatever
  /// authentication is required by `RetrieverService`. You can do that using
  /// [`package:googleapis_auth`](https://pub.dev/packages/googleapis_auth).
  RetrieverService({required http.Client client})
    : _client = ServiceClient(client: client);

  /// Creates a `RetrieverService` that does authentication through an API key.
  ///
  /// If called without arguments, the API key is taken from these environment
  /// variables:
  ///
  /// - `GOOGLE_API_KEY`
  /// - `GEMINI_API_KEY`
  ///
  /// Throws [ConfigurationException] if called without arguments and none of
  /// the above environment variables are set. On the web,
  /// always throws [ConfigurationException] if called without arguments.
  ///
  /// See [API Keys Overview](https://cloud.google.com/api-keys/docs/overview).
  factory RetrieverService.fromApiKey([String? apiKey]) =>
      RetrieverService(client: httpClientFromApiKey(apiKey, _apiKeys));

  /// Creates an empty `Corpus`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<Corpus> createCorpus(CreateCorpusRequest request) async {
    final url = Uri.https(_host, '/v1beta/corpora');
    final response = await _client.post(url, body: request.corpus);
    return Corpus.fromJson(response);
  }

  /// Gets information about a specific `Corpus`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<Corpus> getCorpus(GetCorpusRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    final response = await _client.get(url);
    return Corpus.fromJson(response);
  }

  /// Updates a `Corpus`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<Corpus> updateCorpus(UpdateCorpusRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.corpus!.name}', {
      if (request.updateMask case final $1?) 'updateMask': $1.toJson(),
    });
    final response = await _client.patch(url, body: request.corpus);
    return Corpus.fromJson(response);
  }

  /// Deletes a `Corpus`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> deleteCorpus(DeleteCorpusRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}', {
      if (request.force case final $1 when $1.isNotDefault) 'force': '${$1}',
    });
    await _client.delete(url);
  }

  /// Lists all `Corpora` owned by the user.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<ListCorporaResponse> listCorpora(ListCorporaRequest request) async {
    final url = Uri.https(_host, '/v1beta/corpora', {
      if (request.pageSize case final $1 when $1.isNotDefault)
        'pageSize': '${$1}',
      if (request.pageToken case final $1 when $1.isNotDefault) 'pageToken': $1,
    });
    final response = await _client.get(url);
    return ListCorporaResponse.fromJson(response);
  }

  /// Performs semantic search over a `Corpus`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<QueryCorpusResponse> queryCorpus(QueryCorpusRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}:query');
    final response = await _client.post(url, body: request);
    return QueryCorpusResponse.fromJson(response);
  }

  /// Creates an empty `Document`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<Document> createDocument(CreateDocumentRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.parent}/documents');
    final response = await _client.post(url, body: request.document);
    return Document.fromJson(response);
  }

  /// Gets information about a specific `Document`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<Document> getDocument(GetDocumentRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    final response = await _client.get(url);
    return Document.fromJson(response);
  }

  /// Updates a `Document`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<Document> updateDocument(UpdateDocumentRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.document!.name}', {
      if (request.updateMask case final $1?) 'updateMask': $1.toJson(),
    });
    final response = await _client.patch(url, body: request.document);
    return Document.fromJson(response);
  }

  /// Deletes a `Document`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> deleteDocument(DeleteDocumentRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}', {
      if (request.force case final $1 when $1.isNotDefault) 'force': '${$1}',
    });
    await _client.delete(url);
  }

  /// Lists all `Document`s in a `Corpus`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<ListDocumentsResponse> listDocuments(
    ListDocumentsRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.parent}/documents', {
      if (request.pageSize case final $1 when $1.isNotDefault)
        'pageSize': '${$1}',
      if (request.pageToken case final $1 when $1.isNotDefault) 'pageToken': $1,
    });
    final response = await _client.get(url);
    return ListDocumentsResponse.fromJson(response);
  }

  /// Performs semantic search over a `Document`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<QueryDocumentResponse> queryDocument(
    QueryDocumentRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.name}:query');
    final response = await _client.post(url, body: request);
    return QueryDocumentResponse.fromJson(response);
  }

  /// Creates a `Chunk`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<Chunk> createChunk(CreateChunkRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.parent}/chunks');
    final response = await _client.post(url, body: request.chunk);
    return Chunk.fromJson(response);
  }

  /// Batch create `Chunk`s.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<BatchCreateChunksResponse> batchCreateChunks(
    BatchCreateChunksRequest request,
  ) async {
    final url = Uri.https(
      _host,
      '/v1beta/${request.parent}/chunks:batchCreate',
    );
    final response = await _client.post(url, body: request);
    return BatchCreateChunksResponse.fromJson(response);
  }

  /// Gets information about a specific `Chunk`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<Chunk> getChunk(GetChunkRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    final response = await _client.get(url);
    return Chunk.fromJson(response);
  }

  /// Updates a `Chunk`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<Chunk> updateChunk(UpdateChunkRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.chunk!.name}', {
      if (request.updateMask case final $1?) 'updateMask': $1.toJson(),
    });
    final response = await _client.patch(url, body: request.chunk);
    return Chunk.fromJson(response);
  }

  /// Batch update `Chunk`s.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<BatchUpdateChunksResponse> batchUpdateChunks(
    BatchUpdateChunksRequest request,
  ) async {
    final url = Uri.https(
      _host,
      '/v1beta/${request.parent}/chunks:batchUpdate',
    );
    final response = await _client.post(url, body: request);
    return BatchUpdateChunksResponse.fromJson(response);
  }

  /// Deletes a `Chunk`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> deleteChunk(DeleteChunkRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    await _client.delete(url);
  }

  /// Batch delete `Chunk`s.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> batchDeleteChunks(BatchDeleteChunksRequest request) async {
    final url = Uri.https(
      _host,
      '/v1beta/${request.parent}/chunks:batchDelete',
    );
    await _client.post(url, body: request);
  }

  /// Lists all `Chunk`s in a `Document`.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<ListChunksResponse> listChunks(ListChunksRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.parent}/chunks', {
      if (request.pageSize case final $1 when $1.isNotDefault)
        'pageSize': '${$1}',
      if (request.pageToken case final $1 when $1.isNotDefault) 'pageToken': $1,
    });
    final response = await _client.get(url);
    return ListChunksResponse.fromJson(response);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<ListOperationsResponse> listOperations(
    ListOperationsRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.name}/operations', {
      if (request.filter case final $1 when $1.isNotDefault) 'filter': $1,
      if (request.pageSize case final $1 when $1.isNotDefault)
        'pageSize': '${$1}',
      if (request.pageToken case final $1 when $1.isNotDefault) 'pageToken': $1,
      if (request.returnPartialSuccess case final $1 when $1.isNotDefault)
        'returnPartialSuccess': '${$1}',
    });
    final response = await _client.get(url);
    return ListOperationsResponse.fromJson(response);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  ///
  /// This method can be used to get the current status of a long-running
  /// operation.
  Future<Operation<T, S>> getOperation<
    T extends ProtoMessage,
    S extends ProtoMessage
  >(Operation<T, S> request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    final response = await _client.get(url);
    return Operation.fromJson(response, request.operationHelper);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> deleteOperation(DeleteOperationRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    await _client.delete(url);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> cancelOperation(CancelOperationRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}:cancel');
    await _client.post(url);
  }

  /// Closes the client and cleans up any resources associated with it.
  ///
  /// Once [close] is called, no other methods should be called.
  void close() => _client.close();
}

/// API for using Generative Language Models (GLMs) trained to generate text.
///
/// Also known as Large Language Models (LLM)s, these generate text given an
/// input prompt from the user.
final class TextService {
  static const _host = 'generativelanguage.googleapis.com';

  final ServiceClient _client;

  /// Creates a `TextService` using [client] for transport.
  ///
  /// The provided [http.Client] must be configured to provide whatever
  /// authentication is required by `TextService`. You can do that using
  /// [`package:googleapis_auth`](https://pub.dev/packages/googleapis_auth).
  TextService({required http.Client client})
    : _client = ServiceClient(client: client);

  /// Creates a `TextService` that does authentication through an API key.
  ///
  /// If called without arguments, the API key is taken from these environment
  /// variables:
  ///
  /// - `GOOGLE_API_KEY`
  /// - `GEMINI_API_KEY`
  ///
  /// Throws [ConfigurationException] if called without arguments and none of
  /// the above environment variables are set. On the web,
  /// always throws [ConfigurationException] if called without arguments.
  ///
  /// See [API Keys Overview](https://cloud.google.com/api-keys/docs/overview).
  factory TextService.fromApiKey([String? apiKey]) =>
      TextService(client: httpClientFromApiKey(apiKey, _apiKeys));

  /// Generates a response from the model given an input message.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<GenerateTextResponse> generateText(GenerateTextRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.model}:generateText');
    final response = await _client.post(url, body: request);
    return GenerateTextResponse.fromJson(response);
  }

  /// Generates an embedding from the model given an input message.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<EmbedTextResponse> embedText(EmbedTextRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.model}:embedText');
    final response = await _client.post(url, body: request);
    return EmbedTextResponse.fromJson(response);
  }

  /// Generates multiple embeddings from the model given input text in a
  /// synchronous call.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<BatchEmbedTextResponse> batchEmbedText(
    BatchEmbedTextRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.model}:batchEmbedText');
    final response = await _client.post(url, body: request);
    return BatchEmbedTextResponse.fromJson(response);
  }

  /// Runs a model's tokenizer on a text and returns the token count.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<CountTextTokensResponse> countTextTokens(
    CountTextTokensRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.model}:countTextTokens');
    final response = await _client.post(url, body: request);
    return CountTextTokensResponse.fromJson(response);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<ListOperationsResponse> listOperations(
    ListOperationsRequest request,
  ) async {
    final url = Uri.https(_host, '/v1beta/${request.name}/operations', {
      if (request.filter case final $1 when $1.isNotDefault) 'filter': $1,
      if (request.pageSize case final $1 when $1.isNotDefault)
        'pageSize': '${$1}',
      if (request.pageToken case final $1 when $1.isNotDefault) 'pageToken': $1,
      if (request.returnPartialSuccess case final $1 when $1.isNotDefault)
        'returnPartialSuccess': '${$1}',
    });
    final response = await _client.get(url);
    return ListOperationsResponse.fromJson(response);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  ///
  /// This method can be used to get the current status of a long-running
  /// operation.
  Future<Operation<T, S>> getOperation<
    T extends ProtoMessage,
    S extends ProtoMessage
  >(Operation<T, S> request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    final response = await _client.get(url);
    return Operation.fromJson(response, request.operationHelper);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> deleteOperation(DeleteOperationRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}');
    await _client.delete(url);
  }

  /// Provides the `Operations` service functionality in this service.
  ///
  /// Throws a [http.ClientException] if there were problems communicating with
  /// the API service. Throws a [StatusException] if the API failed with a
  /// [Status] message. Throws a [ServiceException] for any other failure.
  Future<void> cancelOperation(CancelOperationRequest request) async {
    final url = Uri.https(_host, '/v1beta/${request.name}:cancel');
    await _client.post(url);
  }

  /// Closes the client and cleans up any resources associated with it.
  ///
  /// Once [close] is called, no other methods should be called.
  void close() => _client.close();
}

/// Request to list CachedContents.
final class ListCachedContentsRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ListCachedContentsRequest';

  /// Optional. The maximum number of cached contents to return. The service may
  /// return fewer than this value. If unspecified, some default (under maximum)
  /// number of items will be returned. The maximum value is 1000; values above
  /// 1000 will be coerced to 1000.
  final int pageSize;

  /// Optional. A page token, received from a previous `ListCachedContents` call.
  /// Provide this to retrieve the subsequent page.
  ///
  /// When paginating, all other parameters provided to `ListCachedContents` must
  /// match the call that provided the page token.
  final String pageToken;

  ListCachedContentsRequest({this.pageSize = 0, this.pageToken = ''})
    : super(fullyQualifiedName);

  factory ListCachedContentsRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ListCachedContentsRequest(
      pageSize: switch (json['pageSize']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      pageToken: switch (json['pageToken']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (pageSize.isNotDefault) 'pageSize': pageSize,
    if (pageToken.isNotDefault) 'pageToken': pageToken,
  };

  @override
  String toString() {
    final contents = ['pageSize=$pageSize', 'pageToken=$pageToken'].join(',');
    return 'ListCachedContentsRequest($contents)';
  }
}

/// Response with CachedContents list.
final class ListCachedContentsResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ListCachedContentsResponse';

  /// List of cached contents.
  final List<CachedContent> cachedContents;

  /// A token, which can be sent as `page_token` to retrieve the next page.
  /// If this field is omitted, there are no subsequent pages.
  final String nextPageToken;

  ListCachedContentsResponse({
    this.cachedContents = const [],
    this.nextPageToken = '',
  }) : super(fullyQualifiedName);

  factory ListCachedContentsResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ListCachedContentsResponse(
      cachedContents: switch (json['cachedContents']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) CachedContent.fromJson(i)],
        _ => throw const FormatException('"cachedContents" is not a list'),
      },
      nextPageToken: switch (json['nextPageToken']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (cachedContents.isNotDefault)
      'cachedContents': encodeList(cachedContents),
    if (nextPageToken.isNotDefault) 'nextPageToken': nextPageToken,
  };

  @override
  String toString() {
    final contents = ['nextPageToken=$nextPageToken'].join(',');
    return 'ListCachedContentsResponse($contents)';
  }
}

/// Request to create CachedContent.
final class CreateCachedContentRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CreateCachedContentRequest';

  /// Required. The cached content to create.
  final CachedContent? cachedContent;

  CreateCachedContentRequest({required this.cachedContent})
    : super(fullyQualifiedName);

  factory CreateCachedContentRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CreateCachedContentRequest(
      cachedContent: switch (json['cachedContent']) {
        null => null,
        Object $1 => CachedContent.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (cachedContent != null) 'cachedContent': cachedContent!.toJson(),
  };

  @override
  String toString() => 'CreateCachedContentRequest()';
}

/// Request to read CachedContent.
final class GetCachedContentRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GetCachedContentRequest';

  /// Required. The resource name referring to the content cache entry.
  /// Format: `cachedContents/{id}`
  final String name;

  GetCachedContentRequest({required this.name}) : super(fullyQualifiedName);

  factory GetCachedContentRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GetCachedContentRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {'name': name};

  @override
  String toString() {
    final contents = ['name=$name'].join(',');
    return 'GetCachedContentRequest($contents)';
  }
}

/// Request to update CachedContent.
final class UpdateCachedContentRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.UpdateCachedContentRequest';

  /// Required. The content cache entry to update
  final CachedContent? cachedContent;

  /// The list of fields to update.
  final FieldMask? updateMask;

  UpdateCachedContentRequest({required this.cachedContent, this.updateMask})
    : super(fullyQualifiedName);

  factory UpdateCachedContentRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return UpdateCachedContentRequest(
      cachedContent: switch (json['cachedContent']) {
        null => null,
        Object $1 => CachedContent.fromJson($1),
      },
      updateMask: switch (json['updateMask']) {
        null => null,
        Object $1 => FieldMask.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (cachedContent != null) 'cachedContent': cachedContent!.toJson(),
    if (updateMask != null) 'updateMask': updateMask!.toJson(),
  };

  @override
  String toString() => 'UpdateCachedContentRequest()';
}

/// Request to delete CachedContent.
final class DeleteCachedContentRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.DeleteCachedContentRequest';

  /// Required. The resource name referring to the content cache entry
  /// Format: `cachedContents/{id}`
  final String name;

  DeleteCachedContentRequest({required this.name}) : super(fullyQualifiedName);

  factory DeleteCachedContentRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return DeleteCachedContentRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {'name': name};

  @override
  String toString() {
    final contents = ['name=$name'].join(',');
    return 'DeleteCachedContentRequest($contents)';
  }
}

/// Content that has been preprocessed and can be used in subsequent request
/// to GenerativeService.
///
/// Cached content can be only used with model it was created for.
final class CachedContent extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CachedContent';

  /// Timestamp in UTC of when this resource is considered expired.
  /// This is *always* provided on output, regardless of what was sent
  /// on input.
  final Timestamp? expireTime;

  /// Input only. New TTL for this resource, input only.
  final Duration? ttl;

  /// Output only. Identifier. The resource name referring to the cached content.
  /// Format: `cachedContents/{id}`
  final String? name;

  /// Optional. Immutable. The user-generated meaningful display name of the
  /// cached content. Maximum 128 Unicode characters.
  final String? displayName;

  /// Required. Immutable. The name of the `Model` to use for cached content
  /// Format: `models/{model}`
  final String? model;

  /// Optional. Input only. Immutable. Developer set system instruction.
  /// Currently text only.
  final Content? systemInstruction;

  /// Optional. Input only. Immutable. The content to cache.
  final List<Content> contents;

  /// Optional. Input only. Immutable. A list of `Tools` the model may use to
  /// generate the next response
  final List<Tool> tools;

  /// Optional. Input only. Immutable. Tool config. This config is shared for all
  /// tools.
  final ToolConfig? toolConfig;

  /// Output only. Creation time of the cache entry.
  final Timestamp? createTime;

  /// Output only. When the cache entry was last updated in UTC time.
  final Timestamp? updateTime;

  /// Output only. Metadata on the usage of the cached content.
  final CachedContent_UsageMetadata? usageMetadata;

  CachedContent({
    this.expireTime,
    this.ttl,
    this.name,
    this.displayName,
    required this.model,
    this.systemInstruction,
    this.contents = const [],
    this.tools = const [],
    this.toolConfig,
    this.createTime,
    this.updateTime,
    this.usageMetadata,
  }) : super(fullyQualifiedName);

  factory CachedContent.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CachedContent(
      expireTime: switch (json['expireTime']) {
        null => null,
        Object $1 => Timestamp.fromJson($1),
      },
      ttl: switch (json['ttl']) {
        null => null,
        Object $1 => Duration.fromJson($1),
      },
      name: switch (json['name']) {
        null => null,
        Object $1 => decodeString($1),
      },
      displayName: switch (json['displayName']) {
        null => null,
        Object $1 => decodeString($1),
      },
      model: switch (json['model']) {
        null => null,
        Object $1 => decodeString($1),
      },
      systemInstruction: switch (json['systemInstruction']) {
        null => null,
        Object $1 => Content.fromJson($1),
      },
      contents: switch (json['contents']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Content.fromJson(i)],
        _ => throw const FormatException('"contents" is not a list'),
      },
      tools: switch (json['tools']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Tool.fromJson(i)],
        _ => throw const FormatException('"tools" is not a list'),
      },
      toolConfig: switch (json['toolConfig']) {
        null => null,
        Object $1 => ToolConfig.fromJson($1),
      },
      createTime: switch (json['createTime']) {
        null => null,
        Object $1 => Timestamp.fromJson($1),
      },
      updateTime: switch (json['updateTime']) {
        null => null,
        Object $1 => Timestamp.fromJson($1),
      },
      usageMetadata: switch (json['usageMetadata']) {
        null => null,
        Object $1 => CachedContent_UsageMetadata.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (expireTime != null) 'expireTime': expireTime!.toJson(),
    if (ttl != null) 'ttl': ttl!.toJson(),
    if (name != null) 'name': name,
    if (displayName != null) 'displayName': displayName,
    if (model != null) 'model': model,
    if (systemInstruction != null)
      'systemInstruction': systemInstruction!.toJson(),
    if (contents.isNotDefault) 'contents': encodeList(contents),
    if (tools.isNotDefault) 'tools': encodeList(tools),
    if (toolConfig != null) 'toolConfig': toolConfig!.toJson(),
    if (createTime != null) 'createTime': createTime!.toJson(),
    if (updateTime != null) 'updateTime': updateTime!.toJson(),
    if (usageMetadata != null) 'usageMetadata': usageMetadata!.toJson(),
  };

  @override
  String toString() {
    final contents = [
      if (name != null) 'name=$name',
      if (displayName != null) 'displayName=$displayName',
      if (model != null) 'model=$model',
    ].join(',');
    return 'CachedContent($contents)';
  }
}

/// Metadata on the usage of the cached content.
final class CachedContent_UsageMetadata extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CachedContent.UsageMetadata';

  /// Total number of tokens that the cached content consumes.
  final int totalTokenCount;

  CachedContent_UsageMetadata({this.totalTokenCount = 0})
    : super(fullyQualifiedName);

  factory CachedContent_UsageMetadata.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CachedContent_UsageMetadata(
      totalTokenCount: switch (json['totalTokenCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (totalTokenCount.isNotDefault) 'totalTokenCount': totalTokenCount,
  };

  @override
  String toString() {
    final contents = ['totalTokenCount=$totalTokenCount'].join(',');
    return 'UsageMetadata($contents)';
  }
}

/// A collection of source attributions for a piece of content.
final class CitationMetadata extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CitationMetadata';

  /// Citations to sources for a specific response.
  final List<CitationSource> citationSources;

  CitationMetadata({this.citationSources = const []})
    : super(fullyQualifiedName);

  factory CitationMetadata.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CitationMetadata(
      citationSources: switch (json['citationSources']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) CitationSource.fromJson(i)],
        _ => throw const FormatException('"citationSources" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (citationSources.isNotDefault)
      'citationSources': encodeList(citationSources),
  };

  @override
  String toString() => 'CitationMetadata()';
}

/// A citation to a source for a portion of a specific response.
final class CitationSource extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CitationSource';

  /// Optional. Start of segment of the response that is attributed to this
  /// source.
  ///
  /// Index indicates the start of the segment, measured in bytes.
  final int? startIndex;

  /// Optional. End of the attributed segment, exclusive.
  final int? endIndex;

  /// Optional. URI that is attributed as a source for a portion of the text.
  final String? uri;

  /// Optional. License for the GitHub project that is attributed as a source for
  /// segment.
  ///
  /// License info is required for code citations.
  final String? license;

  CitationSource({this.startIndex, this.endIndex, this.uri, this.license})
    : super(fullyQualifiedName);

  factory CitationSource.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CitationSource(
      startIndex: switch (json['startIndex']) {
        null => null,
        Object $1 => decodeInt($1),
      },
      endIndex: switch (json['endIndex']) {
        null => null,
        Object $1 => decodeInt($1),
      },
      uri: switch (json['uri']) {
        null => null,
        Object $1 => decodeString($1),
      },
      license: switch (json['license']) {
        null => null,
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (startIndex != null) 'startIndex': startIndex,
    if (endIndex != null) 'endIndex': endIndex,
    if (uri != null) 'uri': uri,
    if (license != null) 'license': license,
  };

  @override
  String toString() {
    final contents = [
      if (startIndex != null) 'startIndex=$startIndex',
      if (endIndex != null) 'endIndex=$endIndex',
      if (uri != null) 'uri=$uri',
      if (license != null) 'license=$license',
    ].join(',');
    return 'CitationSource($contents)';
  }
}

/// The base structured datatype containing multi-part content of a message.
///
/// A `Content` includes a `role` field designating the producer of the `Content`
/// and a `parts` field containing multi-part data that contains the content of
/// the message turn.
final class Content extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Content';

  /// Ordered `Parts` that constitute a single message. Parts may have different
  /// MIME types.
  final List<Part> parts;

  /// Optional. The producer of the content. Must be either 'user' or 'model'.
  ///
  /// Useful to set for multi-turn conversations, otherwise can be left blank
  /// or unset.
  final String role;

  Content({this.parts = const [], this.role = ''}) : super(fullyQualifiedName);

  factory Content.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Content(
      parts: switch (json['parts']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Part.fromJson(i)],
        _ => throw const FormatException('"parts" is not a list'),
      },
      role: switch (json['role']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (parts.isNotDefault) 'parts': encodeList(parts),
    if (role.isNotDefault) 'role': role,
  };

  @override
  String toString() {
    final contents = ['role=$role'].join(',');
    return 'Content($contents)';
  }
}

/// A datatype containing media that is part of a multi-part `Content` message.
///
/// A `Part` consists of data which has an associated datatype. A `Part` can only
/// contain one of the accepted types in `Part.data`.
///
/// A `Part` must have a fixed IANA MIME type identifying the type and subtype
/// of the media if the `inline_data` field is filled with raw bytes.
final class Part extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Part';

  /// Inline text.
  final String? text;

  /// Inline media bytes.
  final Blob? inlineData;

  /// A predicted `FunctionCall` returned from the model that contains
  /// a string representing the `FunctionDeclaration.name` with the
  /// arguments and their values.
  final FunctionCall? functionCall;

  /// The result output of a `FunctionCall` that contains a string
  /// representing the `FunctionDeclaration.name` and a structured JSON
  /// object containing any output from the function is used as context to
  /// the model.
  final FunctionResponse? functionResponse;

  /// URI based data.
  final FileData? fileData;

  /// Code generated by the model that is meant to be executed.
  final ExecutableCode? executableCode;

  /// Result of executing the `ExecutableCode`.
  final CodeExecutionResult? codeExecutionResult;

  /// Optional. Video metadata. The metadata should only be specified while the
  /// video data is presented in inline_data or file_data.
  final VideoMetadata? videoMetadata;

  /// Optional. Indicates if the part is thought from the model.
  final bool thought;

  /// Optional. An opaque signature for the thought so it can be reused in
  /// subsequent requests.
  final Uint8List thoughtSignature;

  /// Custom metadata associated with the Part.
  /// Agents using genai.Part as content representation may need to keep track
  /// of the additional information. For example it can be name of a file/source
  /// from which the Part originates or a way to multiplex multiple Part streams.
  final Struct? partMetadata;

  Part({
    this.text,
    this.inlineData,
    this.functionCall,
    this.functionResponse,
    this.fileData,
    this.executableCode,
    this.codeExecutionResult,
    this.videoMetadata,
    this.thought = false,
    Uint8List? thoughtSignature,
    this.partMetadata,
  }) : thoughtSignature = thoughtSignature ?? Uint8List(0),
       super(fullyQualifiedName);

  factory Part.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Part(
      text: switch (json['text']) {
        null => null,
        Object $1 => decodeString($1),
      },
      inlineData: switch (json['inlineData']) {
        null => null,
        Object $1 => Blob.fromJson($1),
      },
      functionCall: switch (json['functionCall']) {
        null => null,
        Object $1 => FunctionCall.fromJson($1),
      },
      functionResponse: switch (json['functionResponse']) {
        null => null,
        Object $1 => FunctionResponse.fromJson($1),
      },
      fileData: switch (json['fileData']) {
        null => null,
        Object $1 => FileData.fromJson($1),
      },
      executableCode: switch (json['executableCode']) {
        null => null,
        Object $1 => ExecutableCode.fromJson($1),
      },
      codeExecutionResult: switch (json['codeExecutionResult']) {
        null => null,
        Object $1 => CodeExecutionResult.fromJson($1),
      },
      videoMetadata: switch (json['videoMetadata']) {
        null => null,
        Object $1 => VideoMetadata.fromJson($1),
      },
      thought: switch (json['thought']) {
        null => false,
        Object $1 => decodeBool($1),
      },
      thoughtSignature: switch (json['thoughtSignature']) {
        null => Uint8List(0),
        Object $1 => decodeBytes($1),
      },
      partMetadata: switch (json['partMetadata']) {
        null => null,
        Object $1 => Struct.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (text != null) 'text': text,
    if (inlineData != null) 'inlineData': inlineData!.toJson(),
    if (functionCall != null) 'functionCall': functionCall!.toJson(),
    if (functionResponse != null)
      'functionResponse': functionResponse!.toJson(),
    if (fileData != null) 'fileData': fileData!.toJson(),
    if (executableCode != null) 'executableCode': executableCode!.toJson(),
    if (codeExecutionResult != null)
      'codeExecutionResult': codeExecutionResult!.toJson(),
    if (videoMetadata != null) 'videoMetadata': videoMetadata!.toJson(),
    if (thought.isNotDefault) 'thought': thought,
    if (thoughtSignature.isNotDefault)
      'thoughtSignature': encodeBytes(thoughtSignature),
    if (partMetadata != null) 'partMetadata': partMetadata!.toJson(),
  };

  @override
  String toString() {
    final contents = [
      if (text != null) 'text=$text',
      'thought=$thought',
      'thoughtSignature=$thoughtSignature',
    ].join(',');
    return 'Part($contents)';
  }
}

/// A datatype containing media that is part of a `FunctionResponse` message.
///
/// A `FunctionResponsePart` consists of data which has an associated datatype. A
/// `FunctionResponsePart` can only contain one of the accepted types in
/// `FunctionResponsePart.data`.
///
/// A `FunctionResponsePart` must have a fixed IANA MIME type identifying the
/// type and subtype of the media if the `inline_data` field is filled with raw
/// bytes.
final class FunctionResponsePart extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.FunctionResponsePart';

  /// Inline media bytes.
  final FunctionResponseBlob? inlineData;

  FunctionResponsePart({this.inlineData}) : super(fullyQualifiedName);

  factory FunctionResponsePart.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return FunctionResponsePart(
      inlineData: switch (json['inlineData']) {
        null => null,
        Object $1 => FunctionResponseBlob.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (inlineData != null) 'inlineData': inlineData!.toJson(),
  };

  @override
  String toString() => 'FunctionResponsePart()';
}

/// Raw media bytes.
///
/// Text should not be sent as raw bytes, use the 'text' field.
final class Blob extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Blob';

  /// The IANA standard MIME type of the source data.
  /// Examples:
  ///   - image/png
  ///   - image/jpeg
  /// If an unsupported MIME type is provided, an error will be returned. For a
  /// complete list of supported types, see [Supported file
  /// formats](https://ai.google.dev/gemini-api/docs/prompting_with_media#supported_file_formats).
  final String mimeType;

  /// Raw bytes for media formats.
  final Uint8List data;

  Blob({this.mimeType = '', Uint8List? data})
    : data = data ?? Uint8List(0),
      super(fullyQualifiedName);

  factory Blob.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Blob(
      mimeType: switch (json['mimeType']) {
        null => '',
        Object $1 => decodeString($1),
      },
      data: switch (json['data']) {
        null => Uint8List(0),
        Object $1 => decodeBytes($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (mimeType.isNotDefault) 'mimeType': mimeType,
    if (data.isNotDefault) 'data': encodeBytes(data),
  };

  @override
  String toString() {
    final contents = ['mimeType=$mimeType', 'data=$data'].join(',');
    return 'Blob($contents)';
  }
}

/// Raw media bytes for function response.
///
/// Text should not be sent as raw bytes, use the 'FunctionResponse.response'
/// field.
final class FunctionResponseBlob extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.FunctionResponseBlob';

  /// The IANA standard MIME type of the source data.
  /// Examples:
  ///   - image/png
  ///   - image/jpeg
  /// If an unsupported MIME type is provided, an error will be returned. For a
  /// complete list of supported types, see [Supported file
  /// formats](https://ai.google.dev/gemini-api/docs/prompting_with_media#supported_file_formats).
  final String mimeType;

  /// Raw bytes for media formats.
  final Uint8List data;

  FunctionResponseBlob({this.mimeType = '', Uint8List? data})
    : data = data ?? Uint8List(0),
      super(fullyQualifiedName);

  factory FunctionResponseBlob.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return FunctionResponseBlob(
      mimeType: switch (json['mimeType']) {
        null => '',
        Object $1 => decodeString($1),
      },
      data: switch (json['data']) {
        null => Uint8List(0),
        Object $1 => decodeBytes($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (mimeType.isNotDefault) 'mimeType': mimeType,
    if (data.isNotDefault) 'data': encodeBytes(data),
  };

  @override
  String toString() {
    final contents = ['mimeType=$mimeType', 'data=$data'].join(',');
    return 'FunctionResponseBlob($contents)';
  }
}

/// URI based data.
final class FileData extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.FileData';

  /// Optional. The IANA standard MIME type of the source data.
  final String mimeType;

  /// Required. URI.
  final String fileUri;

  FileData({this.mimeType = '', required this.fileUri})
    : super(fullyQualifiedName);

  factory FileData.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return FileData(
      mimeType: switch (json['mimeType']) {
        null => '',
        Object $1 => decodeString($1),
      },
      fileUri: switch (json['fileUri']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (mimeType.isNotDefault) 'mimeType': mimeType,
    'fileUri': fileUri,
  };

  @override
  String toString() {
    final contents = ['mimeType=$mimeType', 'fileUri=$fileUri'].join(',');
    return 'FileData($contents)';
  }
}

/// Metadata describes the input video content.
final class VideoMetadata extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.VideoMetadata';

  /// Optional. The start offset of the video.
  final Duration? startOffset;

  /// Optional. The end offset of the video.
  final Duration? endOffset;

  /// Optional. The frame rate of the video sent to the model. If not specified,
  /// the default value will be 1.0. The fps range is (0.0, 24.0].
  final double fps;

  VideoMetadata({this.startOffset, this.endOffset, this.fps = 0})
    : super(fullyQualifiedName);

  factory VideoMetadata.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return VideoMetadata(
      startOffset: switch (json['startOffset']) {
        null => null,
        Object $1 => Duration.fromJson($1),
      },
      endOffset: switch (json['endOffset']) {
        null => null,
        Object $1 => Duration.fromJson($1),
      },
      fps: switch (json['fps']) {
        null => 0,
        Object $1 => decodeDouble($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (startOffset != null) 'startOffset': startOffset!.toJson(),
    if (endOffset != null) 'endOffset': endOffset!.toJson(),
    if (fps.isNotDefault) 'fps': encodeDouble(fps),
  };

  @override
  String toString() {
    final contents = ['fps=$fps'].join(',');
    return 'VideoMetadata($contents)';
  }
}

/// Code generated by the model that is meant to be executed, and the result
/// returned to the model.
///
/// Only generated when using the `CodeExecution` tool, in which the code will
/// be automatically executed, and a corresponding `CodeExecutionResult` will
/// also be generated.
final class ExecutableCode extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ExecutableCode';

  /// Required. Programming language of the `code`.
  final ExecutableCode_Language language;

  /// Required. The code to be executed.
  final String code;

  ExecutableCode({required this.language, required this.code})
    : super(fullyQualifiedName);

  factory ExecutableCode.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ExecutableCode(
      language: switch (json['language']) {
        null => ExecutableCode_Language.$default,
        Object $1 => ExecutableCode_Language.fromJson($1),
      },
      code: switch (json['code']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {'language': language.toJson(), 'code': code};

  @override
  String toString() {
    final contents = ['language=$language', 'code=$code'].join(',');
    return 'ExecutableCode($contents)';
  }
}

/// Supported programming languages for the generated code.
final class ExecutableCode_Language extends ProtoEnum {
  /// Unspecified language. This value should not be used.
  static const languageUnspecified = ExecutableCode_Language(
    'LANGUAGE_UNSPECIFIED',
  );

  /// Python >= 3.10, with numpy and simpy available.
  static const python = ExecutableCode_Language('PYTHON');

  /// The default value for [ExecutableCode_Language].
  static const $default = languageUnspecified;

  const ExecutableCode_Language(super.value);

  factory ExecutableCode_Language.fromJson(Object? json) =>
      ExecutableCode_Language(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'Language.$value';
}

/// Result of executing the `ExecutableCode`.
///
/// Only generated when using the `CodeExecution`, and always follows a `part`
/// containing the `ExecutableCode`.
final class CodeExecutionResult extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CodeExecutionResult';

  /// Required. Outcome of the code execution.
  final CodeExecutionResult_Outcome outcome;

  /// Optional. Contains stdout when code execution is successful, stderr or
  /// other description otherwise.
  final String output;

  CodeExecutionResult({required this.outcome, this.output = ''})
    : super(fullyQualifiedName);

  factory CodeExecutionResult.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CodeExecutionResult(
      outcome: switch (json['outcome']) {
        null => CodeExecutionResult_Outcome.$default,
        Object $1 => CodeExecutionResult_Outcome.fromJson($1),
      },
      output: switch (json['output']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    'outcome': outcome.toJson(),
    if (output.isNotDefault) 'output': output,
  };

  @override
  String toString() {
    final contents = ['outcome=$outcome', 'output=$output'].join(',');
    return 'CodeExecutionResult($contents)';
  }
}

/// Enumeration of possible outcomes of the code execution.
final class CodeExecutionResult_Outcome extends ProtoEnum {
  /// Unspecified status. This value should not be used.
  static const outcomeUnspecified = CodeExecutionResult_Outcome(
    'OUTCOME_UNSPECIFIED',
  );

  /// Code execution completed successfully.
  static const outcomeOk = CodeExecutionResult_Outcome('OUTCOME_OK');

  /// Code execution finished but with a failure. `stderr` should contain the
  /// reason.
  static const outcomeFailed = CodeExecutionResult_Outcome('OUTCOME_FAILED');

  /// Code execution ran for too long, and was cancelled. There may or may not
  /// be a partial output present.
  static const outcomeDeadlineExceeded = CodeExecutionResult_Outcome(
    'OUTCOME_DEADLINE_EXCEEDED',
  );

  /// The default value for [CodeExecutionResult_Outcome].
  static const $default = outcomeUnspecified;

  const CodeExecutionResult_Outcome(super.value);

  factory CodeExecutionResult_Outcome.fromJson(Object? json) =>
      CodeExecutionResult_Outcome(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'Outcome.$value';
}

/// Tool details that the model may use to generate response.
///
/// A `Tool` is a piece of code that enables the system to interact with
/// external systems to perform an action, or set of actions, outside of
/// knowledge and scope of the model.
///
/// Next ID: 12
final class Tool extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Tool';

  /// Optional. A list of `FunctionDeclarations` available to the model that can
  /// be used for function calling.
  ///
  /// The model or system does not execute the function. Instead the defined
  /// function may be returned as a
  /// `FunctionCall` with
  /// arguments to the client side for execution. The model may decide to call a
  /// subset of these functions by populating
  /// `FunctionCall` in
  /// the response. The next conversation turn may contain a
  /// `FunctionResponse`
  /// with the `Content.role`
  /// "function" generation context for the next model turn.
  final List<FunctionDeclaration> functionDeclarations;

  /// Optional. Retrieval tool that is powered by Google search.
  final GoogleSearchRetrieval? googleSearchRetrieval;

  /// Optional. Enables the model to execute code as part of generation.
  final CodeExecution? codeExecution;

  /// Optional. GoogleSearch tool type.
  /// Tool to support Google Search in Model. Powered by Google.
  final Tool_GoogleSearch? googleSearch;

  /// Optional. Tool to support the model interacting directly with the computer.
  /// If enabled, it automatically populates computer-use specific Function
  /// Declarations.
  final Tool_ComputerUse? computerUse;

  /// Optional. Tool to support URL context retrieval.
  final UrlContext? urlContext;

  Tool({
    this.functionDeclarations = const [],
    this.googleSearchRetrieval,
    this.codeExecution,
    this.googleSearch,
    this.computerUse,
    this.urlContext,
  }) : super(fullyQualifiedName);

  factory Tool.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Tool(
      functionDeclarations: switch (json['functionDeclarations']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) FunctionDeclaration.fromJson(i),
        ],
        _ => throw const FormatException(
          '"functionDeclarations" is not a list',
        ),
      },
      googleSearchRetrieval: switch (json['googleSearchRetrieval']) {
        null => null,
        Object $1 => GoogleSearchRetrieval.fromJson($1),
      },
      codeExecution: switch (json['codeExecution']) {
        null => null,
        Object $1 => CodeExecution.fromJson($1),
      },
      googleSearch: switch (json['googleSearch']) {
        null => null,
        Object $1 => Tool_GoogleSearch.fromJson($1),
      },
      computerUse: switch (json['computerUse']) {
        null => null,
        Object $1 => Tool_ComputerUse.fromJson($1),
      },
      urlContext: switch (json['urlContext']) {
        null => null,
        Object $1 => UrlContext.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (functionDeclarations.isNotDefault)
      'functionDeclarations': encodeList(functionDeclarations),
    if (googleSearchRetrieval != null)
      'googleSearchRetrieval': googleSearchRetrieval!.toJson(),
    if (codeExecution != null) 'codeExecution': codeExecution!.toJson(),
    if (googleSearch != null) 'googleSearch': googleSearch!.toJson(),
    if (computerUse != null) 'computerUse': computerUse!.toJson(),
    if (urlContext != null) 'urlContext': urlContext!.toJson(),
  };

  @override
  String toString() => 'Tool()';
}

/// GoogleSearch tool type.
/// Tool to support Google Search in Model. Powered by Google.
final class Tool_GoogleSearch extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Tool.GoogleSearch';

  /// Optional. Filter search results to a specific time range.
  /// If customers set a start time, they must set an end time (and vice
  /// versa).
  final Interval? timeRangeFilter;

  Tool_GoogleSearch({this.timeRangeFilter}) : super(fullyQualifiedName);

  factory Tool_GoogleSearch.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Tool_GoogleSearch(
      timeRangeFilter: switch (json['timeRangeFilter']) {
        null => null,
        Object $1 => Interval.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (timeRangeFilter != null) 'timeRangeFilter': timeRangeFilter!.toJson(),
  };

  @override
  String toString() => 'GoogleSearch()';
}

/// Computer Use tool type.
final class Tool_ComputerUse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Tool.ComputerUse';

  /// Required. The environment being operated.
  final Tool_ComputerUse_Environment environment;

  /// Optional. By default, predefined functions are included in the final
  /// model call. Some of them can be explicitly excluded from being
  /// automatically included. This can serve two purposes:
  /// 1. Using a more restricted / different action space.
  /// 2. Improving the definitions / instructions of predefined functions.
  final List<String> excludedPredefinedFunctions;

  Tool_ComputerUse({
    required this.environment,
    this.excludedPredefinedFunctions = const [],
  }) : super(fullyQualifiedName);

  factory Tool_ComputerUse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Tool_ComputerUse(
      environment: switch (json['environment']) {
        null => Tool_ComputerUse_Environment.$default,
        Object $1 => Tool_ComputerUse_Environment.fromJson($1),
      },
      excludedPredefinedFunctions:
          switch (json['excludedPredefinedFunctions']) {
            null => [],
            List<Object?> $1 => [for (final i in $1) decodeString(i)],
            _ => throw const FormatException(
              '"excludedPredefinedFunctions" is not a list',
            ),
          },
    );
  }

  @override
  Object toJson() => {
    'environment': environment.toJson(),
    if (excludedPredefinedFunctions.isNotDefault)
      'excludedPredefinedFunctions': excludedPredefinedFunctions,
  };

  @override
  String toString() {
    final contents = ['environment=$environment'].join(',');
    return 'ComputerUse($contents)';
  }
}

/// Represents the environment being operated, such as a web browser.
final class Tool_ComputerUse_Environment extends ProtoEnum {
  /// Defaults to browser.
  static const environmentUnspecified = Tool_ComputerUse_Environment(
    'ENVIRONMENT_UNSPECIFIED',
  );

  /// Operates in a web browser.
  static const environmentBrowser = Tool_ComputerUse_Environment(
    'ENVIRONMENT_BROWSER',
  );

  /// The default value for [Tool_ComputerUse_Environment].
  static const $default = environmentUnspecified;

  const Tool_ComputerUse_Environment(super.value);

  factory Tool_ComputerUse_Environment.fromJson(Object? json) =>
      Tool_ComputerUse_Environment(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'Environment.$value';
}

/// Tool to support URL context retrieval.
final class UrlContext extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.UrlContext';

  UrlContext() : super(fullyQualifiedName);

  factory UrlContext.fromJson(Object? j) => UrlContext();

  @override
  Object toJson() => {};

  @override
  String toString() => 'UrlContext()';
}

/// Tool to retrieve public web data for grounding, powered by Google.
final class GoogleSearchRetrieval extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GoogleSearchRetrieval';

  /// Specifies the dynamic retrieval configuration for the given source.
  final DynamicRetrievalConfig? dynamicRetrievalConfig;

  GoogleSearchRetrieval({this.dynamicRetrievalConfig})
    : super(fullyQualifiedName);

  factory GoogleSearchRetrieval.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GoogleSearchRetrieval(
      dynamicRetrievalConfig: switch (json['dynamicRetrievalConfig']) {
        null => null,
        Object $1 => DynamicRetrievalConfig.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (dynamicRetrievalConfig != null)
      'dynamicRetrievalConfig': dynamicRetrievalConfig!.toJson(),
  };

  @override
  String toString() => 'GoogleSearchRetrieval()';
}

/// Describes the options to customize dynamic retrieval.
final class DynamicRetrievalConfig extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.DynamicRetrievalConfig';

  /// The mode of the predictor to be used in dynamic retrieval.
  final DynamicRetrievalConfig_Mode mode;

  /// The threshold to be used in dynamic retrieval.
  /// If not set, a system default value is used.
  final double? dynamicThreshold;

  DynamicRetrievalConfig({
    this.mode = DynamicRetrievalConfig_Mode.$default,
    this.dynamicThreshold,
  }) : super(fullyQualifiedName);

  factory DynamicRetrievalConfig.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return DynamicRetrievalConfig(
      mode: switch (json['mode']) {
        null => DynamicRetrievalConfig_Mode.$default,
        Object $1 => DynamicRetrievalConfig_Mode.fromJson($1),
      },
      dynamicThreshold: switch (json['dynamicThreshold']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (mode.isNotDefault) 'mode': mode.toJson(),
    if (dynamicThreshold != null)
      'dynamicThreshold': encodeDouble(dynamicThreshold),
  };

  @override
  String toString() {
    final contents = [
      'mode=$mode',
      if (dynamicThreshold != null) 'dynamicThreshold=$dynamicThreshold',
    ].join(',');
    return 'DynamicRetrievalConfig($contents)';
  }
}

/// The mode of the predictor to be used in dynamic retrieval.
final class DynamicRetrievalConfig_Mode extends ProtoEnum {
  /// Always trigger retrieval.
  static const modeUnspecified = DynamicRetrievalConfig_Mode(
    'MODE_UNSPECIFIED',
  );

  /// Run retrieval only when system decides it is necessary.
  static const modeDynamic = DynamicRetrievalConfig_Mode('MODE_DYNAMIC');

  /// The default value for [DynamicRetrievalConfig_Mode].
  static const $default = modeUnspecified;

  const DynamicRetrievalConfig_Mode(super.value);

  factory DynamicRetrievalConfig_Mode.fromJson(Object? json) =>
      DynamicRetrievalConfig_Mode(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'Mode.$value';
}

/// Tool that executes code generated by the model, and automatically returns
/// the result to the model.
///
/// See also `ExecutableCode` and `CodeExecutionResult` which are only generated
/// when using this tool.
final class CodeExecution extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CodeExecution';

  CodeExecution() : super(fullyQualifiedName);

  factory CodeExecution.fromJson(Object? j) => CodeExecution();

  @override
  Object toJson() => {};

  @override
  String toString() => 'CodeExecution()';
}

/// The Tool configuration containing parameters for specifying `Tool` use
/// in the request.
final class ToolConfig extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ToolConfig';

  /// Optional. Function calling config.
  final FunctionCallingConfig? functionCallingConfig;

  ToolConfig({this.functionCallingConfig}) : super(fullyQualifiedName);

  factory ToolConfig.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ToolConfig(
      functionCallingConfig: switch (json['functionCallingConfig']) {
        null => null,
        Object $1 => FunctionCallingConfig.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (functionCallingConfig != null)
      'functionCallingConfig': functionCallingConfig!.toJson(),
  };

  @override
  String toString() => 'ToolConfig()';
}

/// Configuration for specifying function calling behavior.
final class FunctionCallingConfig extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.FunctionCallingConfig';

  /// Optional. Specifies the mode in which function calling should execute. If
  /// unspecified, the default value will be set to AUTO.
  final FunctionCallingConfig_Mode mode;

  /// Optional. A set of function names that, when provided, limits the functions
  /// the model will call.
  ///
  /// This should only be set when the Mode is ANY or VALIDATED. Function names
  /// should match [FunctionDeclaration.name]. When set, model will
  /// predict a function call from only allowed function names.
  final List<String> allowedFunctionNames;

  FunctionCallingConfig({
    this.mode = FunctionCallingConfig_Mode.$default,
    this.allowedFunctionNames = const [],
  }) : super(fullyQualifiedName);

  factory FunctionCallingConfig.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return FunctionCallingConfig(
      mode: switch (json['mode']) {
        null => FunctionCallingConfig_Mode.$default,
        Object $1 => FunctionCallingConfig_Mode.fromJson($1),
      },
      allowedFunctionNames: switch (json['allowedFunctionNames']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) decodeString(i)],
        _ => throw const FormatException(
          '"allowedFunctionNames" is not a list',
        ),
      },
    );
  }

  @override
  Object toJson() => {
    if (mode.isNotDefault) 'mode': mode.toJson(),
    if (allowedFunctionNames.isNotDefault)
      'allowedFunctionNames': allowedFunctionNames,
  };

  @override
  String toString() {
    final contents = ['mode=$mode'].join(',');
    return 'FunctionCallingConfig($contents)';
  }
}

/// Defines the execution behavior for function calling by defining the
/// execution mode.
final class FunctionCallingConfig_Mode extends ProtoEnum {
  /// Unspecified function calling mode. This value should not be used.
  static const modeUnspecified = FunctionCallingConfig_Mode('MODE_UNSPECIFIED');

  /// Default model behavior, model decides to predict either a function call
  /// or a natural language response.
  static const auto = FunctionCallingConfig_Mode('AUTO');

  /// Model is constrained to always predicting a function call only.
  /// If "allowed_function_names" are set, the predicted function call will be
  /// limited to any one of "allowed_function_names", else the predicted
  /// function call will be any one of the provided "function_declarations".
  static const any = FunctionCallingConfig_Mode('ANY');

  /// Model will not predict any function call. Model behavior is same as when
  /// not passing any function declarations.
  static const none = FunctionCallingConfig_Mode('NONE');

  /// Model decides to predict either a function call
  /// or a natural language response, but will validate function calls with
  /// constrained decoding.
  /// If "allowed_function_names" are set, the predicted function call will be
  /// limited to any one of "allowed_function_names", else the predicted
  /// function call will be any one of the provided "function_declarations".
  static const validated = FunctionCallingConfig_Mode('VALIDATED');

  /// The default value for [FunctionCallingConfig_Mode].
  static const $default = modeUnspecified;

  const FunctionCallingConfig_Mode(super.value);

  factory FunctionCallingConfig_Mode.fromJson(Object? json) =>
      FunctionCallingConfig_Mode(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'Mode.$value';
}

/// Structured representation of a function declaration as defined by the
/// [OpenAPI 3.03 specification](https://spec.openapis.org/oas/v3.0.3). Included
/// in this declaration are the function name and parameters. This
/// FunctionDeclaration is a representation of a block of code that can be used
/// as a `Tool` by the model and executed by the client.
final class FunctionDeclaration extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.FunctionDeclaration';

  /// Required. The name of the function.
  /// Must be a-z, A-Z, 0-9, or contain underscores, colons, dots, and dashes,
  /// with a maximum length of 64.
  final String name;

  /// Required. A brief description of the function.
  final String description;

  /// Optional. Describes the parameters to this function. Reflects the Open
  /// API 3.03 Parameter Object string Key: the name of the parameter. Parameter
  /// names are case sensitive. Schema Value: the Schema defining the type used
  /// for the parameter.
  final Schema? parameters;

  /// Optional. Describes the parameters to the function in JSON Schema format.
  /// The schema must describe an object where the properties are the parameters
  /// to the function. For example:
  ///
  /// ```
  /// {
  ///   "type": "object",
  ///   "properties": {
  ///     "name": { "type": "string" },
  ///     "age": { "type": "integer" }
  ///   },
  ///   "additionalProperties": false,
  ///   "required": ["name", "age"],
  ///   "propertyOrdering": ["name", "age"]
  /// }
  /// ```
  ///
  /// This field is mutually exclusive with `parameters`.
  final Value? parametersJsonSchema;

  /// Optional. Describes the output from this function in JSON Schema format.
  /// Reflects the Open API 3.03 Response Object. The Schema defines the type
  /// used for the response value of the function.
  final Schema? response;

  /// Optional. Describes the output from this function in JSON Schema format.
  /// The value specified by the schema is the response value of the function.
  ///
  /// This field is mutually exclusive with `response`.
  final Value? responseJsonSchema;

  /// Optional. Specifies the function Behavior.
  /// Currently only supported by the BidiGenerateContent method.
  final FunctionDeclaration_Behavior behavior;

  FunctionDeclaration({
    required this.name,
    required this.description,
    this.parameters,
    this.parametersJsonSchema,
    this.response,
    this.responseJsonSchema,
    this.behavior = FunctionDeclaration_Behavior.$default,
  }) : super(fullyQualifiedName);

  factory FunctionDeclaration.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return FunctionDeclaration(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
      description: switch (json['description']) {
        null => '',
        Object $1 => decodeString($1),
      },
      parameters: switch (json['parameters']) {
        null => null,
        Object $1 => Schema.fromJson($1),
      },
      parametersJsonSchema: switch (json['parametersJsonSchema']) {
        null => null,
        Object $1 => Value.fromJson($1),
      },
      response: switch (json['response']) {
        null => null,
        Object $1 => Schema.fromJson($1),
      },
      responseJsonSchema: switch (json['responseJsonSchema']) {
        null => null,
        Object $1 => Value.fromJson($1),
      },
      behavior: switch (json['behavior']) {
        null => FunctionDeclaration_Behavior.$default,
        Object $1 => FunctionDeclaration_Behavior.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    'name': name,
    'description': description,
    if (parameters != null) 'parameters': parameters!.toJson(),
    if (parametersJsonSchema != null)
      'parametersJsonSchema': parametersJsonSchema!.toJson(),
    if (response != null) 'response': response!.toJson(),
    if (responseJsonSchema != null)
      'responseJsonSchema': responseJsonSchema!.toJson(),
    if (behavior.isNotDefault) 'behavior': behavior.toJson(),
  };

  @override
  String toString() {
    final contents = [
      'name=$name',
      'description=$description',
      'behavior=$behavior',
    ].join(',');
    return 'FunctionDeclaration($contents)';
  }
}

/// Defines the function behavior. Defaults to `BLOCKING`.
final class FunctionDeclaration_Behavior extends ProtoEnum {
  /// This value is unused.
  static const unspecified = FunctionDeclaration_Behavior('UNSPECIFIED');

  /// If set, the system will wait to receive the function response before
  /// continuing the conversation.
  static const blocking = FunctionDeclaration_Behavior('BLOCKING');

  /// If set, the system will not wait to receive the function response.
  /// Instead, it will attempt to handle function responses as they become
  /// available while maintaining the conversation between the user and the
  /// model.
  static const nonBlocking = FunctionDeclaration_Behavior('NON_BLOCKING');

  /// The default value for [FunctionDeclaration_Behavior].
  static const $default = unspecified;

  const FunctionDeclaration_Behavior(super.value);

  factory FunctionDeclaration_Behavior.fromJson(Object? json) =>
      FunctionDeclaration_Behavior(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'Behavior.$value';
}

/// A predicted `FunctionCall` returned from the model that contains
/// a string representing the `FunctionDeclaration.name` with the
/// arguments and their values.
final class FunctionCall extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.FunctionCall';

  /// Optional. The unique id of the function call. If populated, the client to
  /// execute the `function_call` and return the response with the matching `id`.
  final String id;

  /// Required. The name of the function to call.
  /// Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum
  /// length of 64.
  final String name;

  /// Optional. The function parameters and values in JSON object format.
  final Struct? args;

  FunctionCall({this.id = '', required this.name, this.args})
    : super(fullyQualifiedName);

  factory FunctionCall.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return FunctionCall(
      id: switch (json['id']) {
        null => '',
        Object $1 => decodeString($1),
      },
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
      args: switch (json['args']) {
        null => null,
        Object $1 => Struct.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (id.isNotDefault) 'id': id,
    'name': name,
    if (args != null) 'args': args!.toJson(),
  };

  @override
  String toString() {
    final contents = ['id=$id', 'name=$name'].join(',');
    return 'FunctionCall($contents)';
  }
}

/// The result output from a `FunctionCall` that contains a string
/// representing the `FunctionDeclaration.name` and a structured JSON
/// object containing any output from the function is used as context to
/// the model. This should contain the result of a`FunctionCall` made
/// based on model prediction.
final class FunctionResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.FunctionResponse';

  /// Optional. The id of the function call this response is for. Populated by
  /// the client to match the corresponding function call `id`.
  final String id;

  /// Required. The name of the function to call.
  /// Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum
  /// length of 64.
  final String name;

  /// Required. The function response in JSON object format.
  /// Callers can use any keys of their choice that fit the function's syntax
  /// to return the function output, e.g. "output", "result", etc.
  /// In particular, if the function call failed to execute, the response can
  /// have an "error" key to return error details to the model.
  final Struct? response;

  /// Optional. Ordered `Parts` that constitute a function response. Parts may
  /// have different IANA MIME types.
  final List<FunctionResponsePart> parts;

  /// Optional. Signals that function call continues, and more responses will be
  /// returned, turning the function call into a generator.
  /// Is only applicable to NON_BLOCKING function calls, is ignored otherwise.
  /// If set to false, future responses will not be considered.
  /// It is allowed to return empty `response` with `will_continue=False` to
  /// signal that the function call is finished. This may still trigger the model
  /// generation. To avoid triggering the generation and finish the function
  /// call, additionally set `scheduling` to `SILENT`.
  final bool willContinue;

  /// Optional. Specifies how the response should be scheduled in the
  /// conversation. Only applicable to NON_BLOCKING function calls, is ignored
  /// otherwise. Defaults to WHEN_IDLE.
  final FunctionResponse_Scheduling? scheduling;

  FunctionResponse({
    this.id = '',
    required this.name,
    required this.response,
    this.parts = const [],
    this.willContinue = false,
    this.scheduling,
  }) : super(fullyQualifiedName);

  factory FunctionResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return FunctionResponse(
      id: switch (json['id']) {
        null => '',
        Object $1 => decodeString($1),
      },
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
      response: switch (json['response']) {
        null => null,
        Object $1 => Struct.fromJson($1),
      },
      parts: switch (json['parts']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) FunctionResponsePart.fromJson(i),
        ],
        _ => throw const FormatException('"parts" is not a list'),
      },
      willContinue: switch (json['willContinue']) {
        null => false,
        Object $1 => decodeBool($1),
      },
      scheduling: switch (json['scheduling']) {
        null => null,
        Object $1 => FunctionResponse_Scheduling.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (id.isNotDefault) 'id': id,
    'name': name,
    if (response != null) 'response': response!.toJson(),
    if (parts.isNotDefault) 'parts': encodeList(parts),
    if (willContinue.isNotDefault) 'willContinue': willContinue,
    if (scheduling != null) 'scheduling': scheduling!.toJson(),
  };

  @override
  String toString() {
    final contents = [
      'id=$id',
      'name=$name',
      'willContinue=$willContinue',
      if (scheduling != null) 'scheduling=$scheduling',
    ].join(',');
    return 'FunctionResponse($contents)';
  }
}

/// Specifies how the response should be scheduled in the conversation.
final class FunctionResponse_Scheduling extends ProtoEnum {
  /// This value is unused.
  static const schedulingUnspecified = FunctionResponse_Scheduling(
    'SCHEDULING_UNSPECIFIED',
  );

  /// Only add the result to the conversation context, do not interrupt or
  /// trigger generation.
  static const silent = FunctionResponse_Scheduling('SILENT');

  /// Add the result to the conversation context, and prompt to generate output
  /// without interrupting ongoing generation.
  static const whenIdle = FunctionResponse_Scheduling('WHEN_IDLE');

  /// Add the result to the conversation context, interrupt ongoing generation
  /// and prompt to generate output.
  static const interrupt = FunctionResponse_Scheduling('INTERRUPT');

  /// The default value for [FunctionResponse_Scheduling].
  static const $default = schedulingUnspecified;

  const FunctionResponse_Scheduling(super.value);

  factory FunctionResponse_Scheduling.fromJson(Object? json) =>
      FunctionResponse_Scheduling(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'Scheduling.$value';
}

/// The `Schema` object allows the definition of input and output data types.
/// These types can be objects, but also primitives and arrays.
/// Represents a select subset of an [OpenAPI 3.0 schema
/// object](https://spec.openapis.org/oas/v3.0.3#schema).
final class Schema extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Schema';

  /// Required. Data type.
  final Type type;

  /// Optional. The format of the data. Any value is allowed, but most do not
  /// trigger any special functionality.
  final String format;

  /// Optional. The title of the schema.
  final String title;

  /// Optional. A brief description of the parameter. This could contain examples
  /// of use. Parameter description may be formatted as Markdown.
  final String description;

  /// Optional. Indicates if the value may be null.
  final bool nullable;

  /// Optional. Possible values of the element of Type.STRING with enum format.
  /// For example we can define an Enum Direction as :
  /// {type:STRING, format:enum, enum:["EAST", NORTH", "SOUTH", "WEST"]}
  final List<String> enum$;

  /// Optional. Schema of the elements of Type.ARRAY.
  final Schema? items;

  /// Optional. Maximum number of the elements for Type.ARRAY.
  final int maxItems;

  /// Optional. Minimum number of the elements for Type.ARRAY.
  final int minItems;

  /// Optional. Properties of Type.OBJECT.
  final Map<String, Schema> properties;

  /// Optional. Required properties of Type.OBJECT.
  final List<String> required;

  /// Optional. Minimum number of the properties for Type.OBJECT.
  final int minProperties;

  /// Optional. Maximum number of the properties for Type.OBJECT.
  final int maxProperties;

  /// Optional. SCHEMA FIELDS FOR TYPE INTEGER and NUMBER
  /// Minimum value of the Type.INTEGER and Type.NUMBER
  final double? minimum;

  /// Optional. Maximum value of the Type.INTEGER and Type.NUMBER
  final double? maximum;

  /// Optional. SCHEMA FIELDS FOR TYPE STRING
  /// Minimum length of the Type.STRING
  final int minLength;

  /// Optional. Maximum length of the Type.STRING
  final int maxLength;

  /// Optional. Pattern of the Type.STRING to restrict a string to a regular
  /// expression.
  final String pattern;

  /// Optional. Example of the object. Will only populated when the object is the
  /// root.
  final Value? example;

  /// Optional. The value should be validated against any (one or more) of the
  /// subschemas in the list.
  final List<Schema> anyOf;

  /// Optional. The order of the properties.
  /// Not a standard field in open api spec. Used to determine the order of the
  /// properties in the response.
  final List<String> propertyOrdering;

  /// Optional. Default value of the field. Per JSON Schema, this field is
  /// intended for documentation generators and doesn't affect validation. Thus
  /// it's included here and ignored so that developers who send schemas with a
  /// `default` field don't get unknown-field errors.
  final Value? default$;

  Schema({
    required this.type,
    this.format = '',
    this.title = '',
    this.description = '',
    this.nullable = false,
    this.enum$ = const [],
    this.items,
    this.maxItems = 0,
    this.minItems = 0,
    this.properties = const {},
    this.required = const [],
    this.minProperties = 0,
    this.maxProperties = 0,
    this.minimum,
    this.maximum,
    this.minLength = 0,
    this.maxLength = 0,
    this.pattern = '',
    this.example,
    this.anyOf = const [],
    this.propertyOrdering = const [],
    this.default$,
  }) : super(fullyQualifiedName);

  factory Schema.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Schema(
      type: switch (json['type']) {
        null => Type.$default,
        Object $1 => Type.fromJson($1),
      },
      format: switch (json['format']) {
        null => '',
        Object $1 => decodeString($1),
      },
      title: switch (json['title']) {
        null => '',
        Object $1 => decodeString($1),
      },
      description: switch (json['description']) {
        null => '',
        Object $1 => decodeString($1),
      },
      nullable: switch (json['nullable']) {
        null => false,
        Object $1 => decodeBool($1),
      },
      enum$: switch (json['enum']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) decodeString(i)],
        _ => throw const FormatException('"enum" is not a list'),
      },
      items: switch (json['items']) {
        null => null,
        Object $1 => Schema.fromJson($1),
      },
      maxItems: switch (json['maxItems']) {
        null => 0,
        Object $1 => decodeInt64($1),
      },
      minItems: switch (json['minItems']) {
        null => 0,
        Object $1 => decodeInt64($1),
      },
      properties: switch (json['properties']) {
        null => {},
        Map<String, Object?> $1 => {
          for (final e in $1.entries)
            decodeString(e.key): Schema.fromJson(e.value),
        },
        _ => throw const FormatException('"properties" is not an object'),
      },
      required: switch (json['required']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) decodeString(i)],
        _ => throw const FormatException('"required" is not a list'),
      },
      minProperties: switch (json['minProperties']) {
        null => 0,
        Object $1 => decodeInt64($1),
      },
      maxProperties: switch (json['maxProperties']) {
        null => 0,
        Object $1 => decodeInt64($1),
      },
      minimum: switch (json['minimum']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      maximum: switch (json['maximum']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      minLength: switch (json['minLength']) {
        null => 0,
        Object $1 => decodeInt64($1),
      },
      maxLength: switch (json['maxLength']) {
        null => 0,
        Object $1 => decodeInt64($1),
      },
      pattern: switch (json['pattern']) {
        null => '',
        Object $1 => decodeString($1),
      },
      example: switch (json['example']) {
        null => null,
        Object $1 => Value.fromJson($1),
      },
      anyOf: switch (json['anyOf']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Schema.fromJson(i)],
        _ => throw const FormatException('"anyOf" is not a list'),
      },
      propertyOrdering: switch (json['propertyOrdering']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) decodeString(i)],
        _ => throw const FormatException('"propertyOrdering" is not a list'),
      },
      default$: switch (json['default']) {
        null => null,
        Object $1 => Value.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    'type': type.toJson(),
    if (format.isNotDefault) 'format': format,
    if (title.isNotDefault) 'title': title,
    if (description.isNotDefault) 'description': description,
    if (nullable.isNotDefault) 'nullable': nullable,
    if (enum$.isNotDefault) 'enum': enum$,
    if (items != null) 'items': items!.toJson(),
    if (maxItems.isNotDefault) 'maxItems': encodeInt64(maxItems),
    if (minItems.isNotDefault) 'minItems': encodeInt64(minItems),
    if (properties.isNotDefault) 'properties': encodeMap(properties),
    if (required.isNotDefault) 'required': required,
    if (minProperties.isNotDefault) 'minProperties': encodeInt64(minProperties),
    if (maxProperties.isNotDefault) 'maxProperties': encodeInt64(maxProperties),
    if (minimum != null) 'minimum': encodeDouble(minimum),
    if (maximum != null) 'maximum': encodeDouble(maximum),
    if (minLength.isNotDefault) 'minLength': encodeInt64(minLength),
    if (maxLength.isNotDefault) 'maxLength': encodeInt64(maxLength),
    if (pattern.isNotDefault) 'pattern': pattern,
    if (example != null) 'example': example!.toJson(),
    if (anyOf.isNotDefault) 'anyOf': encodeList(anyOf),
    if (propertyOrdering.isNotDefault) 'propertyOrdering': propertyOrdering,
    if (default$ != null) 'default': default$!.toJson(),
  };

  @override
  String toString() {
    final contents = [
      'type=$type',
      'format=$format',
      'title=$title',
      'description=$description',
      'nullable=$nullable',
      'maxItems=$maxItems',
      'minItems=$minItems',
      'minProperties=$minProperties',
      'maxProperties=$maxProperties',
      if (minimum != null) 'minimum=$minimum',
      if (maximum != null) 'maximum=$maximum',
      'minLength=$minLength',
      'maxLength=$maxLength',
      'pattern=$pattern',
    ].join(',');
    return 'Schema($contents)';
  }
}

/// Passage included inline with a grounding configuration.
final class GroundingPassage extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GroundingPassage';

  /// Identifier for the passage for attributing this passage in grounded
  /// answers.
  final String id;

  /// Content of the passage.
  final Content? content;

  GroundingPassage({this.id = '', this.content}) : super(fullyQualifiedName);

  factory GroundingPassage.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GroundingPassage(
      id: switch (json['id']) {
        null => '',
        Object $1 => decodeString($1),
      },
      content: switch (json['content']) {
        null => null,
        Object $1 => Content.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (id.isNotDefault) 'id': id,
    if (content != null) 'content': content!.toJson(),
  };

  @override
  String toString() {
    final contents = ['id=$id'].join(',');
    return 'GroundingPassage($contents)';
  }
}

/// A repeated list of passages.
final class GroundingPassages extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GroundingPassages';

  /// List of passages.
  final List<GroundingPassage> passages;

  GroundingPassages({this.passages = const []}) : super(fullyQualifiedName);

  factory GroundingPassages.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GroundingPassages(
      passages: switch (json['passages']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) GroundingPassage.fromJson(i)],
        _ => throw const FormatException('"passages" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (passages.isNotDefault) 'passages': encodeList(passages),
  };

  @override
  String toString() => 'GroundingPassages()';
}

/// Represents token counting info for a single modality.
final class ModalityTokenCount extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ModalityTokenCount';

  /// The modality associated with this token count.
  final Modality modality;

  /// Number of tokens.
  final int tokenCount;

  ModalityTokenCount({this.modality = Modality.$default, this.tokenCount = 0})
    : super(fullyQualifiedName);

  factory ModalityTokenCount.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ModalityTokenCount(
      modality: switch (json['modality']) {
        null => Modality.$default,
        Object $1 => Modality.fromJson($1),
      },
      tokenCount: switch (json['tokenCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (modality.isNotDefault) 'modality': modality.toJson(),
    if (tokenCount.isNotDefault) 'tokenCount': tokenCount,
  };

  @override
  String toString() {
    final contents = ['modality=$modality', 'tokenCount=$tokenCount'].join(',');
    return 'ModalityTokenCount($contents)';
  }
}

/// Request to generate a message response from the model.
final class GenerateMessageRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GenerateMessageRequest';

  /// Required. The name of the model to use.
  ///
  /// Format: `name=models/{model}`.
  final String model;

  /// Required. The structured textual input given to the model as a prompt.
  ///
  /// Given a
  /// prompt, the model will return what it predicts is the next message in the
  /// discussion.
  final MessagePrompt? prompt;

  /// Optional. Controls the randomness of the output.
  ///
  /// Values can range over `[0.0,1.0]`,
  /// inclusive. A value closer to `1.0` will produce responses that are more
  /// varied, while a value closer to `0.0` will typically result in
  /// less surprising responses from the model.
  final double? temperature;

  /// Optional. The number of generated response messages to return.
  ///
  /// This value must be between
  /// `[1, 8]`, inclusive. If unset, this will default to `1`.
  final int? candidateCount;

  /// Optional. The maximum cumulative probability of tokens to consider when
  /// sampling.
  ///
  /// The model uses combined Top-k and nucleus sampling.
  ///
  /// Nucleus sampling considers the smallest set of tokens whose probability
  /// sum is at least `top_p`.
  final double? topP;

  /// Optional. The maximum number of tokens to consider when sampling.
  ///
  /// The model uses combined Top-k and nucleus sampling.
  ///
  /// Top-k sampling considers the set of `top_k` most probable tokens.
  final int? topK;

  GenerateMessageRequest({
    required this.model,
    required this.prompt,
    this.temperature,
    this.candidateCount,
    this.topP,
    this.topK,
  }) : super(fullyQualifiedName);

  factory GenerateMessageRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GenerateMessageRequest(
      model: switch (json['model']) {
        null => '',
        Object $1 => decodeString($1),
      },
      prompt: switch (json['prompt']) {
        null => null,
        Object $1 => MessagePrompt.fromJson($1),
      },
      temperature: switch (json['temperature']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      candidateCount: switch (json['candidateCount']) {
        null => null,
        Object $1 => decodeInt($1),
      },
      topP: switch (json['topP']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      topK: switch (json['topK']) {
        null => null,
        Object $1 => decodeInt($1),
      },
    );
  }

  @override
  Object toJson() => {
    'model': model,
    if (prompt != null) 'prompt': prompt!.toJson(),
    if (temperature != null) 'temperature': encodeDouble(temperature),
    if (candidateCount != null) 'candidateCount': candidateCount,
    if (topP != null) 'topP': encodeDouble(topP),
    if (topK != null) 'topK': topK,
  };

  @override
  String toString() {
    final contents = [
      'model=$model',
      if (temperature != null) 'temperature=$temperature',
      if (candidateCount != null) 'candidateCount=$candidateCount',
      if (topP != null) 'topP=$topP',
      if (topK != null) 'topK=$topK',
    ].join(',');
    return 'GenerateMessageRequest($contents)';
  }
}

/// The response from the model.
///
/// This includes candidate messages and
/// conversation history in the form of chronologically-ordered messages.
final class GenerateMessageResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GenerateMessageResponse';

  /// Candidate response messages from the model.
  final List<Message> candidates;

  /// The conversation history used by the model.
  final List<Message> messages;

  /// A set of content filtering metadata for the prompt and response
  /// text.
  ///
  /// This indicates which `SafetyCategory`(s) blocked a
  /// candidate from this response, the lowest `HarmProbability`
  /// that triggered a block, and the HarmThreshold setting for that category.
  final List<ContentFilter> filters;

  GenerateMessageResponse({
    this.candidates = const [],
    this.messages = const [],
    this.filters = const [],
  }) : super(fullyQualifiedName);

  factory GenerateMessageResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GenerateMessageResponse(
      candidates: switch (json['candidates']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Message.fromJson(i)],
        _ => throw const FormatException('"candidates" is not a list'),
      },
      messages: switch (json['messages']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Message.fromJson(i)],
        _ => throw const FormatException('"messages" is not a list'),
      },
      filters: switch (json['filters']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) ContentFilter.fromJson(i)],
        _ => throw const FormatException('"filters" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (candidates.isNotDefault) 'candidates': encodeList(candidates),
    if (messages.isNotDefault) 'messages': encodeList(messages),
    if (filters.isNotDefault) 'filters': encodeList(filters),
  };

  @override
  String toString() => 'GenerateMessageResponse()';
}

/// The base unit of structured text.
///
/// A `Message` includes an `author` and the `content` of
/// the `Message`.
///
/// The `author` is used to tag messages when they are fed to the
/// model as text.
final class Message extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Message';

  /// Optional. The author of this Message.
  ///
  /// This serves as a key for tagging
  /// the content of this Message when it is fed to the model as text.
  ///
  /// The author can be any alphanumeric string.
  final String author;

  /// Required. The text content of the structured `Message`.
  final String content;

  /// Output only. Citation information for model-generated `content` in this
  /// `Message`.
  ///
  /// If this `Message` was generated as output from the model, this field may be
  /// populated with attribution information for any text included in the
  /// `content`. This field is used only on output.
  final CitationMetadata? citationMetadata;

  Message({this.author = '', required this.content, this.citationMetadata})
    : super(fullyQualifiedName);

  factory Message.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Message(
      author: switch (json['author']) {
        null => '',
        Object $1 => decodeString($1),
      },
      content: switch (json['content']) {
        null => '',
        Object $1 => decodeString($1),
      },
      citationMetadata: switch (json['citationMetadata']) {
        null => null,
        Object $1 => CitationMetadata.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (author.isNotDefault) 'author': author,
    'content': content,
    if (citationMetadata != null)
      'citationMetadata': citationMetadata!.toJson(),
  };

  @override
  String toString() {
    final contents = ['author=$author', 'content=$content'].join(',');
    return 'Message($contents)';
  }
}

/// All of the structured input text passed to the model as a prompt.
///
/// A `MessagePrompt` contains a structured set of fields that provide context
/// for the conversation, examples of user input/model output message pairs that
/// prime the model to respond in different ways, and the conversation history
/// or list of messages representing the alternating turns of the conversation
/// between the user and the model.
final class MessagePrompt extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.MessagePrompt';

  /// Optional. Text that should be provided to the model first to ground the
  /// response.
  ///
  /// If not empty, this `context` will be given to the model first before the
  /// `examples` and `messages`. When using a `context` be sure to provide it
  /// with every request to maintain continuity.
  ///
  /// This field can be a description of your prompt to the model to help provide
  /// context and guide the responses. Examples: "Translate the phrase from
  /// English to French." or "Given a statement, classify the sentiment as happy,
  /// sad or neutral."
  ///
  /// Anything included in this field will take precedence over message history
  /// if the total input size exceeds the model's `input_token_limit` and the
  /// input request is truncated.
  final String context;

  /// Optional. Examples of what the model should generate.
  ///
  /// This includes both user input and the response that the model should
  /// emulate.
  ///
  /// These `examples` are treated identically to conversation messages except
  /// that they take precedence over the history in `messages`:
  /// If the total input size exceeds the model's `input_token_limit` the input
  /// will be truncated. Items will be dropped from `messages` before `examples`.
  final List<Example> examples;

  /// Required. A snapshot of the recent conversation history sorted
  /// chronologically.
  ///
  /// Turns alternate between two authors.
  ///
  /// If the total input size exceeds the model's `input_token_limit` the input
  /// will be truncated: The oldest items will be dropped from `messages`.
  final List<Message> messages;

  MessagePrompt({
    this.context = '',
    this.examples = const [],
    required this.messages,
  }) : super(fullyQualifiedName);

  factory MessagePrompt.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return MessagePrompt(
      context: switch (json['context']) {
        null => '',
        Object $1 => decodeString($1),
      },
      examples: switch (json['examples']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Example.fromJson(i)],
        _ => throw const FormatException('"examples" is not a list'),
      },
      messages: switch (json['messages']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Message.fromJson(i)],
        _ => throw const FormatException('"messages" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (context.isNotDefault) 'context': context,
    if (examples.isNotDefault) 'examples': encodeList(examples),
    'messages': encodeList(messages),
  };

  @override
  String toString() {
    final contents = ['context=$context'].join(',');
    return 'MessagePrompt($contents)';
  }
}

/// An input/output example used to instruct the Model.
///
/// It demonstrates how the model should respond or format its response.
final class Example extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Example';

  /// Required. An example of an input `Message` from the user.
  final Message? input;

  /// Required. An example of what the model should output given the input.
  final Message? output;

  Example({required this.input, required this.output})
    : super(fullyQualifiedName);

  factory Example.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Example(
      input: switch (json['input']) {
        null => null,
        Object $1 => Message.fromJson($1),
      },
      output: switch (json['output']) {
        null => null,
        Object $1 => Message.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (input != null) 'input': input!.toJson(),
    if (output != null) 'output': output!.toJson(),
  };

  @override
  String toString() => 'Example()';
}

/// Counts the number of tokens in the `prompt` sent to a model.
///
/// Models may tokenize text differently, so each model may return a different
/// `token_count`.
final class CountMessageTokensRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CountMessageTokensRequest';

  /// Required. The model's resource name. This serves as an ID for the Model to
  /// use.
  ///
  /// This name should match a model name returned by the `ListModels` method.
  ///
  /// Format: `models/{model}`
  final String model;

  /// Required. The prompt, whose token count is to be returned.
  final MessagePrompt? prompt;

  CountMessageTokensRequest({required this.model, required this.prompt})
    : super(fullyQualifiedName);

  factory CountMessageTokensRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CountMessageTokensRequest(
      model: switch (json['model']) {
        null => '',
        Object $1 => decodeString($1),
      },
      prompt: switch (json['prompt']) {
        null => null,
        Object $1 => MessagePrompt.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    'model': model,
    if (prompt != null) 'prompt': prompt!.toJson(),
  };

  @override
  String toString() {
    final contents = ['model=$model'].join(',');
    return 'CountMessageTokensRequest($contents)';
  }
}

/// A response from `CountMessageTokens`.
///
/// It returns the model's `token_count` for the `prompt`.
final class CountMessageTokensResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CountMessageTokensResponse';

  /// The number of tokens that the `model` tokenizes the `prompt` into.
  ///
  /// Always non-negative.
  final int tokenCount;

  CountMessageTokensResponse({this.tokenCount = 0}) : super(fullyQualifiedName);

  factory CountMessageTokensResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CountMessageTokensResponse(
      tokenCount: switch (json['tokenCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
    );
  }

  @override
  Object toJson() => {if (tokenCount.isNotDefault) 'tokenCount': tokenCount};

  @override
  String toString() {
    final contents = ['tokenCount=$tokenCount'].join(',');
    return 'CountMessageTokensResponse($contents)';
  }
}

/// A file uploaded to the API.
/// Next ID: 15
final class File extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.File';

  /// Output only. Metadata for a video.
  final VideoFileMetadata? videoMetadata;

  /// Immutable. Identifier. The `File` resource name. The ID (name excluding the
  /// "files/" prefix) can contain up to 40 characters that are lowercase
  /// alphanumeric or dashes (-). The ID cannot start or end with a dash. If the
  /// name is empty on create, a unique name will be generated. Example:
  /// `files/123-456`
  final String name;

  /// Optional. The human-readable display name for the `File`. The display name
  /// must be no more than 512 characters in length, including spaces. Example:
  /// "Welcome Image"
  final String displayName;

  /// Output only. MIME type of the file.
  final String mimeType;

  /// Output only. Size of the file in bytes.
  final int sizeBytes;

  /// Output only. The timestamp of when the `File` was created.
  final Timestamp? createTime;

  /// Output only. The timestamp of when the `File` was last updated.
  final Timestamp? updateTime;

  /// Output only. The timestamp of when the `File` will be deleted. Only set if
  /// the `File` is scheduled to expire.
  final Timestamp? expirationTime;

  /// Output only. SHA-256 hash of the uploaded bytes.
  final Uint8List sha256Hash;

  /// Output only. The uri of the `File`.
  final String uri;

  /// Output only. The download uri of the `File`.
  final String downloadUri;

  /// Output only. Processing state of the File.
  final File_State state;

  /// Source of the File.
  final File_Source source;

  /// Output only. Error status if File processing failed.
  final Status? error;

  File({
    this.videoMetadata,
    this.name = '',
    this.displayName = '',
    this.mimeType = '',
    this.sizeBytes = 0,
    this.createTime,
    this.updateTime,
    this.expirationTime,
    Uint8List? sha256Hash,
    this.uri = '',
    this.downloadUri = '',
    this.state = File_State.$default,
    this.source = File_Source.$default,
    this.error,
  }) : sha256Hash = sha256Hash ?? Uint8List(0),
       super(fullyQualifiedName);

  factory File.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return File(
      videoMetadata: switch (json['videoMetadata']) {
        null => null,
        Object $1 => VideoFileMetadata.fromJson($1),
      },
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
      displayName: switch (json['displayName']) {
        null => '',
        Object $1 => decodeString($1),
      },
      mimeType: switch (json['mimeType']) {
        null => '',
        Object $1 => decodeString($1),
      },
      sizeBytes: switch (json['sizeBytes']) {
        null => 0,
        Object $1 => decodeInt64($1),
      },
      createTime: switch (json['createTime']) {
        null => null,
        Object $1 => Timestamp.fromJson($1),
      },
      updateTime: switch (json['updateTime']) {
        null => null,
        Object $1 => Timestamp.fromJson($1),
      },
      expirationTime: switch (json['expirationTime']) {
        null => null,
        Object $1 => Timestamp.fromJson($1),
      },
      sha256Hash: switch (json['sha256Hash']) {
        null => Uint8List(0),
        Object $1 => decodeBytes($1),
      },
      uri: switch (json['uri']) {
        null => '',
        Object $1 => decodeString($1),
      },
      downloadUri: switch (json['downloadUri']) {
        null => '',
        Object $1 => decodeString($1),
      },
      state: switch (json['state']) {
        null => File_State.$default,
        Object $1 => File_State.fromJson($1),
      },
      source: switch (json['source']) {
        null => File_Source.$default,
        Object $1 => File_Source.fromJson($1),
      },
      error: switch (json['error']) {
        null => null,
        Object $1 => Status.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (videoMetadata != null) 'videoMetadata': videoMetadata!.toJson(),
    if (name.isNotDefault) 'name': name,
    if (displayName.isNotDefault) 'displayName': displayName,
    if (mimeType.isNotDefault) 'mimeType': mimeType,
    if (sizeBytes.isNotDefault) 'sizeBytes': encodeInt64(sizeBytes),
    if (createTime != null) 'createTime': createTime!.toJson(),
    if (updateTime != null) 'updateTime': updateTime!.toJson(),
    if (expirationTime != null) 'expirationTime': expirationTime!.toJson(),
    if (sha256Hash.isNotDefault) 'sha256Hash': encodeBytes(sha256Hash),
    if (uri.isNotDefault) 'uri': uri,
    if (downloadUri.isNotDefault) 'downloadUri': downloadUri,
    if (state.isNotDefault) 'state': state.toJson(),
    if (source.isNotDefault) 'source': source.toJson(),
    if (error != null) 'error': error!.toJson(),
  };

  @override
  String toString() {
    final contents = [
      'name=$name',
      'displayName=$displayName',
      'mimeType=$mimeType',
      'sizeBytes=$sizeBytes',
      'sha256Hash=$sha256Hash',
      'uri=$uri',
      'downloadUri=$downloadUri',
      'state=$state',
      'source=$source',
    ].join(',');
    return 'File($contents)';
  }
}

/// States for the lifecycle of a File.
final class File_State extends ProtoEnum {
  /// The default value. This value is used if the state is omitted.
  static const stateUnspecified = File_State('STATE_UNSPECIFIED');

  /// File is being processed and cannot be used for inference yet.
  static const processing = File_State('PROCESSING');

  /// File is processed and available for inference.
  static const active = File_State('ACTIVE');

  /// File failed processing.
  static const failed = File_State('FAILED');

  /// The default value for [File_State].
  static const $default = stateUnspecified;

  const File_State(super.value);

  factory File_State.fromJson(Object? json) => File_State(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'State.$value';
}

final class File_Source extends ProtoEnum {
  /// Used if source is not specified.
  static const sourceUnspecified = File_Source('SOURCE_UNSPECIFIED');

  /// Indicates the file is uploaded by the user.
  static const uploaded = File_Source('UPLOADED');

  /// Indicates the file is generated by Google.
  static const generated = File_Source('GENERATED');

  /// Indicates the file is a registered, i.e. a Google Cloud Storage file.
  static const registered = File_Source('REGISTERED');

  /// The default value for [File_Source].
  static const $default = sourceUnspecified;

  const File_Source(super.value);

  factory File_Source.fromJson(Object? json) => File_Source(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'Source.$value';
}

/// Metadata for a video `File`.
final class VideoFileMetadata extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.VideoFileMetadata';

  /// Duration of the video.
  final Duration? videoDuration;

  VideoFileMetadata({this.videoDuration}) : super(fullyQualifiedName);

  factory VideoFileMetadata.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return VideoFileMetadata(
      videoDuration: switch (json['videoDuration']) {
        null => null,
        Object $1 => Duration.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (videoDuration != null) 'videoDuration': videoDuration!.toJson(),
  };

  @override
  String toString() => 'VideoFileMetadata()';
}

/// Request for `CreateFile`.
final class CreateFileRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CreateFileRequest';

  /// Optional. Metadata for the file to create.
  final File? file;

  CreateFileRequest({this.file}) : super(fullyQualifiedName);

  factory CreateFileRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CreateFileRequest(
      file: switch (json['file']) {
        null => null,
        Object $1 => File.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {if (file != null) 'file': file!.toJson()};

  @override
  String toString() => 'CreateFileRequest()';
}

/// Response for `CreateFile`.
final class CreateFileResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CreateFileResponse';

  /// Metadata for the created file.
  final File? file;

  CreateFileResponse({this.file}) : super(fullyQualifiedName);

  factory CreateFileResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CreateFileResponse(
      file: switch (json['file']) {
        null => null,
        Object $1 => File.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {if (file != null) 'file': file!.toJson()};

  @override
  String toString() => 'CreateFileResponse()';
}

/// Request for `ListFiles`.
final class ListFilesRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ListFilesRequest';

  /// Optional. Maximum number of `File`s to return per page.
  /// If unspecified, defaults to 10. Maximum `page_size` is 100.
  final int pageSize;

  /// Optional. A page token from a previous `ListFiles` call.
  final String pageToken;

  ListFilesRequest({this.pageSize = 0, this.pageToken = ''})
    : super(fullyQualifiedName);

  factory ListFilesRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ListFilesRequest(
      pageSize: switch (json['pageSize']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      pageToken: switch (json['pageToken']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (pageSize.isNotDefault) 'pageSize': pageSize,
    if (pageToken.isNotDefault) 'pageToken': pageToken,
  };

  @override
  String toString() {
    final contents = ['pageSize=$pageSize', 'pageToken=$pageToken'].join(',');
    return 'ListFilesRequest($contents)';
  }
}

/// Response for `ListFiles`.
final class ListFilesResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ListFilesResponse';

  /// The list of `File`s.
  final List<File> files;

  /// A token that can be sent as a `page_token` into a subsequent `ListFiles`
  /// call.
  final String nextPageToken;

  ListFilesResponse({this.files = const [], this.nextPageToken = ''})
    : super(fullyQualifiedName);

  factory ListFilesResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ListFilesResponse(
      files: switch (json['files']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) File.fromJson(i)],
        _ => throw const FormatException('"files" is not a list'),
      },
      nextPageToken: switch (json['nextPageToken']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (files.isNotDefault) 'files': encodeList(files),
    if (nextPageToken.isNotDefault) 'nextPageToken': nextPageToken,
  };

  @override
  String toString() {
    final contents = ['nextPageToken=$nextPageToken'].join(',');
    return 'ListFilesResponse($contents)';
  }
}

/// Request for `GetFile`.
final class GetFileRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GetFileRequest';

  /// Required. The name of the `File` to get.
  /// Example: `files/abc-123`
  final String name;

  GetFileRequest({required this.name}) : super(fullyQualifiedName);

  factory GetFileRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GetFileRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {'name': name};

  @override
  String toString() {
    final contents = ['name=$name'].join(',');
    return 'GetFileRequest($contents)';
  }
}

/// Request for `DeleteFile`.
final class DeleteFileRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.DeleteFileRequest';

  /// Required. The name of the `File` to delete.
  /// Example: `files/abc-123`
  final String name;

  DeleteFileRequest({required this.name}) : super(fullyQualifiedName);

  factory DeleteFileRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return DeleteFileRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {'name': name};

  @override
  String toString() {
    final contents = ['name=$name'].join(',');
    return 'DeleteFileRequest($contents)';
  }
}

/// Request for `DownloadFile`.
final class DownloadFileRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.DownloadFileRequest';

  /// Required. The name of the `File` to download.
  /// Example: `files/abc-123`
  final String name;

  DownloadFileRequest({required this.name}) : super(fullyQualifiedName);

  factory DownloadFileRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return DownloadFileRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {'name': name};

  @override
  String toString() {
    final contents = ['name=$name'].join(',');
    return 'DownloadFileRequest($contents)';
  }
}

/// Response for `DownloadFile`.
final class DownloadFileResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.DownloadFileResponse';

  DownloadFileResponse() : super(fullyQualifiedName);

  factory DownloadFileResponse.fromJson(Object? j) => DownloadFileResponse();

  @override
  Object toJson() => {};

  @override
  String toString() => 'DownloadFileResponse()';
}

/// Request to generate a completion from the model.
/// NEXT ID: 18
final class GenerateContentRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GenerateContentRequest';

  /// Required. The name of the `Model` to use for generating the completion.
  ///
  /// Format: `models/{model}`.
  final String model;

  /// Optional. Developer set [system
  /// instruction(s)](https://ai.google.dev/gemini-api/docs/system-instructions).
  /// Currently, text only.
  final Content? systemInstruction;

  /// Required. The content of the current conversation with the model.
  ///
  /// For single-turn queries, this is a single instance. For multi-turn queries
  /// like [chat](https://ai.google.dev/gemini-api/docs/text-generation#chat),
  /// this is a repeated field that contains the conversation history and the
  /// latest request.
  final List<Content> contents;

  /// Optional. A list of `Tools` the `Model` may use to generate the next
  /// response.
  ///
  /// A `Tool` is a piece of code that enables the system to interact with
  /// external systems to perform an action, or set of actions, outside of
  /// knowledge and scope of the `Model`. Supported `Tool`s are `Function` and
  /// `code_execution`. Refer to the [Function
  /// calling](https://ai.google.dev/gemini-api/docs/function-calling) and the
  /// [Code execution](https://ai.google.dev/gemini-api/docs/code-execution)
  /// guides to learn more.
  final List<Tool> tools;

  /// Optional. Tool configuration for any `Tool` specified in the request. Refer
  /// to the [Function calling
  /// guide](https://ai.google.dev/gemini-api/docs/function-calling#function_calling_mode)
  /// for a usage example.
  final ToolConfig? toolConfig;

  /// Optional. A list of unique `SafetySetting` instances for blocking unsafe
  /// content.
  ///
  /// This will be enforced on the `GenerateContentRequest.contents` and
  /// `GenerateContentResponse.candidates`. There should not be more than one
  /// setting for each `SafetyCategory` type. The API will block any contents and
  /// responses that fail to meet the thresholds set by these settings. This list
  /// overrides the default settings for each `SafetyCategory` specified in the
  /// safety_settings. If there is no `SafetySetting` for a given
  /// `SafetyCategory` provided in the list, the API will use the default safety
  /// setting for that category. Harm categories HARM_CATEGORY_HATE_SPEECH,
  /// HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT,
  /// HARM_CATEGORY_HARASSMENT, HARM_CATEGORY_CIVIC_INTEGRITY are supported.
  /// Refer to the [guide](https://ai.google.dev/gemini-api/docs/safety-settings)
  /// for detailed information on available safety settings. Also refer to the
  /// [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance) to
  /// learn how to incorporate safety considerations in your AI applications.
  final List<SafetySetting> safetySettings;

  /// Optional. Configuration options for model generation and outputs.
  final GenerationConfig? generationConfig;

  /// Optional. The name of the content
  /// [cached](https://ai.google.dev/gemini-api/docs/caching) to use as context
  /// to serve the prediction. Format: `cachedContents/{cachedContent}`
  final String? cachedContent;

  GenerateContentRequest({
    required this.model,
    this.systemInstruction,
    required this.contents,
    this.tools = const [],
    this.toolConfig,
    this.safetySettings = const [],
    this.generationConfig,
    this.cachedContent,
  }) : super(fullyQualifiedName);

  factory GenerateContentRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GenerateContentRequest(
      model: switch (json['model']) {
        null => '',
        Object $1 => decodeString($1),
      },
      systemInstruction: switch (json['systemInstruction']) {
        null => null,
        Object $1 => Content.fromJson($1),
      },
      contents: switch (json['contents']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Content.fromJson(i)],
        _ => throw const FormatException('"contents" is not a list'),
      },
      tools: switch (json['tools']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Tool.fromJson(i)],
        _ => throw const FormatException('"tools" is not a list'),
      },
      toolConfig: switch (json['toolConfig']) {
        null => null,
        Object $1 => ToolConfig.fromJson($1),
      },
      safetySettings: switch (json['safetySettings']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) SafetySetting.fromJson(i)],
        _ => throw const FormatException('"safetySettings" is not a list'),
      },
      generationConfig: switch (json['generationConfig']) {
        null => null,
        Object $1 => GenerationConfig.fromJson($1),
      },
      cachedContent: switch (json['cachedContent']) {
        null => null,
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    'model': model,
    if (systemInstruction != null)
      'systemInstruction': systemInstruction!.toJson(),
    'contents': encodeList(contents),
    if (tools.isNotDefault) 'tools': encodeList(tools),
    if (toolConfig != null) 'toolConfig': toolConfig!.toJson(),
    if (safetySettings.isNotDefault)
      'safetySettings': encodeList(safetySettings),
    if (generationConfig != null)
      'generationConfig': generationConfig!.toJson(),
    if (cachedContent != null) 'cachedContent': cachedContent,
  };

  @override
  String toString() {
    final contents = [
      'model=$model',
      if (cachedContent != null) 'cachedContent=$cachedContent',
    ].join(',');
    return 'GenerateContentRequest($contents)';
  }
}

/// The configuration for the prebuilt speaker to use.
final class PrebuiltVoiceConfig extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.PrebuiltVoiceConfig';

  /// The name of the preset voice to use.
  final String? voiceName;

  PrebuiltVoiceConfig({this.voiceName}) : super(fullyQualifiedName);

  factory PrebuiltVoiceConfig.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return PrebuiltVoiceConfig(
      voiceName: switch (json['voiceName']) {
        null => null,
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {if (voiceName != null) 'voiceName': voiceName};

  @override
  String toString() {
    final contents = [if (voiceName != null) 'voiceName=$voiceName'].join(',');
    return 'PrebuiltVoiceConfig($contents)';
  }
}

/// The configuration for the voice to use.
final class VoiceConfig extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.VoiceConfig';

  /// The configuration for the prebuilt voice to use.
  final PrebuiltVoiceConfig? prebuiltVoiceConfig;

  VoiceConfig({this.prebuiltVoiceConfig}) : super(fullyQualifiedName);

  factory VoiceConfig.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return VoiceConfig(
      prebuiltVoiceConfig: switch (json['prebuiltVoiceConfig']) {
        null => null,
        Object $1 => PrebuiltVoiceConfig.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (prebuiltVoiceConfig != null)
      'prebuiltVoiceConfig': prebuiltVoiceConfig!.toJson(),
  };

  @override
  String toString() => 'VoiceConfig()';
}

/// The configuration for a single speaker in a multi speaker setup.
final class SpeakerVoiceConfig extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.SpeakerVoiceConfig';

  /// Required. The name of the speaker to use. Should be the same as in the
  /// prompt.
  final String speaker;

  /// Required. The configuration for the voice to use.
  final VoiceConfig? voiceConfig;

  SpeakerVoiceConfig({required this.speaker, required this.voiceConfig})
    : super(fullyQualifiedName);

  factory SpeakerVoiceConfig.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return SpeakerVoiceConfig(
      speaker: switch (json['speaker']) {
        null => '',
        Object $1 => decodeString($1),
      },
      voiceConfig: switch (json['voiceConfig']) {
        null => null,
        Object $1 => VoiceConfig.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    'speaker': speaker,
    if (voiceConfig != null) 'voiceConfig': voiceConfig!.toJson(),
  };

  @override
  String toString() {
    final contents = ['speaker=$speaker'].join(',');
    return 'SpeakerVoiceConfig($contents)';
  }
}

/// The configuration for the multi-speaker setup.
final class MultiSpeakerVoiceConfig extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.MultiSpeakerVoiceConfig';

  /// Required. All the enabled speaker voices.
  final List<SpeakerVoiceConfig> speakerVoiceConfigs;

  MultiSpeakerVoiceConfig({required this.speakerVoiceConfigs})
    : super(fullyQualifiedName);

  factory MultiSpeakerVoiceConfig.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return MultiSpeakerVoiceConfig(
      speakerVoiceConfigs: switch (json['speakerVoiceConfigs']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) SpeakerVoiceConfig.fromJson(i),
        ],
        _ => throw const FormatException('"speakerVoiceConfigs" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {'speakerVoiceConfigs': encodeList(speakerVoiceConfigs)};

  @override
  String toString() => 'MultiSpeakerVoiceConfig()';
}

/// The speech generation config.
final class SpeechConfig extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.SpeechConfig';

  /// The configuration in case of single-voice output.
  final VoiceConfig? voiceConfig;

  /// Optional. The configuration for the multi-speaker setup.
  /// It is mutually exclusive with the voice_config field.
  final MultiSpeakerVoiceConfig? multiSpeakerVoiceConfig;

  /// Optional. Language code (in BCP 47 format, e.g. "en-US") for speech
  /// synthesis.
  ///
  /// Valid values are: de-DE, en-AU, en-GB, en-IN, en-US, es-US, fr-FR, hi-IN,
  /// pt-BR, ar-XA, es-ES, fr-CA, id-ID, it-IT, ja-JP, tr-TR, vi-VN, bn-IN,
  /// gu-IN, kn-IN, ml-IN, mr-IN, ta-IN, te-IN, nl-NL, ko-KR, cmn-CN, pl-PL,
  /// ru-RU, and th-TH.
  final String languageCode;

  SpeechConfig({
    this.voiceConfig,
    this.multiSpeakerVoiceConfig,
    this.languageCode = '',
  }) : super(fullyQualifiedName);

  factory SpeechConfig.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return SpeechConfig(
      voiceConfig: switch (json['voiceConfig']) {
        null => null,
        Object $1 => VoiceConfig.fromJson($1),
      },
      multiSpeakerVoiceConfig: switch (json['multiSpeakerVoiceConfig']) {
        null => null,
        Object $1 => MultiSpeakerVoiceConfig.fromJson($1),
      },
      languageCode: switch (json['languageCode']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (voiceConfig != null) 'voiceConfig': voiceConfig!.toJson(),
    if (multiSpeakerVoiceConfig != null)
      'multiSpeakerVoiceConfig': multiSpeakerVoiceConfig!.toJson(),
    if (languageCode.isNotDefault) 'languageCode': languageCode,
  };

  @override
  String toString() {
    final contents = ['languageCode=$languageCode'].join(',');
    return 'SpeechConfig($contents)';
  }
}

/// Config for thinking features.
final class ThinkingConfig extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ThinkingConfig';

  /// Indicates whether to include thoughts in the response.
  /// If true, thoughts are returned only when available.
  final bool? includeThoughts;

  /// The number of thoughts tokens that the model should generate.
  final int? thinkingBudget;

  ThinkingConfig({this.includeThoughts, this.thinkingBudget})
    : super(fullyQualifiedName);

  factory ThinkingConfig.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ThinkingConfig(
      includeThoughts: switch (json['includeThoughts']) {
        null => null,
        Object $1 => decodeBool($1),
      },
      thinkingBudget: switch (json['thinkingBudget']) {
        null => null,
        Object $1 => decodeInt($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (includeThoughts != null) 'includeThoughts': includeThoughts,
    if (thinkingBudget != null) 'thinkingBudget': thinkingBudget,
  };

  @override
  String toString() {
    final contents = [
      if (includeThoughts != null) 'includeThoughts=$includeThoughts',
      if (thinkingBudget != null) 'thinkingBudget=$thinkingBudget',
    ].join(',');
    return 'ThinkingConfig($contents)';
  }
}

/// Config for image generation features.
final class ImageConfig extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ImageConfig';

  /// Optional. The aspect ratio of the image to generate. Supported aspect
  /// ratios: 1:1, 2:3, 3:2, 3:4, 4:3, 9:16, 16:9, 21:9.
  ///
  /// If not specified, the model will choose a default aspect ratio based on any
  /// reference images provided.
  final String? aspectRatio;

  ImageConfig({this.aspectRatio}) : super(fullyQualifiedName);

  factory ImageConfig.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ImageConfig(
      aspectRatio: switch (json['aspectRatio']) {
        null => null,
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {if (aspectRatio != null) 'aspectRatio': aspectRatio};

  @override
  String toString() {
    final contents = [
      if (aspectRatio != null) 'aspectRatio=$aspectRatio',
    ].join(',');
    return 'ImageConfig($contents)';
  }
}

/// Configuration options for model generation and outputs. Not all parameters
/// are configurable for every model.
/// Next ID: 29
final class GenerationConfig extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GenerationConfig';

  /// Optional. Number of generated responses to return. If unset, this will
  /// default to 1. Please note that this doesn't work for previous generation
  /// models (Gemini 1.0 family)
  final int? candidateCount;

  /// Optional. The set of character sequences (up to 5) that will stop output
  /// generation. If specified, the API will stop at the first appearance of a
  /// `stop_sequence`. The stop sequence will not be included as part of the
  /// response.
  final List<String> stopSequences;

  /// Optional. The maximum number of tokens to include in a response candidate.
  ///
  /// Note: The default value varies by model, see the `Model.output_token_limit`
  /// attribute of the `Model` returned from the `getModel` function.
  final int? maxOutputTokens;

  /// Optional. Controls the randomness of the output.
  ///
  /// Note: The default value varies by model, see the `Model.temperature`
  /// attribute of the `Model` returned from the `getModel` function.
  ///
  /// Values can range from [0.0, 2.0].
  final double? temperature;

  /// Optional. The maximum cumulative probability of tokens to consider when
  /// sampling.
  ///
  /// The model uses combined Top-k and Top-p (nucleus) sampling.
  ///
  /// Tokens are sorted based on their assigned probabilities so that only the
  /// most likely tokens are considered. Top-k sampling directly limits the
  /// maximum number of tokens to consider, while Nucleus sampling limits the
  /// number of tokens based on the cumulative probability.
  ///
  /// Note: The default value varies by `Model` and is specified by
  /// the`Model.top_p` attribute returned from the `getModel` function. An empty
  /// `top_k` attribute indicates that the model doesn't apply top-k sampling
  /// and doesn't allow setting `top_k` on requests.
  final double? topP;

  /// Optional. The maximum number of tokens to consider when sampling.
  ///
  /// Gemini models use Top-p (nucleus) sampling or a combination of Top-k and
  /// nucleus sampling. Top-k sampling considers the set of `top_k` most probable
  /// tokens. Models running with nucleus sampling don't allow top_k setting.
  ///
  /// Note: The default value varies by `Model` and is specified by
  /// the`Model.top_p` attribute returned from the `getModel` function. An empty
  /// `top_k` attribute indicates that the model doesn't apply top-k sampling
  /// and doesn't allow setting `top_k` on requests.
  final int? topK;

  /// Optional. Seed used in decoding. If not set, the request uses a randomly
  /// generated seed.
  final int? seed;

  /// Optional. MIME type of the generated candidate text.
  /// Supported MIME types are:
  /// `text/plain`: (default) Text output.
  /// `application/json`: JSON response in the response candidates.
  /// `text/x.enum`: ENUM as a string response in the response candidates.
  /// Refer to the
  /// [docs](https://ai.google.dev/gemini-api/docs/prompting_with_media#plain_text_formats)
  /// for a list of all supported text MIME types.
  final String responseMimeType;

  /// Optional. Output schema of the generated candidate text. Schemas must be a
  /// subset of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schema)
  /// and can be objects, primitives or arrays.
  ///
  /// If set, a compatible `response_mime_type` must also be set.
  /// Compatible MIME types:
  /// `application/json`: Schema for JSON response.
  /// Refer to the [JSON text generation
  /// guide](https://ai.google.dev/gemini-api/docs/json-mode) for more details.
  final Schema? responseSchema;

  /// Optional. Output schema of the generated response. This is an alternative
  /// to `response_schema` that accepts [JSON Schema](https://json-schema.org/).
  ///
  /// If set, `response_schema` must be omitted, but `response_mime_type` is
  /// required.
  ///
  /// While the full JSON Schema may be sent, not all features are supported.
  /// Specifically, only the following properties are supported:
  ///
  /// - `$id`
  /// - `$defs`
  /// - `$ref`
  /// - `$anchor`
  /// - `type`
  /// - `format`
  /// - `title`
  /// - `description`
  /// - `enum` (for strings and numbers)
  /// - `items`
  /// - `prefixItems`
  /// - `minItems`
  /// - `maxItems`
  /// - `minimum`
  /// - `maximum`
  /// - `anyOf`
  /// - `oneOf` (interpreted the same as `anyOf`)
  /// - `properties`
  /// - `additionalProperties`
  /// - `required`
  ///
  /// The non-standard `propertyOrdering` property may also be set.
  ///
  /// Cyclic references are unrolled to a limited degree and, as such, may only
  /// be used within non-required properties. (Nullable properties are not
  /// sufficient.) If `$ref` is set on a sub-schema, no other properties, except
  /// for than those starting as a `$`, may be set.
  final Value? responseJsonSchema;

  /// Optional. An internal detail. Use `responseJsonSchema` rather than this
  /// field.
  final Value? responseJsonSchemaOrdered;

  /// Optional. Presence penalty applied to the next token's logprobs if the
  /// token has already been seen in the response.
  ///
  /// This penalty is binary on/off and not dependant on the number of times the
  /// token is used (after the first). Use
  /// `frequency_penalty`
  /// for a penalty that increases with each use.
  ///
  /// A positive penalty will discourage the use of tokens that have already
  /// been used in the response, increasing the vocabulary.
  ///
  /// A negative penalty will encourage the use of tokens that have already been
  /// used in the response, decreasing the vocabulary.
  final double? presencePenalty;

  /// Optional. Frequency penalty applied to the next token's logprobs,
  /// multiplied by the number of times each token has been seen in the respponse
  /// so far.
  ///
  /// A positive penalty will discourage the use of tokens that have already
  /// been used, proportional to the number of times the token has been used:
  /// The more a token is used, the more difficult it is for the model to use
  /// that token again increasing the vocabulary of responses.
  ///
  /// Caution: A _negative_ penalty will encourage the model to reuse tokens
  /// proportional to the number of times the token has been used. Small
  /// negative values will reduce the vocabulary of a response. Larger negative
  /// values will cause the model to start repeating a common token  until it
  /// hits the
  /// `max_output_tokens`
  /// limit.
  final double? frequencyPenalty;

  /// Optional. If true, export the logprobs results in response.
  final bool? responseLogprobs;

  /// Optional. Only valid if
  /// [response_logprobs=True][google.ai.generativelanguage.v1beta.GenerationConfig.response_logprobs].
  /// This sets the number of top logprobs to return at each decoding step in the
  /// `Candidate.logprobs_result`.
  /// The number must be in the range of [0, 20].
  final int? logprobs;

  /// Optional. Enables enhanced civic answers. It may not be available for all
  /// models.
  final bool? enableEnhancedCivicAnswers;

  /// Optional. The requested modalities of the response. Represents the set of
  /// modalities that the model can return, and should be expected in the
  /// response. This is an exact match to the modalities of the response.
  ///
  /// A model may have multiple combinations of supported modalities. If the
  /// requested modalities do not match any of the supported combinations, an
  /// error will be returned.
  ///
  /// An empty list is equivalent to requesting only text.
  final List<GenerationConfig_Modality> responseModalities;

  /// Optional. The speech generation config.
  final SpeechConfig? speechConfig;

  /// Optional. Config for thinking features.
  /// An error will be returned if this field is set for models that don't
  /// support thinking.
  final ThinkingConfig? thinkingConfig;

  /// Optional. Config for image generation.
  /// An error will be returned if this field is set for models that don't
  /// support these config options.
  final ImageConfig? imageConfig;

  /// Optional. If specified, the media resolution specified will be used.
  final GenerationConfig_MediaResolution? mediaResolution;

  GenerationConfig({
    this.candidateCount,
    this.stopSequences = const [],
    this.maxOutputTokens,
    this.temperature,
    this.topP,
    this.topK,
    this.seed,
    this.responseMimeType = '',
    this.responseSchema,
    this.responseJsonSchema,
    this.responseJsonSchemaOrdered,
    this.presencePenalty,
    this.frequencyPenalty,
    this.responseLogprobs,
    this.logprobs,
    this.enableEnhancedCivicAnswers,
    this.responseModalities = const [],
    this.speechConfig,
    this.thinkingConfig,
    this.imageConfig,
    this.mediaResolution,
  }) : super(fullyQualifiedName);

  factory GenerationConfig.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GenerationConfig(
      candidateCount: switch (json['candidateCount']) {
        null => null,
        Object $1 => decodeInt($1),
      },
      stopSequences: switch (json['stopSequences']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) decodeString(i)],
        _ => throw const FormatException('"stopSequences" is not a list'),
      },
      maxOutputTokens: switch (json['maxOutputTokens']) {
        null => null,
        Object $1 => decodeInt($1),
      },
      temperature: switch (json['temperature']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      topP: switch (json['topP']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      topK: switch (json['topK']) {
        null => null,
        Object $1 => decodeInt($1),
      },
      seed: switch (json['seed']) {
        null => null,
        Object $1 => decodeInt($1),
      },
      responseMimeType: switch (json['responseMimeType']) {
        null => '',
        Object $1 => decodeString($1),
      },
      responseSchema: switch (json['responseSchema']) {
        null => null,
        Object $1 => Schema.fromJson($1),
      },
      responseJsonSchema: switch (json['_responseJsonSchema']) {
        null => null,
        Object $1 => Value.fromJson($1),
      },
      responseJsonSchemaOrdered: switch (json['responseJsonSchema']) {
        null => null,
        Object $1 => Value.fromJson($1),
      },
      presencePenalty: switch (json['presencePenalty']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      frequencyPenalty: switch (json['frequencyPenalty']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      responseLogprobs: switch (json['responseLogprobs']) {
        null => null,
        Object $1 => decodeBool($1),
      },
      logprobs: switch (json['logprobs']) {
        null => null,
        Object $1 => decodeInt($1),
      },
      enableEnhancedCivicAnswers: switch (json['enableEnhancedCivicAnswers']) {
        null => null,
        Object $1 => decodeBool($1),
      },
      responseModalities: switch (json['responseModalities']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) GenerationConfig_Modality.fromJson(i),
        ],
        _ => throw const FormatException('"responseModalities" is not a list'),
      },
      speechConfig: switch (json['speechConfig']) {
        null => null,
        Object $1 => SpeechConfig.fromJson($1),
      },
      thinkingConfig: switch (json['thinkingConfig']) {
        null => null,
        Object $1 => ThinkingConfig.fromJson($1),
      },
      imageConfig: switch (json['imageConfig']) {
        null => null,
        Object $1 => ImageConfig.fromJson($1),
      },
      mediaResolution: switch (json['mediaResolution']) {
        null => null,
        Object $1 => GenerationConfig_MediaResolution.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (candidateCount != null) 'candidateCount': candidateCount,
    if (stopSequences.isNotDefault) 'stopSequences': stopSequences,
    if (maxOutputTokens != null) 'maxOutputTokens': maxOutputTokens,
    if (temperature != null) 'temperature': encodeDouble(temperature),
    if (topP != null) 'topP': encodeDouble(topP),
    if (topK != null) 'topK': topK,
    if (seed != null) 'seed': seed,
    if (responseMimeType.isNotDefault) 'responseMimeType': responseMimeType,
    if (responseSchema != null) 'responseSchema': responseSchema!.toJson(),
    if (responseJsonSchema != null)
      '_responseJsonSchema': responseJsonSchema!.toJson(),
    if (responseJsonSchemaOrdered != null)
      'responseJsonSchema': responseJsonSchemaOrdered!.toJson(),
    if (presencePenalty != null)
      'presencePenalty': encodeDouble(presencePenalty),
    if (frequencyPenalty != null)
      'frequencyPenalty': encodeDouble(frequencyPenalty),
    if (responseLogprobs != null) 'responseLogprobs': responseLogprobs,
    if (logprobs != null) 'logprobs': logprobs,
    if (enableEnhancedCivicAnswers != null)
      'enableEnhancedCivicAnswers': enableEnhancedCivicAnswers,
    if (responseModalities.isNotDefault)
      'responseModalities': encodeList(responseModalities),
    if (speechConfig != null) 'speechConfig': speechConfig!.toJson(),
    if (thinkingConfig != null) 'thinkingConfig': thinkingConfig!.toJson(),
    if (imageConfig != null) 'imageConfig': imageConfig!.toJson(),
    if (mediaResolution != null) 'mediaResolution': mediaResolution!.toJson(),
  };

  @override
  String toString() {
    final contents = [
      if (candidateCount != null) 'candidateCount=$candidateCount',
      if (maxOutputTokens != null) 'maxOutputTokens=$maxOutputTokens',
      if (temperature != null) 'temperature=$temperature',
      if (topP != null) 'topP=$topP',
      if (topK != null) 'topK=$topK',
      if (seed != null) 'seed=$seed',
      'responseMimeType=$responseMimeType',
      if (presencePenalty != null) 'presencePenalty=$presencePenalty',
      if (frequencyPenalty != null) 'frequencyPenalty=$frequencyPenalty',
      if (responseLogprobs != null) 'responseLogprobs=$responseLogprobs',
      if (logprobs != null) 'logprobs=$logprobs',
      if (enableEnhancedCivicAnswers != null)
        'enableEnhancedCivicAnswers=$enableEnhancedCivicAnswers',
      if (mediaResolution != null) 'mediaResolution=$mediaResolution',
    ].join(',');
    return 'GenerationConfig($contents)';
  }
}

/// Supported modalities of the response.
final class GenerationConfig_Modality extends ProtoEnum {
  /// Default value.
  static const modalityUnspecified = GenerationConfig_Modality(
    'MODALITY_UNSPECIFIED',
  );

  /// Indicates the model should return text.
  static const text = GenerationConfig_Modality('TEXT');

  /// Indicates the model should return images.
  static const image = GenerationConfig_Modality('IMAGE');

  /// Indicates the model should return audio.
  static const audio = GenerationConfig_Modality('AUDIO');

  /// The default value for [GenerationConfig_Modality].
  static const $default = modalityUnspecified;

  const GenerationConfig_Modality(super.value);

  factory GenerationConfig_Modality.fromJson(Object? json) =>
      GenerationConfig_Modality(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'Modality.$value';
}

/// Media resolution for the input media.
final class GenerationConfig_MediaResolution extends ProtoEnum {
  /// Media resolution has not been set.
  static const mediaResolutionUnspecified = GenerationConfig_MediaResolution(
    'MEDIA_RESOLUTION_UNSPECIFIED',
  );

  /// Media resolution set to low (64 tokens).
  static const mediaResolutionLow = GenerationConfig_MediaResolution(
    'MEDIA_RESOLUTION_LOW',
  );

  /// Media resolution set to medium (256 tokens).
  static const mediaResolutionMedium = GenerationConfig_MediaResolution(
    'MEDIA_RESOLUTION_MEDIUM',
  );

  /// Media resolution set to high (zoomed reframing with 256 tokens).
  static const mediaResolutionHigh = GenerationConfig_MediaResolution(
    'MEDIA_RESOLUTION_HIGH',
  );

  /// The default value for [GenerationConfig_MediaResolution].
  static const $default = mediaResolutionUnspecified;

  const GenerationConfig_MediaResolution(super.value);

  factory GenerationConfig_MediaResolution.fromJson(Object? json) =>
      GenerationConfig_MediaResolution(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'MediaResolution.$value';
}

/// Configuration for retrieving grounding content from a `Corpus` or
/// `Document` created using the Semantic Retriever API.
final class SemanticRetrieverConfig extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.SemanticRetrieverConfig';

  /// Required. Name of the resource for retrieval. Example: `corpora/123` or
  /// `corpora/123/documents/abc`.
  final String source;

  /// Required. Query to use for matching `Chunk`s in the given resource by
  /// similarity.
  final Content? query;

  /// Optional. Filters for selecting `Document`s and/or `Chunk`s from the
  /// resource.
  final List<MetadataFilter> metadataFilters;

  /// Optional. Maximum number of relevant `Chunk`s to retrieve.
  final int? maxChunksCount;

  /// Optional. Minimum relevance score for retrieved relevant `Chunk`s.
  final double? minimumRelevanceScore;

  SemanticRetrieverConfig({
    required this.source,
    required this.query,
    this.metadataFilters = const [],
    this.maxChunksCount,
    this.minimumRelevanceScore,
  }) : super(fullyQualifiedName);

  factory SemanticRetrieverConfig.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return SemanticRetrieverConfig(
      source: switch (json['source']) {
        null => '',
        Object $1 => decodeString($1),
      },
      query: switch (json['query']) {
        null => null,
        Object $1 => Content.fromJson($1),
      },
      metadataFilters: switch (json['metadataFilters']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) MetadataFilter.fromJson(i)],
        _ => throw const FormatException('"metadataFilters" is not a list'),
      },
      maxChunksCount: switch (json['maxChunksCount']) {
        null => null,
        Object $1 => decodeInt($1),
      },
      minimumRelevanceScore: switch (json['minimumRelevanceScore']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
    );
  }

  @override
  Object toJson() => {
    'source': source,
    if (query != null) 'query': query!.toJson(),
    if (metadataFilters.isNotDefault)
      'metadataFilters': encodeList(metadataFilters),
    if (maxChunksCount != null) 'maxChunksCount': maxChunksCount,
    if (minimumRelevanceScore != null)
      'minimumRelevanceScore': encodeDouble(minimumRelevanceScore),
  };

  @override
  String toString() {
    final contents = [
      'source=$source',
      if (maxChunksCount != null) 'maxChunksCount=$maxChunksCount',
      if (minimumRelevanceScore != null)
        'minimumRelevanceScore=$minimumRelevanceScore',
    ].join(',');
    return 'SemanticRetrieverConfig($contents)';
  }
}

/// Response from the model supporting multiple candidate responses.
///
/// Safety ratings and content filtering are reported for both
/// prompt in `GenerateContentResponse.prompt_feedback` and for each candidate
/// in `finish_reason` and in `safety_ratings`. The API:
///  - Returns either all requested candidates or none of them
///  - Returns no candidates at all only if there was something wrong with the
///    prompt (check `prompt_feedback`)
///  - Reports feedback on each candidate in `finish_reason` and
///    `safety_ratings`.
final class GenerateContentResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GenerateContentResponse';

  /// Candidate responses from the model.
  final List<Candidate> candidates;

  /// Returns the prompt's feedback related to the content filters.
  final GenerateContentResponse_PromptFeedback? promptFeedback;

  /// Output only. Metadata on the generation requests' token usage.
  final GenerateContentResponse_UsageMetadata? usageMetadata;

  /// Output only. The model version used to generate the response.
  final String modelVersion;

  /// Output only. response_id is used to identify each response.
  final String responseId;

  GenerateContentResponse({
    this.candidates = const [],
    this.promptFeedback,
    this.usageMetadata,
    this.modelVersion = '',
    this.responseId = '',
  }) : super(fullyQualifiedName);

  factory GenerateContentResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GenerateContentResponse(
      candidates: switch (json['candidates']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Candidate.fromJson(i)],
        _ => throw const FormatException('"candidates" is not a list'),
      },
      promptFeedback: switch (json['promptFeedback']) {
        null => null,
        Object $1 => GenerateContentResponse_PromptFeedback.fromJson($1),
      },
      usageMetadata: switch (json['usageMetadata']) {
        null => null,
        Object $1 => GenerateContentResponse_UsageMetadata.fromJson($1),
      },
      modelVersion: switch (json['modelVersion']) {
        null => '',
        Object $1 => decodeString($1),
      },
      responseId: switch (json['responseId']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (candidates.isNotDefault) 'candidates': encodeList(candidates),
    if (promptFeedback != null) 'promptFeedback': promptFeedback!.toJson(),
    if (usageMetadata != null) 'usageMetadata': usageMetadata!.toJson(),
    if (modelVersion.isNotDefault) 'modelVersion': modelVersion,
    if (responseId.isNotDefault) 'responseId': responseId,
  };

  @override
  String toString() {
    final contents = [
      'modelVersion=$modelVersion',
      'responseId=$responseId',
    ].join(',');
    return 'GenerateContentResponse($contents)';
  }
}

/// A set of the feedback metadata the prompt specified in
/// `GenerateContentRequest.content`.
final class GenerateContentResponse_PromptFeedback extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GenerateContentResponse.PromptFeedback';

  /// Optional. If set, the prompt was blocked and no candidates are returned.
  /// Rephrase the prompt.
  final GenerateContentResponse_PromptFeedback_BlockReason blockReason;

  /// Ratings for safety of the prompt.
  /// There is at most one rating per category.
  final List<SafetyRating> safetyRatings;

  GenerateContentResponse_PromptFeedback({
    this.blockReason =
        GenerateContentResponse_PromptFeedback_BlockReason.$default,
    this.safetyRatings = const [],
  }) : super(fullyQualifiedName);

  factory GenerateContentResponse_PromptFeedback.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GenerateContentResponse_PromptFeedback(
      blockReason: switch (json['blockReason']) {
        null => GenerateContentResponse_PromptFeedback_BlockReason.$default,
        Object $1 =>
          GenerateContentResponse_PromptFeedback_BlockReason.fromJson($1),
      },
      safetyRatings: switch (json['safetyRatings']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) SafetyRating.fromJson(i)],
        _ => throw const FormatException('"safetyRatings" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (blockReason.isNotDefault) 'blockReason': blockReason.toJson(),
    if (safetyRatings.isNotDefault) 'safetyRatings': encodeList(safetyRatings),
  };

  @override
  String toString() {
    final contents = ['blockReason=$blockReason'].join(',');
    return 'PromptFeedback($contents)';
  }
}

/// Specifies the reason why the prompt was blocked.
final class GenerateContentResponse_PromptFeedback_BlockReason
    extends ProtoEnum {
  /// Default value. This value is unused.
  static const blockReasonUnspecified =
      GenerateContentResponse_PromptFeedback_BlockReason(
        'BLOCK_REASON_UNSPECIFIED',
      );

  /// Prompt was blocked due to safety reasons. Inspect `safety_ratings`
  /// to understand which safety category blocked it.
  static const safety = GenerateContentResponse_PromptFeedback_BlockReason(
    'SAFETY',
  );

  /// Prompt was blocked due to unknown reasons.
  static const other = GenerateContentResponse_PromptFeedback_BlockReason(
    'OTHER',
  );

  /// Prompt was blocked due to the terms which are included from the
  /// terminology blocklist.
  static const blocklist = GenerateContentResponse_PromptFeedback_BlockReason(
    'BLOCKLIST',
  );

  /// Prompt was blocked due to prohibited content.
  static const prohibitedContent =
      GenerateContentResponse_PromptFeedback_BlockReason('PROHIBITED_CONTENT');

  /// Candidates blocked due to unsafe image generation content.
  static const imageSafety = GenerateContentResponse_PromptFeedback_BlockReason(
    'IMAGE_SAFETY',
  );

  /// The default value for [GenerateContentResponse_PromptFeedback_BlockReason].
  static const $default = blockReasonUnspecified;

  const GenerateContentResponse_PromptFeedback_BlockReason(super.value);

  factory GenerateContentResponse_PromptFeedback_BlockReason.fromJson(
    Object? json,
  ) => GenerateContentResponse_PromptFeedback_BlockReason(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'BlockReason.$value';
}

/// Metadata on the generation request's token usage.
final class GenerateContentResponse_UsageMetadata extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GenerateContentResponse.UsageMetadata';

  /// Number of tokens in the prompt. When `cached_content` is set, this is
  /// still the total effective prompt size meaning this includes the number of
  /// tokens in the cached content.
  final int promptTokenCount;

  /// Number of tokens in the cached part of the prompt (the cached content)
  final int cachedContentTokenCount;

  /// Total number of tokens across all the generated response candidates.
  final int candidatesTokenCount;

  /// Output only. Number of tokens present in tool-use prompt(s).
  final int toolUsePromptTokenCount;

  /// Output only. Number of tokens of thoughts for thinking models.
  final int thoughtsTokenCount;

  /// Total token count for the generation request (prompt + response
  /// candidates).
  final int totalTokenCount;

  /// Output only. List of modalities that were processed in the request input.
  final List<ModalityTokenCount> promptTokensDetails;

  /// Output only. List of modalities of the cached content in the request
  /// input.
  final List<ModalityTokenCount> cacheTokensDetails;

  /// Output only. List of modalities that were returned in the response.
  final List<ModalityTokenCount> candidatesTokensDetails;

  /// Output only. List of modalities that were processed for tool-use request
  /// inputs.
  final List<ModalityTokenCount> toolUsePromptTokensDetails;

  GenerateContentResponse_UsageMetadata({
    this.promptTokenCount = 0,
    this.cachedContentTokenCount = 0,
    this.candidatesTokenCount = 0,
    this.toolUsePromptTokenCount = 0,
    this.thoughtsTokenCount = 0,
    this.totalTokenCount = 0,
    this.promptTokensDetails = const [],
    this.cacheTokensDetails = const [],
    this.candidatesTokensDetails = const [],
    this.toolUsePromptTokensDetails = const [],
  }) : super(fullyQualifiedName);

  factory GenerateContentResponse_UsageMetadata.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GenerateContentResponse_UsageMetadata(
      promptTokenCount: switch (json['promptTokenCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      cachedContentTokenCount: switch (json['cachedContentTokenCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      candidatesTokenCount: switch (json['candidatesTokenCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      toolUsePromptTokenCount: switch (json['toolUsePromptTokenCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      thoughtsTokenCount: switch (json['thoughtsTokenCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      totalTokenCount: switch (json['totalTokenCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      promptTokensDetails: switch (json['promptTokensDetails']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) ModalityTokenCount.fromJson(i),
        ],
        _ => throw const FormatException('"promptTokensDetails" is not a list'),
      },
      cacheTokensDetails: switch (json['cacheTokensDetails']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) ModalityTokenCount.fromJson(i),
        ],
        _ => throw const FormatException('"cacheTokensDetails" is not a list'),
      },
      candidatesTokensDetails: switch (json['candidatesTokensDetails']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) ModalityTokenCount.fromJson(i),
        ],
        _ => throw const FormatException(
          '"candidatesTokensDetails" is not a list',
        ),
      },
      toolUsePromptTokensDetails: switch (json['toolUsePromptTokensDetails']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) ModalityTokenCount.fromJson(i),
        ],
        _ => throw const FormatException(
          '"toolUsePromptTokensDetails" is not a list',
        ),
      },
    );
  }

  @override
  Object toJson() => {
    if (promptTokenCount.isNotDefault) 'promptTokenCount': promptTokenCount,
    if (cachedContentTokenCount.isNotDefault)
      'cachedContentTokenCount': cachedContentTokenCount,
    if (candidatesTokenCount.isNotDefault)
      'candidatesTokenCount': candidatesTokenCount,
    if (toolUsePromptTokenCount.isNotDefault)
      'toolUsePromptTokenCount': toolUsePromptTokenCount,
    if (thoughtsTokenCount.isNotDefault)
      'thoughtsTokenCount': thoughtsTokenCount,
    if (totalTokenCount.isNotDefault) 'totalTokenCount': totalTokenCount,
    if (promptTokensDetails.isNotDefault)
      'promptTokensDetails': encodeList(promptTokensDetails),
    if (cacheTokensDetails.isNotDefault)
      'cacheTokensDetails': encodeList(cacheTokensDetails),
    if (candidatesTokensDetails.isNotDefault)
      'candidatesTokensDetails': encodeList(candidatesTokensDetails),
    if (toolUsePromptTokensDetails.isNotDefault)
      'toolUsePromptTokensDetails': encodeList(toolUsePromptTokensDetails),
  };

  @override
  String toString() {
    final contents = [
      'promptTokenCount=$promptTokenCount',
      'cachedContentTokenCount=$cachedContentTokenCount',
      'candidatesTokenCount=$candidatesTokenCount',
      'toolUsePromptTokenCount=$toolUsePromptTokenCount',
      'thoughtsTokenCount=$thoughtsTokenCount',
      'totalTokenCount=$totalTokenCount',
    ].join(',');
    return 'UsageMetadata($contents)';
  }
}

/// A response candidate generated from the model.
final class Candidate extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Candidate';

  /// Output only. Index of the candidate in the list of response candidates.
  final int? index;

  /// Output only. Generated content returned from the model.
  final Content? content;

  /// Optional. Output only. The reason why the model stopped generating tokens.
  ///
  /// If empty, the model has not stopped generating tokens.
  final Candidate_FinishReason finishReason;

  /// Optional. Output only. Details the reason why the model stopped generating
  /// tokens. This is populated only when `finish_reason` is set.
  final String? finishMessage;

  /// List of ratings for the safety of a response candidate.
  ///
  /// There is at most one rating per category.
  final List<SafetyRating> safetyRatings;

  /// Output only. Citation information for model-generated candidate.
  ///
  /// This field may be populated with recitation information for any text
  /// included in the `content`. These are passages that are "recited" from
  /// copyrighted material in the foundational LLM's training data.
  final CitationMetadata? citationMetadata;

  /// Output only. Token count for this candidate.
  final int tokenCount;

  /// Output only. Attribution information for sources that contributed to a
  /// grounded answer.
  ///
  /// This field is populated for `GenerateAnswer` calls.
  final List<GroundingAttribution> groundingAttributions;

  /// Output only. Grounding metadata for the candidate.
  ///
  /// This field is populated for `GenerateContent` calls.
  final GroundingMetadata? groundingMetadata;

  /// Output only. Average log probability score of the candidate.
  final double avgLogprobs;

  /// Output only. Log-likelihood scores for the response tokens and top tokens
  final LogprobsResult? logprobsResult;

  /// Output only. Metadata related to url context retrieval tool.
  final UrlContextMetadata? urlContextMetadata;

  Candidate({
    this.index,
    this.content,
    this.finishReason = Candidate_FinishReason.$default,
    this.finishMessage,
    this.safetyRatings = const [],
    this.citationMetadata,
    this.tokenCount = 0,
    this.groundingAttributions = const [],
    this.groundingMetadata,
    this.avgLogprobs = 0,
    this.logprobsResult,
    this.urlContextMetadata,
  }) : super(fullyQualifiedName);

  factory Candidate.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Candidate(
      index: switch (json['index']) {
        null => null,
        Object $1 => decodeInt($1),
      },
      content: switch (json['content']) {
        null => null,
        Object $1 => Content.fromJson($1),
      },
      finishReason: switch (json['finishReason']) {
        null => Candidate_FinishReason.$default,
        Object $1 => Candidate_FinishReason.fromJson($1),
      },
      finishMessage: switch (json['finishMessage']) {
        null => null,
        Object $1 => decodeString($1),
      },
      safetyRatings: switch (json['safetyRatings']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) SafetyRating.fromJson(i)],
        _ => throw const FormatException('"safetyRatings" is not a list'),
      },
      citationMetadata: switch (json['citationMetadata']) {
        null => null,
        Object $1 => CitationMetadata.fromJson($1),
      },
      tokenCount: switch (json['tokenCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      groundingAttributions: switch (json['groundingAttributions']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) GroundingAttribution.fromJson(i),
        ],
        _ => throw const FormatException(
          '"groundingAttributions" is not a list',
        ),
      },
      groundingMetadata: switch (json['groundingMetadata']) {
        null => null,
        Object $1 => GroundingMetadata.fromJson($1),
      },
      avgLogprobs: switch (json['avgLogprobs']) {
        null => 0,
        Object $1 => decodeDouble($1),
      },
      logprobsResult: switch (json['logprobsResult']) {
        null => null,
        Object $1 => LogprobsResult.fromJson($1),
      },
      urlContextMetadata: switch (json['urlContextMetadata']) {
        null => null,
        Object $1 => UrlContextMetadata.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (index != null) 'index': index,
    if (content != null) 'content': content!.toJson(),
    if (finishReason.isNotDefault) 'finishReason': finishReason.toJson(),
    if (finishMessage != null) 'finishMessage': finishMessage,
    if (safetyRatings.isNotDefault) 'safetyRatings': encodeList(safetyRatings),
    if (citationMetadata != null)
      'citationMetadata': citationMetadata!.toJson(),
    if (tokenCount.isNotDefault) 'tokenCount': tokenCount,
    if (groundingAttributions.isNotDefault)
      'groundingAttributions': encodeList(groundingAttributions),
    if (groundingMetadata != null)
      'groundingMetadata': groundingMetadata!.toJson(),
    if (avgLogprobs.isNotDefault) 'avgLogprobs': encodeDouble(avgLogprobs),
    if (logprobsResult != null) 'logprobsResult': logprobsResult!.toJson(),
    if (urlContextMetadata != null)
      'urlContextMetadata': urlContextMetadata!.toJson(),
  };

  @override
  String toString() {
    final contents = [
      if (index != null) 'index=$index',
      'finishReason=$finishReason',
      if (finishMessage != null) 'finishMessage=$finishMessage',
      'tokenCount=$tokenCount',
      'avgLogprobs=$avgLogprobs',
    ].join(',');
    return 'Candidate($contents)';
  }
}

/// Defines the reason why the model stopped generating tokens.
final class Candidate_FinishReason extends ProtoEnum {
  /// Default value. This value is unused.
  static const finishReasonUnspecified = Candidate_FinishReason(
    'FINISH_REASON_UNSPECIFIED',
  );

  /// Natural stop point of the model or provided stop sequence.
  static const stop = Candidate_FinishReason('STOP');

  /// The maximum number of tokens as specified in the request was reached.
  static const maxTokens = Candidate_FinishReason('MAX_TOKENS');

  /// The response candidate content was flagged for safety reasons.
  static const safety = Candidate_FinishReason('SAFETY');

  /// The response candidate content was flagged for recitation reasons.
  static const recitation = Candidate_FinishReason('RECITATION');

  /// The response candidate content was flagged for using an unsupported
  /// language.
  static const language = Candidate_FinishReason('LANGUAGE');

  /// Unknown reason.
  static const other = Candidate_FinishReason('OTHER');

  /// Token generation stopped because the content contains forbidden terms.
  static const blocklist = Candidate_FinishReason('BLOCKLIST');

  /// Token generation stopped for potentially containing prohibited content.
  static const prohibitedContent = Candidate_FinishReason('PROHIBITED_CONTENT');

  /// Token generation stopped because the content potentially contains
  /// Sensitive Personally Identifiable Information (SPII).
  static const spii = Candidate_FinishReason('SPII');

  /// The function call generated by the model is invalid.
  static const malformedFunctionCall = Candidate_FinishReason(
    'MALFORMED_FUNCTION_CALL',
  );

  /// Token generation stopped because generated images contain safety
  /// violations.
  static const imageSafety = Candidate_FinishReason('IMAGE_SAFETY');

  /// Image generation stopped because generated images has other prohibited
  /// content.
  static const imageProhibitedContent = Candidate_FinishReason(
    'IMAGE_PROHIBITED_CONTENT',
  );

  /// Image generation stopped because of other miscellaneous issue.
  static const imageOther = Candidate_FinishReason('IMAGE_OTHER');

  /// The model was expected to generate an image, but none was generated.
  static const noImage = Candidate_FinishReason('NO_IMAGE');

  /// Image generation stopped due to recitation.
  static const imageRecitation = Candidate_FinishReason('IMAGE_RECITATION');

  /// Model generated a tool call but no tools were enabled in the request.
  static const unexpectedToolCall = Candidate_FinishReason(
    'UNEXPECTED_TOOL_CALL',
  );

  /// Model called too many tools consecutively, thus the system exited
  /// execution.
  static const tooManyToolCalls = Candidate_FinishReason('TOO_MANY_TOOL_CALLS');

  /// The default value for [Candidate_FinishReason].
  static const $default = finishReasonUnspecified;

  const Candidate_FinishReason(super.value);

  factory Candidate_FinishReason.fromJson(Object? json) =>
      Candidate_FinishReason(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'FinishReason.$value';
}

/// Metadata related to url context retrieval tool.
final class UrlContextMetadata extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.UrlContextMetadata';

  /// List of url context.
  final List<UrlMetadata> urlMetadata;

  UrlContextMetadata({this.urlMetadata = const []}) : super(fullyQualifiedName);

  factory UrlContextMetadata.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return UrlContextMetadata(
      urlMetadata: switch (json['urlMetadata']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) UrlMetadata.fromJson(i)],
        _ => throw const FormatException('"urlMetadata" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (urlMetadata.isNotDefault) 'urlMetadata': encodeList(urlMetadata),
  };

  @override
  String toString() => 'UrlContextMetadata()';
}

/// Context of the a single url retrieval.
final class UrlMetadata extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.UrlMetadata';

  /// Retrieved url by the tool.
  final String retrievedUrl;

  /// Status of the url retrieval.
  final UrlMetadata_UrlRetrievalStatus urlRetrievalStatus;

  UrlMetadata({
    this.retrievedUrl = '',
    this.urlRetrievalStatus = UrlMetadata_UrlRetrievalStatus.$default,
  }) : super(fullyQualifiedName);

  factory UrlMetadata.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return UrlMetadata(
      retrievedUrl: switch (json['retrievedUrl']) {
        null => '',
        Object $1 => decodeString($1),
      },
      urlRetrievalStatus: switch (json['urlRetrievalStatus']) {
        null => UrlMetadata_UrlRetrievalStatus.$default,
        Object $1 => UrlMetadata_UrlRetrievalStatus.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (retrievedUrl.isNotDefault) 'retrievedUrl': retrievedUrl,
    if (urlRetrievalStatus.isNotDefault)
      'urlRetrievalStatus': urlRetrievalStatus.toJson(),
  };

  @override
  String toString() {
    final contents = [
      'retrievedUrl=$retrievedUrl',
      'urlRetrievalStatus=$urlRetrievalStatus',
    ].join(',');
    return 'UrlMetadata($contents)';
  }
}

/// Status of the url retrieval.
final class UrlMetadata_UrlRetrievalStatus extends ProtoEnum {
  /// Default value. This value is unused.
  static const urlRetrievalStatusUnspecified = UrlMetadata_UrlRetrievalStatus(
    'URL_RETRIEVAL_STATUS_UNSPECIFIED',
  );

  /// Url retrieval is successful.
  static const urlRetrievalStatusSuccess = UrlMetadata_UrlRetrievalStatus(
    'URL_RETRIEVAL_STATUS_SUCCESS',
  );

  /// Url retrieval is failed due to error.
  static const urlRetrievalStatusError = UrlMetadata_UrlRetrievalStatus(
    'URL_RETRIEVAL_STATUS_ERROR',
  );

  /// Url retrieval is failed because the content is behind paywall.
  static const urlRetrievalStatusPaywall = UrlMetadata_UrlRetrievalStatus(
    'URL_RETRIEVAL_STATUS_PAYWALL',
  );

  /// Url retrieval is failed because the content is unsafe.
  static const urlRetrievalStatusUnsafe = UrlMetadata_UrlRetrievalStatus(
    'URL_RETRIEVAL_STATUS_UNSAFE',
  );

  /// The default value for [UrlMetadata_UrlRetrievalStatus].
  static const $default = urlRetrievalStatusUnspecified;

  const UrlMetadata_UrlRetrievalStatus(super.value);

  factory UrlMetadata_UrlRetrievalStatus.fromJson(Object? json) =>
      UrlMetadata_UrlRetrievalStatus(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'UrlRetrievalStatus.$value';
}

/// Logprobs Result
final class LogprobsResult extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.LogprobsResult';

  /// Sum of log probabilities for all tokens.
  final double? logProbabilitySum;

  /// Length = total number of decoding steps.
  final List<LogprobsResult_TopCandidates> topCandidates;

  /// Length = total number of decoding steps.
  /// The chosen candidates may or may not be in top_candidates.
  final List<LogprobsResult_Candidate> chosenCandidates;

  LogprobsResult({
    this.logProbabilitySum,
    this.topCandidates = const [],
    this.chosenCandidates = const [],
  }) : super(fullyQualifiedName);

  factory LogprobsResult.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return LogprobsResult(
      logProbabilitySum: switch (json['logProbabilitySum']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      topCandidates: switch (json['topCandidates']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) LogprobsResult_TopCandidates.fromJson(i),
        ],
        _ => throw const FormatException('"topCandidates" is not a list'),
      },
      chosenCandidates: switch (json['chosenCandidates']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) LogprobsResult_Candidate.fromJson(i),
        ],
        _ => throw const FormatException('"chosenCandidates" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (logProbabilitySum != null)
      'logProbabilitySum': encodeDouble(logProbabilitySum),
    if (topCandidates.isNotDefault) 'topCandidates': encodeList(topCandidates),
    if (chosenCandidates.isNotDefault)
      'chosenCandidates': encodeList(chosenCandidates),
  };

  @override
  String toString() {
    final contents = [
      if (logProbabilitySum != null) 'logProbabilitySum=$logProbabilitySum',
    ].join(',');
    return 'LogprobsResult($contents)';
  }
}

/// Candidate for the logprobs token and score.
final class LogprobsResult_Candidate extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.LogprobsResult.Candidate';

  /// The candidates token string value.
  final String? token;

  /// The candidates token id value.
  final int? tokenId;

  /// The candidate's log probability.
  final double? logProbability;

  LogprobsResult_Candidate({this.token, this.tokenId, this.logProbability})
    : super(fullyQualifiedName);

  factory LogprobsResult_Candidate.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return LogprobsResult_Candidate(
      token: switch (json['token']) {
        null => null,
        Object $1 => decodeString($1),
      },
      tokenId: switch (json['tokenId']) {
        null => null,
        Object $1 => decodeInt($1),
      },
      logProbability: switch (json['logProbability']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (token != null) 'token': token,
    if (tokenId != null) 'tokenId': tokenId,
    if (logProbability != null) 'logProbability': encodeDouble(logProbability),
  };

  @override
  String toString() {
    final contents = [
      if (token != null) 'token=$token',
      if (tokenId != null) 'tokenId=$tokenId',
      if (logProbability != null) 'logProbability=$logProbability',
    ].join(',');
    return 'Candidate($contents)';
  }
}

/// Candidates with top log probabilities at each decoding step.
final class LogprobsResult_TopCandidates extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.LogprobsResult.TopCandidates';

  /// Sorted by log probability in descending order.
  final List<LogprobsResult_Candidate> candidates;

  LogprobsResult_TopCandidates({this.candidates = const []})
    : super(fullyQualifiedName);

  factory LogprobsResult_TopCandidates.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return LogprobsResult_TopCandidates(
      candidates: switch (json['candidates']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) LogprobsResult_Candidate.fromJson(i),
        ],
        _ => throw const FormatException('"candidates" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (candidates.isNotDefault) 'candidates': encodeList(candidates),
  };

  @override
  String toString() => 'TopCandidates()';
}

/// Identifier for the source contributing to this attribution.
final class AttributionSourceId extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.AttributionSourceId';

  /// Identifier for an inline passage.
  final AttributionSourceId_GroundingPassageId? groundingPassage;

  /// Identifier for a `Chunk` fetched via Semantic Retriever.
  final AttributionSourceId_SemanticRetrieverChunk? semanticRetrieverChunk;

  AttributionSourceId({this.groundingPassage, this.semanticRetrieverChunk})
    : super(fullyQualifiedName);

  factory AttributionSourceId.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return AttributionSourceId(
      groundingPassage: switch (json['groundingPassage']) {
        null => null,
        Object $1 => AttributionSourceId_GroundingPassageId.fromJson($1),
      },
      semanticRetrieverChunk: switch (json['semanticRetrieverChunk']) {
        null => null,
        Object $1 => AttributionSourceId_SemanticRetrieverChunk.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (groundingPassage != null)
      'groundingPassage': groundingPassage!.toJson(),
    if (semanticRetrieverChunk != null)
      'semanticRetrieverChunk': semanticRetrieverChunk!.toJson(),
  };

  @override
  String toString() => 'AttributionSourceId()';
}

/// Identifier for a part within a `GroundingPassage`.
final class AttributionSourceId_GroundingPassageId extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.AttributionSourceId.GroundingPassageId';

  /// Output only. ID of the passage matching the `GenerateAnswerRequest`'s
  /// `GroundingPassage.id`.
  final String passageId;

  /// Output only. Index of the part within the `GenerateAnswerRequest`'s
  /// `GroundingPassage.content`.
  final int partIndex;

  AttributionSourceId_GroundingPassageId({
    this.passageId = '',
    this.partIndex = 0,
  }) : super(fullyQualifiedName);

  factory AttributionSourceId_GroundingPassageId.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return AttributionSourceId_GroundingPassageId(
      passageId: switch (json['passageId']) {
        null => '',
        Object $1 => decodeString($1),
      },
      partIndex: switch (json['partIndex']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (passageId.isNotDefault) 'passageId': passageId,
    if (partIndex.isNotDefault) 'partIndex': partIndex,
  };

  @override
  String toString() {
    final contents = ['passageId=$passageId', 'partIndex=$partIndex'].join(',');
    return 'GroundingPassageId($contents)';
  }
}

/// Identifier for a `Chunk` retrieved via Semantic Retriever specified in the
/// `GenerateAnswerRequest` using `SemanticRetrieverConfig`.
final class AttributionSourceId_SemanticRetrieverChunk extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.AttributionSourceId.SemanticRetrieverChunk';

  /// Output only. Name of the source matching the request's
  /// `SemanticRetrieverConfig.source`. Example: `corpora/123` or
  /// `corpora/123/documents/abc`
  final String source;

  /// Output only. Name of the `Chunk` containing the attributed text.
  /// Example: `corpora/123/documents/abc/chunks/xyz`
  final String chunk;

  AttributionSourceId_SemanticRetrieverChunk({
    this.source = '',
    this.chunk = '',
  }) : super(fullyQualifiedName);

  factory AttributionSourceId_SemanticRetrieverChunk.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return AttributionSourceId_SemanticRetrieverChunk(
      source: switch (json['source']) {
        null => '',
        Object $1 => decodeString($1),
      },
      chunk: switch (json['chunk']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (source.isNotDefault) 'source': source,
    if (chunk.isNotDefault) 'chunk': chunk,
  };

  @override
  String toString() {
    final contents = ['source=$source', 'chunk=$chunk'].join(',');
    return 'SemanticRetrieverChunk($contents)';
  }
}

/// Attribution for a source that contributed to an answer.
final class GroundingAttribution extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GroundingAttribution';

  /// Output only. Identifier for the source contributing to this attribution.
  final AttributionSourceId? sourceId;

  /// Grounding source content that makes up this attribution.
  final Content? content;

  GroundingAttribution({this.sourceId, this.content})
    : super(fullyQualifiedName);

  factory GroundingAttribution.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GroundingAttribution(
      sourceId: switch (json['sourceId']) {
        null => null,
        Object $1 => AttributionSourceId.fromJson($1),
      },
      content: switch (json['content']) {
        null => null,
        Object $1 => Content.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (sourceId != null) 'sourceId': sourceId!.toJson(),
    if (content != null) 'content': content!.toJson(),
  };

  @override
  String toString() => 'GroundingAttribution()';
}

/// Metadata related to retrieval in the grounding flow.
final class RetrievalMetadata extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.RetrievalMetadata';

  /// Optional. Score indicating how likely information from google search could
  /// help answer the prompt. The score is in the range [0, 1], where 0 is the
  /// least likely and 1 is the most likely. This score is only populated when
  /// google search grounding and dynamic retrieval is enabled. It will be
  /// compared to the threshold to determine whether to trigger google search.
  final double googleSearchDynamicRetrievalScore;

  RetrievalMetadata({this.googleSearchDynamicRetrievalScore = 0})
    : super(fullyQualifiedName);

  factory RetrievalMetadata.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return RetrievalMetadata(
      googleSearchDynamicRetrievalScore:
          switch (json['googleSearchDynamicRetrievalScore']) {
            null => 0,
            Object $1 => decodeDouble($1),
          },
    );
  }

  @override
  Object toJson() => {
    if (googleSearchDynamicRetrievalScore.isNotDefault)
      'googleSearchDynamicRetrievalScore': encodeDouble(
        googleSearchDynamicRetrievalScore,
      ),
  };

  @override
  String toString() {
    final contents = [
      'googleSearchDynamicRetrievalScore=$googleSearchDynamicRetrievalScore',
    ].join(',');
    return 'RetrievalMetadata($contents)';
  }
}

/// Metadata returned to client when grounding is enabled.
final class GroundingMetadata extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GroundingMetadata';

  /// Optional. Google search entry for the following-up web searches.
  final SearchEntryPoint? searchEntryPoint;

  /// List of supporting references retrieved from specified grounding source.
  final List<GroundingChunk> groundingChunks;

  /// List of grounding support.
  final List<GroundingSupport> groundingSupports;

  /// Metadata related to retrieval in the grounding flow.
  final RetrievalMetadata? retrievalMetadata;

  /// Web search queries for the following-up web search.
  final List<String> webSearchQueries;

  GroundingMetadata({
    this.searchEntryPoint,
    this.groundingChunks = const [],
    this.groundingSupports = const [],
    this.retrievalMetadata,
    this.webSearchQueries = const [],
  }) : super(fullyQualifiedName);

  factory GroundingMetadata.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GroundingMetadata(
      searchEntryPoint: switch (json['searchEntryPoint']) {
        null => null,
        Object $1 => SearchEntryPoint.fromJson($1),
      },
      groundingChunks: switch (json['groundingChunks']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) GroundingChunk.fromJson(i)],
        _ => throw const FormatException('"groundingChunks" is not a list'),
      },
      groundingSupports: switch (json['groundingSupports']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) GroundingSupport.fromJson(i)],
        _ => throw const FormatException('"groundingSupports" is not a list'),
      },
      retrievalMetadata: switch (json['retrievalMetadata']) {
        null => null,
        Object $1 => RetrievalMetadata.fromJson($1),
      },
      webSearchQueries: switch (json['webSearchQueries']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) decodeString(i)],
        _ => throw const FormatException('"webSearchQueries" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (searchEntryPoint != null)
      'searchEntryPoint': searchEntryPoint!.toJson(),
    if (groundingChunks.isNotDefault)
      'groundingChunks': encodeList(groundingChunks),
    if (groundingSupports.isNotDefault)
      'groundingSupports': encodeList(groundingSupports),
    if (retrievalMetadata != null)
      'retrievalMetadata': retrievalMetadata!.toJson(),
    if (webSearchQueries.isNotDefault) 'webSearchQueries': webSearchQueries,
  };

  @override
  String toString() => 'GroundingMetadata()';
}

/// Google search entry point.
final class SearchEntryPoint extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.SearchEntryPoint';

  /// Optional. Web content snippet that can be embedded in a web page or an app
  /// webview.
  final String renderedContent;

  /// Optional. Base64 encoded JSON representing array of <search term, search
  /// url> tuple.
  final Uint8List sdkBlob;

  SearchEntryPoint({this.renderedContent = '', Uint8List? sdkBlob})
    : sdkBlob = sdkBlob ?? Uint8List(0),
      super(fullyQualifiedName);

  factory SearchEntryPoint.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return SearchEntryPoint(
      renderedContent: switch (json['renderedContent']) {
        null => '',
        Object $1 => decodeString($1),
      },
      sdkBlob: switch (json['sdkBlob']) {
        null => Uint8List(0),
        Object $1 => decodeBytes($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (renderedContent.isNotDefault) 'renderedContent': renderedContent,
    if (sdkBlob.isNotDefault) 'sdkBlob': encodeBytes(sdkBlob),
  };

  @override
  String toString() {
    final contents = [
      'renderedContent=$renderedContent',
      'sdkBlob=$sdkBlob',
    ].join(',');
    return 'SearchEntryPoint($contents)';
  }
}

/// Grounding chunk.
final class GroundingChunk extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GroundingChunk';

  /// Grounding chunk from the web.
  final GroundingChunk_Web? web;

  GroundingChunk({this.web}) : super(fullyQualifiedName);

  factory GroundingChunk.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GroundingChunk(
      web: switch (json['web']) {
        null => null,
        Object $1 => GroundingChunk_Web.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {if (web != null) 'web': web!.toJson()};

  @override
  String toString() => 'GroundingChunk()';
}

/// Chunk from the web.
final class GroundingChunk_Web extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GroundingChunk.Web';

  /// URI reference of the chunk.
  final String? uri;

  /// Title of the chunk.
  final String? title;

  GroundingChunk_Web({this.uri, this.title}) : super(fullyQualifiedName);

  factory GroundingChunk_Web.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GroundingChunk_Web(
      uri: switch (json['uri']) {
        null => null,
        Object $1 => decodeString($1),
      },
      title: switch (json['title']) {
        null => null,
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (uri != null) 'uri': uri,
    if (title != null) 'title': title,
  };

  @override
  String toString() {
    final contents = [
      if (uri != null) 'uri=$uri',
      if (title != null) 'title=$title',
    ].join(',');
    return 'Web($contents)';
  }
}

/// Segment of the content.
final class Segment extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Segment';

  /// Output only. The index of a Part object within its parent Content object.
  final int partIndex;

  /// Output only. Start index in the given Part, measured in bytes. Offset from
  /// the start of the Part, inclusive, starting at zero.
  final int startIndex;

  /// Output only. End index in the given Part, measured in bytes. Offset from
  /// the start of the Part, exclusive, starting at zero.
  final int endIndex;

  /// Output only. The text corresponding to the segment from the response.
  final String text;

  Segment({
    this.partIndex = 0,
    this.startIndex = 0,
    this.endIndex = 0,
    this.text = '',
  }) : super(fullyQualifiedName);

  factory Segment.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Segment(
      partIndex: switch (json['partIndex']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      startIndex: switch (json['startIndex']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      endIndex: switch (json['endIndex']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      text: switch (json['text']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (partIndex.isNotDefault) 'partIndex': partIndex,
    if (startIndex.isNotDefault) 'startIndex': startIndex,
    if (endIndex.isNotDefault) 'endIndex': endIndex,
    if (text.isNotDefault) 'text': text,
  };

  @override
  String toString() {
    final contents = [
      'partIndex=$partIndex',
      'startIndex=$startIndex',
      'endIndex=$endIndex',
      'text=$text',
    ].join(',');
    return 'Segment($contents)';
  }
}

/// Grounding support.
final class GroundingSupport extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GroundingSupport';

  /// Segment of the content this support belongs to.
  final Segment? segment;

  /// A list of indices (into 'grounding_chunk') specifying the
  /// citations associated with the claim. For instance [1,3,4] means
  /// that grounding_chunk[1], grounding_chunk[3],
  /// grounding_chunk[4] are the retrieved content attributed to the claim.
  final List<int> groundingChunkIndices;

  /// Confidence score of the support references. Ranges from 0 to 1. 1 is the
  /// most confident. This list must have the same size as the
  /// grounding_chunk_indices.
  final List<double> confidenceScores;

  GroundingSupport({
    this.segment,
    this.groundingChunkIndices = const [],
    this.confidenceScores = const [],
  }) : super(fullyQualifiedName);

  factory GroundingSupport.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GroundingSupport(
      segment: switch (json['segment']) {
        null => null,
        Object $1 => Segment.fromJson($1),
      },
      groundingChunkIndices: switch (json['groundingChunkIndices']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) decodeInt(i)],
        _ => throw const FormatException(
          '"groundingChunkIndices" is not a list',
        ),
      },
      confidenceScores: switch (json['confidenceScores']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) decodeDouble(i)],
        _ => throw const FormatException('"confidenceScores" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (segment != null) 'segment': segment!.toJson(),
    if (groundingChunkIndices.isNotDefault)
      'groundingChunkIndices': groundingChunkIndices,
    if (confidenceScores.isNotDefault) 'confidenceScores': confidenceScores,
  };

  @override
  String toString() => 'GroundingSupport()';
}

/// Request to generate a grounded answer from the `Model`.
final class GenerateAnswerRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GenerateAnswerRequest';

  /// Passages provided inline with the request.
  final GroundingPassages? inlinePassages;

  /// Content retrieved from resources created via the Semantic Retriever
  /// API.
  final SemanticRetrieverConfig? semanticRetriever;

  /// Required. The name of the `Model` to use for generating the grounded
  /// response.
  ///
  /// Format: `model=models/{model}`.
  final String model;

  /// Required. The content of the current conversation with the `Model`. For
  /// single-turn queries, this is a single question to answer. For multi-turn
  /// queries, this is a repeated field that contains conversation history and
  /// the last `Content` in the list containing the question.
  ///
  /// Note: `GenerateAnswer` only supports queries in English.
  final List<Content> contents;

  /// Required. Style in which answers should be returned.
  final GenerateAnswerRequest_AnswerStyle answerStyle;

  /// Optional. A list of unique `SafetySetting` instances for blocking unsafe
  /// content.
  ///
  /// This will be enforced on the `GenerateAnswerRequest.contents` and
  /// `GenerateAnswerResponse.candidate`. There should not be more than one
  /// setting for each `SafetyCategory` type. The API will block any contents and
  /// responses that fail to meet the thresholds set by these settings. This list
  /// overrides the default settings for each `SafetyCategory` specified in the
  /// safety_settings. If there is no `SafetySetting` for a given
  /// `SafetyCategory` provided in the list, the API will use the default safety
  /// setting for that category. Harm categories HARM_CATEGORY_HATE_SPEECH,
  /// HARM_CATEGORY_SEXUALLY_EXPLICIT, HARM_CATEGORY_DANGEROUS_CONTENT,
  /// HARM_CATEGORY_HARASSMENT are supported.
  /// Refer to the
  /// [guide](https://ai.google.dev/gemini-api/docs/safety-settings)
  /// for detailed information on available safety settings. Also refer to the
  /// [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance) to
  /// learn how to incorporate safety considerations in your AI applications.
  final List<SafetySetting> safetySettings;

  /// Optional. Controls the randomness of the output.
  ///
  /// Values can range from [0.0,1.0], inclusive. A value closer to 1.0 will
  /// produce responses that are more varied and creative, while a value closer
  /// to 0.0 will typically result in more straightforward responses from the
  /// model. A low temperature (~0.2) is usually recommended for
  /// Attributed-Question-Answering use cases.
  final double? temperature;

  GenerateAnswerRequest({
    this.inlinePassages,
    this.semanticRetriever,
    required this.model,
    required this.contents,
    required this.answerStyle,
    this.safetySettings = const [],
    this.temperature,
  }) : super(fullyQualifiedName);

  factory GenerateAnswerRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GenerateAnswerRequest(
      inlinePassages: switch (json['inlinePassages']) {
        null => null,
        Object $1 => GroundingPassages.fromJson($1),
      },
      semanticRetriever: switch (json['semanticRetriever']) {
        null => null,
        Object $1 => SemanticRetrieverConfig.fromJson($1),
      },
      model: switch (json['model']) {
        null => '',
        Object $1 => decodeString($1),
      },
      contents: switch (json['contents']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Content.fromJson(i)],
        _ => throw const FormatException('"contents" is not a list'),
      },
      answerStyle: switch (json['answerStyle']) {
        null => GenerateAnswerRequest_AnswerStyle.$default,
        Object $1 => GenerateAnswerRequest_AnswerStyle.fromJson($1),
      },
      safetySettings: switch (json['safetySettings']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) SafetySetting.fromJson(i)],
        _ => throw const FormatException('"safetySettings" is not a list'),
      },
      temperature: switch (json['temperature']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (inlinePassages != null) 'inlinePassages': inlinePassages!.toJson(),
    if (semanticRetriever != null)
      'semanticRetriever': semanticRetriever!.toJson(),
    'model': model,
    'contents': encodeList(contents),
    'answerStyle': answerStyle.toJson(),
    if (safetySettings.isNotDefault)
      'safetySettings': encodeList(safetySettings),
    if (temperature != null) 'temperature': encodeDouble(temperature),
  };

  @override
  String toString() {
    final contents = [
      'model=$model',
      'answerStyle=$answerStyle',
      if (temperature != null) 'temperature=$temperature',
    ].join(',');
    return 'GenerateAnswerRequest($contents)';
  }
}

/// Style for grounded answers.
final class GenerateAnswerRequest_AnswerStyle extends ProtoEnum {
  /// Unspecified answer style.
  static const answerStyleUnspecified = GenerateAnswerRequest_AnswerStyle(
    'ANSWER_STYLE_UNSPECIFIED',
  );

  /// Succinct but abstract style.
  static const abstractive = GenerateAnswerRequest_AnswerStyle('ABSTRACTIVE');

  /// Very brief and extractive style.
  static const extractive = GenerateAnswerRequest_AnswerStyle('EXTRACTIVE');

  /// Verbose style including extra details. The response may be formatted as a
  /// sentence, paragraph, multiple paragraphs, or bullet points, etc.
  static const verbose = GenerateAnswerRequest_AnswerStyle('VERBOSE');

  /// The default value for [GenerateAnswerRequest_AnswerStyle].
  static const $default = answerStyleUnspecified;

  const GenerateAnswerRequest_AnswerStyle(super.value);

  factory GenerateAnswerRequest_AnswerStyle.fromJson(Object? json) =>
      GenerateAnswerRequest_AnswerStyle(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'AnswerStyle.$value';
}

/// Response from the model for a grounded answer.
final class GenerateAnswerResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GenerateAnswerResponse';

  /// Candidate answer from the model.
  ///
  /// Note: The model *always* attempts to provide a grounded answer, even when
  /// the answer is unlikely to be answerable from the given passages.
  /// In that case, a low-quality or ungrounded answer may be provided, along
  /// with a low `answerable_probability`.
  final Candidate? answer;

  /// Output only. The model's estimate of the probability that its answer is
  /// correct and grounded in the input passages.
  ///
  /// A low `answerable_probability` indicates that the answer might not be
  /// grounded in the sources.
  ///
  /// When `answerable_probability` is low, you may want to:
  ///
  /// * Display a message to the effect of "We couldnt answer that question" to
  /// the user.
  /// * Fall back to a general-purpose LLM that answers the question from world
  /// knowledge. The threshold and nature of such fallbacks will depend on
  /// individual use cases. `0.5` is a good starting threshold.
  final double? answerableProbability;

  /// Output only. Feedback related to the input data used to answer the
  /// question, as opposed to the model-generated response to the question.
  ///
  /// The input data can be one or more of the following:
  ///
  /// - Question specified by the last entry in `GenerateAnswerRequest.content`
  /// - Conversation history specified by the other entries in
  /// `GenerateAnswerRequest.content`
  /// - Grounding sources (`GenerateAnswerRequest.semantic_retriever` or
  /// `GenerateAnswerRequest.inline_passages`)
  final GenerateAnswerResponse_InputFeedback? inputFeedback;

  GenerateAnswerResponse({
    this.answer,
    this.answerableProbability,
    this.inputFeedback,
  }) : super(fullyQualifiedName);

  factory GenerateAnswerResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GenerateAnswerResponse(
      answer: switch (json['answer']) {
        null => null,
        Object $1 => Candidate.fromJson($1),
      },
      answerableProbability: switch (json['answerableProbability']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      inputFeedback: switch (json['inputFeedback']) {
        null => null,
        Object $1 => GenerateAnswerResponse_InputFeedback.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (answer != null) 'answer': answer!.toJson(),
    if (answerableProbability != null)
      'answerableProbability': encodeDouble(answerableProbability),
    if (inputFeedback != null) 'inputFeedback': inputFeedback!.toJson(),
  };

  @override
  String toString() {
    final contents = [
      if (answerableProbability != null)
        'answerableProbability=$answerableProbability',
    ].join(',');
    return 'GenerateAnswerResponse($contents)';
  }
}

/// Feedback related to the input data used to answer the question, as opposed
/// to the model-generated response to the question.
final class GenerateAnswerResponse_InputFeedback extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GenerateAnswerResponse.InputFeedback';

  /// Optional. If set, the input was blocked and no candidates are returned.
  /// Rephrase the input.
  final GenerateAnswerResponse_InputFeedback_BlockReason? blockReason;

  /// Ratings for safety of the input.
  /// There is at most one rating per category.
  final List<SafetyRating> safetyRatings;

  GenerateAnswerResponse_InputFeedback({
    this.blockReason,
    this.safetyRatings = const [],
  }) : super(fullyQualifiedName);

  factory GenerateAnswerResponse_InputFeedback.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GenerateAnswerResponse_InputFeedback(
      blockReason: switch (json['blockReason']) {
        null => null,
        Object $1 => GenerateAnswerResponse_InputFeedback_BlockReason.fromJson(
          $1,
        ),
      },
      safetyRatings: switch (json['safetyRatings']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) SafetyRating.fromJson(i)],
        _ => throw const FormatException('"safetyRatings" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (blockReason != null) 'blockReason': blockReason!.toJson(),
    if (safetyRatings.isNotDefault) 'safetyRatings': encodeList(safetyRatings),
  };

  @override
  String toString() {
    final contents = [
      if (blockReason != null) 'blockReason=$blockReason',
    ].join(',');
    return 'InputFeedback($contents)';
  }
}

/// Specifies what was the reason why input was blocked.
final class GenerateAnswerResponse_InputFeedback_BlockReason extends ProtoEnum {
  /// Default value. This value is unused.
  static const blockReasonUnspecified =
      GenerateAnswerResponse_InputFeedback_BlockReason(
        'BLOCK_REASON_UNSPECIFIED',
      );

  /// Input was blocked due to safety reasons. Inspect
  /// `safety_ratings` to understand which safety category blocked it.
  static const safety = GenerateAnswerResponse_InputFeedback_BlockReason(
    'SAFETY',
  );

  /// Input was blocked due to other reasons.
  static const other = GenerateAnswerResponse_InputFeedback_BlockReason(
    'OTHER',
  );

  /// The default value for [GenerateAnswerResponse_InputFeedback_BlockReason].
  static const $default = blockReasonUnspecified;

  const GenerateAnswerResponse_InputFeedback_BlockReason(super.value);

  factory GenerateAnswerResponse_InputFeedback_BlockReason.fromJson(
    Object? json,
  ) => GenerateAnswerResponse_InputFeedback_BlockReason(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'BlockReason.$value';
}

/// Request containing the `Content` for the model to embed.
final class EmbedContentRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.EmbedContentRequest';

  /// Required. The model's resource name. This serves as an ID for the Model to
  /// use.
  ///
  /// This name should match a model name returned by the `ListModels` method.
  ///
  /// Format: `models/{model}`
  final String model;

  /// Required. The content to embed. Only the `parts.text` fields will be
  /// counted.
  final Content? content;

  /// Optional. Optional task type for which the embeddings will be used. Not
  /// supported on earlier models (`models/embedding-001`).
  final TaskType? taskType;

  /// Optional. An optional title for the text. Only applicable when TaskType is
  /// `RETRIEVAL_DOCUMENT`.
  ///
  /// Note: Specifying a `title` for `RETRIEVAL_DOCUMENT` provides better quality
  /// embeddings for retrieval.
  final String? title;

  /// Optional. Optional reduced dimension for the output embedding. If set,
  /// excessive values in the output embedding are truncated from the end.
  /// Supported by newer models since 2024 only. You cannot set this value if
  /// using the earlier model (`models/embedding-001`).
  final int? outputDimensionality;

  EmbedContentRequest({
    required this.model,
    required this.content,
    this.taskType,
    this.title,
    this.outputDimensionality,
  }) : super(fullyQualifiedName);

  factory EmbedContentRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return EmbedContentRequest(
      model: switch (json['model']) {
        null => '',
        Object $1 => decodeString($1),
      },
      content: switch (json['content']) {
        null => null,
        Object $1 => Content.fromJson($1),
      },
      taskType: switch (json['taskType']) {
        null => null,
        Object $1 => TaskType.fromJson($1),
      },
      title: switch (json['title']) {
        null => null,
        Object $1 => decodeString($1),
      },
      outputDimensionality: switch (json['outputDimensionality']) {
        null => null,
        Object $1 => decodeInt($1),
      },
    );
  }

  @override
  Object toJson() => {
    'model': model,
    if (content != null) 'content': content!.toJson(),
    if (taskType != null) 'taskType': taskType!.toJson(),
    if (title != null) 'title': title,
    if (outputDimensionality != null)
      'outputDimensionality': outputDimensionality,
  };

  @override
  String toString() {
    final contents = [
      'model=$model',
      if (taskType != null) 'taskType=$taskType',
      if (title != null) 'title=$title',
      if (outputDimensionality != null)
        'outputDimensionality=$outputDimensionality',
    ].join(',');
    return 'EmbedContentRequest($contents)';
  }
}

/// A list of floats representing an embedding.
final class ContentEmbedding extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ContentEmbedding';

  /// The embedding values.
  final List<double> values;

  ContentEmbedding({this.values = const []}) : super(fullyQualifiedName);

  factory ContentEmbedding.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ContentEmbedding(
      values: switch (json['values']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) decodeDouble(i)],
        _ => throw const FormatException('"values" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {if (values.isNotDefault) 'values': values};

  @override
  String toString() => 'ContentEmbedding()';
}

/// The response to an `EmbedContentRequest`.
final class EmbedContentResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.EmbedContentResponse';

  /// Output only. The embedding generated from the input content.
  final ContentEmbedding? embedding;

  EmbedContentResponse({this.embedding}) : super(fullyQualifiedName);

  factory EmbedContentResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return EmbedContentResponse(
      embedding: switch (json['embedding']) {
        null => null,
        Object $1 => ContentEmbedding.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {if (embedding != null) 'embedding': embedding!.toJson()};

  @override
  String toString() => 'EmbedContentResponse()';
}

/// Batch request to get embeddings from the model for a list of prompts.
final class BatchEmbedContentsRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BatchEmbedContentsRequest';

  /// Required. The model's resource name. This serves as an ID for the Model to
  /// use.
  ///
  /// This name should match a model name returned by the `ListModels` method.
  ///
  /// Format: `models/{model}`
  final String model;

  /// Required. Embed requests for the batch. The model in each of these requests
  /// must match the model specified `BatchEmbedContentsRequest.model`.
  final List<EmbedContentRequest> requests;

  BatchEmbedContentsRequest({required this.model, required this.requests})
    : super(fullyQualifiedName);

  factory BatchEmbedContentsRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BatchEmbedContentsRequest(
      model: switch (json['model']) {
        null => '',
        Object $1 => decodeString($1),
      },
      requests: switch (json['requests']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) EmbedContentRequest.fromJson(i),
        ],
        _ => throw const FormatException('"requests" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {'model': model, 'requests': encodeList(requests)};

  @override
  String toString() {
    final contents = ['model=$model'].join(',');
    return 'BatchEmbedContentsRequest($contents)';
  }
}

/// The response to a `BatchEmbedContentsRequest`.
final class BatchEmbedContentsResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BatchEmbedContentsResponse';

  /// Output only. The embeddings for each request, in the same order as provided
  /// in the batch request.
  final List<ContentEmbedding> embeddings;

  BatchEmbedContentsResponse({this.embeddings = const []})
    : super(fullyQualifiedName);

  factory BatchEmbedContentsResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BatchEmbedContentsResponse(
      embeddings: switch (json['embeddings']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) ContentEmbedding.fromJson(i)],
        _ => throw const FormatException('"embeddings" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (embeddings.isNotDefault) 'embeddings': encodeList(embeddings),
  };

  @override
  String toString() => 'BatchEmbedContentsResponse()';
}

/// Counts the number of tokens in the `prompt` sent to a model.
///
/// Models may tokenize text differently, so each model may return a different
/// `token_count`.
final class CountTokensRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CountTokensRequest';

  /// Required. The model's resource name. This serves as an ID for the Model to
  /// use.
  ///
  /// This name should match a model name returned by the `ListModels` method.
  ///
  /// Format: `models/{model}`
  final String model;

  /// Optional. The input given to the model as a prompt. This field is ignored
  /// when `generate_content_request` is set.
  final List<Content> contents;

  /// Optional. The overall input given to the `Model`. This includes the prompt
  /// as well as other model steering information like [system
  /// instructions](https://ai.google.dev/gemini-api/docs/system-instructions),
  /// and/or function declarations for [function
  /// calling](https://ai.google.dev/gemini-api/docs/function-calling).
  /// `Model`s/`Content`s and `generate_content_request`s are mutually
  /// exclusive. You can either send `Model` + `Content`s or a
  /// `generate_content_request`, but never both.
  final GenerateContentRequest? generateContentRequest;

  CountTokensRequest({
    required this.model,
    this.contents = const [],
    this.generateContentRequest,
  }) : super(fullyQualifiedName);

  factory CountTokensRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CountTokensRequest(
      model: switch (json['model']) {
        null => '',
        Object $1 => decodeString($1),
      },
      contents: switch (json['contents']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Content.fromJson(i)],
        _ => throw const FormatException('"contents" is not a list'),
      },
      generateContentRequest: switch (json['generateContentRequest']) {
        null => null,
        Object $1 => GenerateContentRequest.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    'model': model,
    if (contents.isNotDefault) 'contents': encodeList(contents),
    if (generateContentRequest != null)
      'generateContentRequest': generateContentRequest!.toJson(),
  };

  @override
  String toString() {
    final contents = ['model=$model'].join(',');
    return 'CountTokensRequest($contents)';
  }
}

/// A response from `CountTokens`.
///
/// It returns the model's `token_count` for the `prompt`.
final class CountTokensResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CountTokensResponse';

  /// The number of tokens that the `Model` tokenizes the `prompt` into. Always
  /// non-negative.
  final int totalTokens;

  /// Number of tokens in the cached part of the prompt (the cached content).
  final int cachedContentTokenCount;

  /// Output only. List of modalities that were processed in the request input.
  final List<ModalityTokenCount> promptTokensDetails;

  /// Output only. List of modalities that were processed in the cached content.
  final List<ModalityTokenCount> cacheTokensDetails;

  CountTokensResponse({
    this.totalTokens = 0,
    this.cachedContentTokenCount = 0,
    this.promptTokensDetails = const [],
    this.cacheTokensDetails = const [],
  }) : super(fullyQualifiedName);

  factory CountTokensResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CountTokensResponse(
      totalTokens: switch (json['totalTokens']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      cachedContentTokenCount: switch (json['cachedContentTokenCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      promptTokensDetails: switch (json['promptTokensDetails']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) ModalityTokenCount.fromJson(i),
        ],
        _ => throw const FormatException('"promptTokensDetails" is not a list'),
      },
      cacheTokensDetails: switch (json['cacheTokensDetails']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) ModalityTokenCount.fromJson(i),
        ],
        _ => throw const FormatException('"cacheTokensDetails" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (totalTokens.isNotDefault) 'totalTokens': totalTokens,
    if (cachedContentTokenCount.isNotDefault)
      'cachedContentTokenCount': cachedContentTokenCount,
    if (promptTokensDetails.isNotDefault)
      'promptTokensDetails': encodeList(promptTokensDetails),
    if (cacheTokensDetails.isNotDefault)
      'cacheTokensDetails': encodeList(cacheTokensDetails),
  };

  @override
  String toString() {
    final contents = [
      'totalTokens=$totalTokens',
      'cachedContentTokenCount=$cachedContentTokenCount',
    ].join(',');
    return 'CountTokensResponse($contents)';
  }
}

/// Configures the realtime input behavior in `BidiGenerateContent`.
final class RealtimeInputConfig extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.RealtimeInputConfig';

  /// Optional. If not set, automatic activity detection is enabled by default.
  /// If automatic voice detection is disabled, the client must send activity
  /// signals.
  final RealtimeInputConfig_AutomaticActivityDetection?
  automaticActivityDetection;

  /// Optional. Defines what effect activity has.
  final RealtimeInputConfig_ActivityHandling? activityHandling;

  /// Optional. Defines which input is included in the user's turn.
  final RealtimeInputConfig_TurnCoverage? turnCoverage;

  RealtimeInputConfig({
    this.automaticActivityDetection,
    this.activityHandling,
    this.turnCoverage,
  }) : super(fullyQualifiedName);

  factory RealtimeInputConfig.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return RealtimeInputConfig(
      automaticActivityDetection: switch (json['automaticActivityDetection']) {
        null => null,
        Object $1 => RealtimeInputConfig_AutomaticActivityDetection.fromJson(
          $1,
        ),
      },
      activityHandling: switch (json['activityHandling']) {
        null => null,
        Object $1 => RealtimeInputConfig_ActivityHandling.fromJson($1),
      },
      turnCoverage: switch (json['turnCoverage']) {
        null => null,
        Object $1 => RealtimeInputConfig_TurnCoverage.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (automaticActivityDetection != null)
      'automaticActivityDetection': automaticActivityDetection!.toJson(),
    if (activityHandling != null)
      'activityHandling': activityHandling!.toJson(),
    if (turnCoverage != null) 'turnCoverage': turnCoverage!.toJson(),
  };

  @override
  String toString() {
    final contents = [
      if (activityHandling != null) 'activityHandling=$activityHandling',
      if (turnCoverage != null) 'turnCoverage=$turnCoverage',
    ].join(',');
    return 'RealtimeInputConfig($contents)';
  }
}

/// Configures automatic detection of activity.
final class RealtimeInputConfig_AutomaticActivityDetection
    extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.RealtimeInputConfig.AutomaticActivityDetection';

  /// Optional. If enabled (the default), detected voice and text input count
  /// as activity. If disabled, the client must send activity signals.
  final bool? disabled;

  /// Optional. Determines how likely speech is to be detected.
  final RealtimeInputConfig_AutomaticActivityDetection_StartSensitivity?
  startOfSpeechSensitivity;

  /// Optional. The required duration of detected speech before start-of-speech
  /// is committed. The lower this value, the more sensitive the
  /// start-of-speech detection is and shorter speech can be recognized.
  /// However, this also increases the probability of false positives.
  final int? prefixPaddingMs;

  /// Optional. Determines how likely detected speech is ended.
  final RealtimeInputConfig_AutomaticActivityDetection_EndSensitivity?
  endOfSpeechSensitivity;

  /// Optional. The required duration of detected non-speech (e.g. silence)
  /// before end-of-speech is committed. The larger this value, the longer
  /// speech gaps can be without interrupting the user's activity but this will
  /// increase the model's latency.
  final int? silenceDurationMs;

  RealtimeInputConfig_AutomaticActivityDetection({
    this.disabled,
    this.startOfSpeechSensitivity,
    this.prefixPaddingMs,
    this.endOfSpeechSensitivity,
    this.silenceDurationMs,
  }) : super(fullyQualifiedName);

  factory RealtimeInputConfig_AutomaticActivityDetection.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return RealtimeInputConfig_AutomaticActivityDetection(
      disabled: switch (json['disabled']) {
        null => null,
        Object $1 => decodeBool($1),
      },
      startOfSpeechSensitivity: switch (json['startOfSpeechSensitivity']) {
        null => null,
        Object $1 =>
          RealtimeInputConfig_AutomaticActivityDetection_StartSensitivity.fromJson(
            $1,
          ),
      },
      prefixPaddingMs: switch (json['prefixPaddingMs']) {
        null => null,
        Object $1 => decodeInt($1),
      },
      endOfSpeechSensitivity: switch (json['endOfSpeechSensitivity']) {
        null => null,
        Object $1 =>
          RealtimeInputConfig_AutomaticActivityDetection_EndSensitivity.fromJson(
            $1,
          ),
      },
      silenceDurationMs: switch (json['silenceDurationMs']) {
        null => null,
        Object $1 => decodeInt($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (disabled != null) 'disabled': disabled,
    if (startOfSpeechSensitivity != null)
      'startOfSpeechSensitivity': startOfSpeechSensitivity!.toJson(),
    if (prefixPaddingMs != null) 'prefixPaddingMs': prefixPaddingMs,
    if (endOfSpeechSensitivity != null)
      'endOfSpeechSensitivity': endOfSpeechSensitivity!.toJson(),
    if (silenceDurationMs != null) 'silenceDurationMs': silenceDurationMs,
  };

  @override
  String toString() {
    final contents = [
      if (disabled != null) 'disabled=$disabled',
      if (startOfSpeechSensitivity != null)
        'startOfSpeechSensitivity=$startOfSpeechSensitivity',
      if (prefixPaddingMs != null) 'prefixPaddingMs=$prefixPaddingMs',
      if (endOfSpeechSensitivity != null)
        'endOfSpeechSensitivity=$endOfSpeechSensitivity',
      if (silenceDurationMs != null) 'silenceDurationMs=$silenceDurationMs',
    ].join(',');
    return 'AutomaticActivityDetection($contents)';
  }
}

/// Determines how start of speech is detected.
final class RealtimeInputConfig_AutomaticActivityDetection_StartSensitivity
    extends ProtoEnum {
  /// The default is START_SENSITIVITY_HIGH.
  static const startSensitivityUnspecified =
      RealtimeInputConfig_AutomaticActivityDetection_StartSensitivity(
        'START_SENSITIVITY_UNSPECIFIED',
      );

  /// Automatic detection will detect the start of speech more often.
  static const startSensitivityHigh =
      RealtimeInputConfig_AutomaticActivityDetection_StartSensitivity(
        'START_SENSITIVITY_HIGH',
      );

  /// Automatic detection will detect the start of speech less often.
  static const startSensitivityLow =
      RealtimeInputConfig_AutomaticActivityDetection_StartSensitivity(
        'START_SENSITIVITY_LOW',
      );

  /// The default value for [RealtimeInputConfig_AutomaticActivityDetection_StartSensitivity].
  static const $default = startSensitivityUnspecified;

  const RealtimeInputConfig_AutomaticActivityDetection_StartSensitivity(
    super.value,
  );

  factory RealtimeInputConfig_AutomaticActivityDetection_StartSensitivity.fromJson(
    Object? json,
  ) => RealtimeInputConfig_AutomaticActivityDetection_StartSensitivity(
    json as String,
  );

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'StartSensitivity.$value';
}

/// Determines how end of speech is detected.
final class RealtimeInputConfig_AutomaticActivityDetection_EndSensitivity
    extends ProtoEnum {
  /// The default is END_SENSITIVITY_HIGH.
  static const endSensitivityUnspecified =
      RealtimeInputConfig_AutomaticActivityDetection_EndSensitivity(
        'END_SENSITIVITY_UNSPECIFIED',
      );

  /// Automatic detection ends speech more often.
  static const endSensitivityHigh =
      RealtimeInputConfig_AutomaticActivityDetection_EndSensitivity(
        'END_SENSITIVITY_HIGH',
      );

  /// Automatic detection ends speech less often.
  static const endSensitivityLow =
      RealtimeInputConfig_AutomaticActivityDetection_EndSensitivity(
        'END_SENSITIVITY_LOW',
      );

  /// The default value for [RealtimeInputConfig_AutomaticActivityDetection_EndSensitivity].
  static const $default = endSensitivityUnspecified;

  const RealtimeInputConfig_AutomaticActivityDetection_EndSensitivity(
    super.value,
  );

  factory RealtimeInputConfig_AutomaticActivityDetection_EndSensitivity.fromJson(
    Object? json,
  ) => RealtimeInputConfig_AutomaticActivityDetection_EndSensitivity(
    json as String,
  );

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'EndSensitivity.$value';
}

/// The different ways of handling user activity.
final class RealtimeInputConfig_ActivityHandling extends ProtoEnum {
  /// If unspecified, the default behavior is `START_OF_ACTIVITY_INTERRUPTS`.
  static const activityHandlingUnspecified =
      RealtimeInputConfig_ActivityHandling('ACTIVITY_HANDLING_UNSPECIFIED');

  /// If true, start of activity will interrupt the model's response (also
  /// called "barge in"). The model's current response will be cut-off in the
  /// moment of the interruption. This is the default behavior.
  static const startOfActivityInterrupts = RealtimeInputConfig_ActivityHandling(
    'START_OF_ACTIVITY_INTERRUPTS',
  );

  /// The model's response will not be interrupted.
  static const noInterruption = RealtimeInputConfig_ActivityHandling(
    'NO_INTERRUPTION',
  );

  /// The default value for [RealtimeInputConfig_ActivityHandling].
  static const $default = activityHandlingUnspecified;

  const RealtimeInputConfig_ActivityHandling(super.value);

  factory RealtimeInputConfig_ActivityHandling.fromJson(Object? json) =>
      RealtimeInputConfig_ActivityHandling(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'ActivityHandling.$value';
}

/// Options about which input is included in the user's turn.
final class RealtimeInputConfig_TurnCoverage extends ProtoEnum {
  /// If unspecified, the default behavior is `TURN_INCLUDES_ONLY_ACTIVITY`.
  static const turnCoverageUnspecified = RealtimeInputConfig_TurnCoverage(
    'TURN_COVERAGE_UNSPECIFIED',
  );

  /// The users turn only includes activity since the last turn, excluding
  /// inactivity (e.g. silence on the audio stream). This is the default
  /// behavior.
  static const turnIncludesOnlyActivity = RealtimeInputConfig_TurnCoverage(
    'TURN_INCLUDES_ONLY_ACTIVITY',
  );

  /// The users turn includes all realtime input since the last turn, including
  /// inactivity (e.g. silence on the audio stream).
  static const turnIncludesAllInput = RealtimeInputConfig_TurnCoverage(
    'TURN_INCLUDES_ALL_INPUT',
  );

  /// The default value for [RealtimeInputConfig_TurnCoverage].
  static const $default = turnCoverageUnspecified;

  const RealtimeInputConfig_TurnCoverage(super.value);

  factory RealtimeInputConfig_TurnCoverage.fromJson(Object? json) =>
      RealtimeInputConfig_TurnCoverage(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'TurnCoverage.$value';
}

/// Session resumption configuration.
///
/// This message is included in the session configuration as
/// `BidiGenerateContentSetup.session_resumption`. If configured, the server
/// will send `SessionResumptionUpdate` messages.
final class SessionResumptionConfig extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.SessionResumptionConfig';

  /// The handle of a previous session. If not present then a new session is
  /// created.
  ///
  /// Session handles come from `SessionResumptionUpdate.token` values in
  /// previous connections.
  final String? handle;

  SessionResumptionConfig({this.handle}) : super(fullyQualifiedName);

  factory SessionResumptionConfig.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return SessionResumptionConfig(
      handle: switch (json['handle']) {
        null => null,
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {if (handle != null) 'handle': handle};

  @override
  String toString() {
    final contents = [if (handle != null) 'handle=$handle'].join(',');
    return 'SessionResumptionConfig($contents)';
  }
}

/// Enables context window compression  a mechanism for managing the model's
/// context window so that it does not exceed a given length.
final class ContextWindowCompressionConfig extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ContextWindowCompressionConfig';

  /// A sliding-window mechanism.
  final ContextWindowCompressionConfig_SlidingWindow? slidingWindow;

  /// The number of tokens (before running a turn) required to trigger a context
  /// window compression.
  ///
  /// This can be used to balance quality against latency as shorter context
  /// windows may result in faster model responses. However, any compression
  /// operation will cause a temporary latency increase, so they should not be
  /// triggered frequently.
  ///
  /// If not set, the default is 80% of the model's context window limit. This
  /// leaves 20% for the next user request/model response.
  final int? triggerTokens;

  ContextWindowCompressionConfig({this.slidingWindow, this.triggerTokens})
    : super(fullyQualifiedName);

  factory ContextWindowCompressionConfig.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ContextWindowCompressionConfig(
      slidingWindow: switch (json['slidingWindow']) {
        null => null,
        Object $1 => ContextWindowCompressionConfig_SlidingWindow.fromJson($1),
      },
      triggerTokens: switch (json['triggerTokens']) {
        null => null,
        Object $1 => decodeInt64($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (slidingWindow != null) 'slidingWindow': slidingWindow!.toJson(),
    if (triggerTokens != null) 'triggerTokens': encodeInt64(triggerTokens),
  };

  @override
  String toString() {
    final contents = [
      if (triggerTokens != null) 'triggerTokens=$triggerTokens',
    ].join(',');
    return 'ContextWindowCompressionConfig($contents)';
  }
}

/// The SlidingWindow method operates by discarding content at the beginning of
/// the context window. The resulting context will always begin at the start of
/// a USER role turn. System instructions and any
/// `BidiGenerateContentSetup.prefix_turns` will always remain at the beginning
/// of the result.
final class ContextWindowCompressionConfig_SlidingWindow extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ContextWindowCompressionConfig.SlidingWindow';

  /// The target number of tokens to keep. The default value is
  /// trigger_tokens/2.
  ///
  /// Discarding parts of the context window causes a temporary latency
  /// increase so this value should be calibrated to avoid frequent compression
  /// operations.
  final int? targetTokens;

  ContextWindowCompressionConfig_SlidingWindow({this.targetTokens})
    : super(fullyQualifiedName);

  factory ContextWindowCompressionConfig_SlidingWindow.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ContextWindowCompressionConfig_SlidingWindow(
      targetTokens: switch (json['targetTokens']) {
        null => null,
        Object $1 => decodeInt64($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (targetTokens != null) 'targetTokens': encodeInt64(targetTokens),
  };

  @override
  String toString() {
    final contents = [
      if (targetTokens != null) 'targetTokens=$targetTokens',
    ].join(',');
    return 'SlidingWindow($contents)';
  }
}

/// The audio transcription configuration.
final class AudioTranscriptionConfig extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.AudioTranscriptionConfig';

  AudioTranscriptionConfig() : super(fullyQualifiedName);

  factory AudioTranscriptionConfig.fromJson(Object? j) =>
      AudioTranscriptionConfig();

  @override
  Object toJson() => {};

  @override
  String toString() => 'AudioTranscriptionConfig()';
}

/// Message to be sent in the first (and only in the first)
/// `BidiGenerateContentClientMessage`. Contains configuration that will apply
/// for the duration of the streaming RPC.
///
/// Clients should wait for a `BidiGenerateContentSetupComplete` message before
/// sending any additional messages.
final class BidiGenerateContentSetup extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BidiGenerateContentSetup';

  /// Required. The model's resource name. This serves as an ID for the Model to
  /// use.
  ///
  /// Format: `models/{model}`
  final String model;

  /// Optional. Generation config.
  ///
  /// The following fields are not supported:
  ///
  ///  - `response_logprobs`
  ///  - `response_mime_type`
  ///  - `logprobs`
  ///  - `response_schema`
  ///  - `response_json_schema`
  ///  - `stop_sequence`
  ///  - `routing_config`
  ///  - `audio_timestamp`
  final GenerationConfig? generationConfig;

  /// Optional. The user provided system instructions for the model.
  ///
  /// Note: Only text should be used in parts and content in each part will be
  /// in a separate paragraph.
  final Content? systemInstruction;

  /// Optional. A list of `Tools` the model may use to generate the next
  /// response.
  ///
  /// A `Tool` is a piece of code that enables the system to interact with
  /// external systems to perform an action, or set of actions, outside of
  /// knowledge and scope of the model.
  final List<Tool> tools;

  /// Optional. Configures the handling of realtime input.
  final RealtimeInputConfig? realtimeInputConfig;

  /// Optional. Configures session resumption mechanism.
  ///
  /// If included, the server will send `SessionResumptionUpdate` messages.
  final SessionResumptionConfig? sessionResumption;

  /// Optional. Configures a context window compression mechanism.
  ///
  /// If included, the server will automatically reduce the size of the context
  /// when it exceeds the configured length.
  final ContextWindowCompressionConfig? contextWindowCompression;

  /// Optional. If set, enables transcription of voice input. The transcription
  /// aligns with the input audio language, if configured.
  final AudioTranscriptionConfig? inputAudioTranscription;

  /// Optional. If set, enables transcription of the model's audio output. The
  /// transcription aligns with the language code specified for the output
  /// audio, if configured.
  final AudioTranscriptionConfig? outputAudioTranscription;

  BidiGenerateContentSetup({
    required this.model,
    this.generationConfig,
    this.systemInstruction,
    this.tools = const [],
    this.realtimeInputConfig,
    this.sessionResumption,
    this.contextWindowCompression,
    this.inputAudioTranscription,
    this.outputAudioTranscription,
  }) : super(fullyQualifiedName);

  factory BidiGenerateContentSetup.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BidiGenerateContentSetup(
      model: switch (json['model']) {
        null => '',
        Object $1 => decodeString($1),
      },
      generationConfig: switch (json['generationConfig']) {
        null => null,
        Object $1 => GenerationConfig.fromJson($1),
      },
      systemInstruction: switch (json['systemInstruction']) {
        null => null,
        Object $1 => Content.fromJson($1),
      },
      tools: switch (json['tools']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Tool.fromJson(i)],
        _ => throw const FormatException('"tools" is not a list'),
      },
      realtimeInputConfig: switch (json['realtimeInputConfig']) {
        null => null,
        Object $1 => RealtimeInputConfig.fromJson($1),
      },
      sessionResumption: switch (json['sessionResumption']) {
        null => null,
        Object $1 => SessionResumptionConfig.fromJson($1),
      },
      contextWindowCompression: switch (json['contextWindowCompression']) {
        null => null,
        Object $1 => ContextWindowCompressionConfig.fromJson($1),
      },
      inputAudioTranscription: switch (json['inputAudioTranscription']) {
        null => null,
        Object $1 => AudioTranscriptionConfig.fromJson($1),
      },
      outputAudioTranscription: switch (json['outputAudioTranscription']) {
        null => null,
        Object $1 => AudioTranscriptionConfig.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    'model': model,
    if (generationConfig != null)
      'generationConfig': generationConfig!.toJson(),
    if (systemInstruction != null)
      'systemInstruction': systemInstruction!.toJson(),
    if (tools.isNotDefault) 'tools': encodeList(tools),
    if (realtimeInputConfig != null)
      'realtimeInputConfig': realtimeInputConfig!.toJson(),
    if (sessionResumption != null)
      'sessionResumption': sessionResumption!.toJson(),
    if (contextWindowCompression != null)
      'contextWindowCompression': contextWindowCompression!.toJson(),
    if (inputAudioTranscription != null)
      'inputAudioTranscription': inputAudioTranscription!.toJson(),
    if (outputAudioTranscription != null)
      'outputAudioTranscription': outputAudioTranscription!.toJson(),
  };

  @override
  String toString() {
    final contents = ['model=$model'].join(',');
    return 'BidiGenerateContentSetup($contents)';
  }
}

/// Incremental update of the current conversation delivered from the client.
/// All of the content here is unconditionally appended to the conversation
/// history and used as part of the prompt to the model to generate content.
///
/// A message here will interrupt any current model generation.
final class BidiGenerateContentClientContent extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BidiGenerateContentClientContent';

  /// Optional. The content appended to the current conversation with the model.
  ///
  /// For single-turn queries, this is a single instance. For multi-turn
  /// queries, this is a repeated field that contains conversation history and
  /// the latest request.
  final List<Content> turns;

  /// Optional. If true, indicates that the server content generation should
  /// start with the currently accumulated prompt. Otherwise, the server awaits
  /// additional messages before starting generation.
  final bool turnComplete;

  BidiGenerateContentClientContent({
    this.turns = const [],
    this.turnComplete = false,
  }) : super(fullyQualifiedName);

  factory BidiGenerateContentClientContent.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BidiGenerateContentClientContent(
      turns: switch (json['turns']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Content.fromJson(i)],
        _ => throw const FormatException('"turns" is not a list'),
      },
      turnComplete: switch (json['turnComplete']) {
        null => false,
        Object $1 => decodeBool($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (turns.isNotDefault) 'turns': encodeList(turns),
    if (turnComplete.isNotDefault) 'turnComplete': turnComplete,
  };

  @override
  String toString() {
    final contents = ['turnComplete=$turnComplete'].join(',');
    return 'BidiGenerateContentClientContent($contents)';
  }
}

/// User input that is sent in real time.
///
/// The different modalities (audio, video and text) are handled as concurrent
/// streams. The ordering across these streams is not guaranteed.
///
/// This is different from
/// `BidiGenerateContentClientContent`
/// in a few ways:
///
/// * Can be sent continuously without interruption to model generation.
/// * If there is a need to mix data interleaved across the
///   `BidiGenerateContentClientContent`
///   and the
///   `BidiGenerateContentRealtimeInput`,
///   the server attempts to optimize for best response, but there are no
///   guarantees.
/// * End of turn is not explicitly specified, but is rather derived from user
///   activity (for example, end of speech).
/// * Even before the end of turn, the data is processed incrementally
///   to optimize for a fast start of the response from the model.
final class BidiGenerateContentRealtimeInput extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BidiGenerateContentRealtimeInput';

  /// Optional. Inlined bytes data for media input. Multiple `media_chunks` are
  /// not supported, all but the first will be ignored.
  ///
  /// DEPRECATED: Use one of `audio`, `video`, or `text` instead.
  final List<Blob> mediaChunks;

  /// Optional. These form the realtime audio input stream.
  final Blob? audio;

  /// Optional. Indicates that the audio stream has ended, e.g. because the
  /// microphone was turned off.
  ///
  /// This should only be sent when automatic activity detection is enabled
  /// (which is the default).
  ///
  /// The client can reopen the stream by sending an audio message.
  final bool? audioStreamEnd;

  /// Optional. These form the realtime video input stream.
  final Blob? video;

  /// Optional. These form the realtime text input stream.
  final String? text;

  /// Optional. Marks the start of user activity. This can only be sent if
  /// automatic (i.e. server-side) activity detection is disabled.
  final BidiGenerateContentRealtimeInput_ActivityStart? activityStart;

  /// Optional. Marks the end of user activity. This can only be sent if
  /// automatic (i.e. server-side) activity detection is disabled.
  final BidiGenerateContentRealtimeInput_ActivityEnd? activityEnd;

  BidiGenerateContentRealtimeInput({
    this.mediaChunks = const [],
    this.audio,
    this.audioStreamEnd,
    this.video,
    this.text,
    this.activityStart,
    this.activityEnd,
  }) : super(fullyQualifiedName);

  factory BidiGenerateContentRealtimeInput.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BidiGenerateContentRealtimeInput(
      mediaChunks: switch (json['mediaChunks']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Blob.fromJson(i)],
        _ => throw const FormatException('"mediaChunks" is not a list'),
      },
      audio: switch (json['audio']) {
        null => null,
        Object $1 => Blob.fromJson($1),
      },
      audioStreamEnd: switch (json['audioStreamEnd']) {
        null => null,
        Object $1 => decodeBool($1),
      },
      video: switch (json['video']) {
        null => null,
        Object $1 => Blob.fromJson($1),
      },
      text: switch (json['text']) {
        null => null,
        Object $1 => decodeString($1),
      },
      activityStart: switch (json['activityStart']) {
        null => null,
        Object $1 => BidiGenerateContentRealtimeInput_ActivityStart.fromJson(
          $1,
        ),
      },
      activityEnd: switch (json['activityEnd']) {
        null => null,
        Object $1 => BidiGenerateContentRealtimeInput_ActivityEnd.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (mediaChunks.isNotDefault) 'mediaChunks': encodeList(mediaChunks),
    if (audio != null) 'audio': audio!.toJson(),
    if (audioStreamEnd != null) 'audioStreamEnd': audioStreamEnd,
    if (video != null) 'video': video!.toJson(),
    if (text != null) 'text': text,
    if (activityStart != null) 'activityStart': activityStart!.toJson(),
    if (activityEnd != null) 'activityEnd': activityEnd!.toJson(),
  };

  @override
  String toString() {
    final contents = [
      if (audioStreamEnd != null) 'audioStreamEnd=$audioStreamEnd',
      if (text != null) 'text=$text',
    ].join(',');
    return 'BidiGenerateContentRealtimeInput($contents)';
  }
}

/// Marks the start of user activity.
final class BidiGenerateContentRealtimeInput_ActivityStart
    extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BidiGenerateContentRealtimeInput.ActivityStart';

  BidiGenerateContentRealtimeInput_ActivityStart() : super(fullyQualifiedName);

  factory BidiGenerateContentRealtimeInput_ActivityStart.fromJson(Object? j) =>
      BidiGenerateContentRealtimeInput_ActivityStart();

  @override
  Object toJson() => {};

  @override
  String toString() => 'ActivityStart()';
}

/// Marks the end of user activity.
final class BidiGenerateContentRealtimeInput_ActivityEnd extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BidiGenerateContentRealtimeInput.ActivityEnd';

  BidiGenerateContentRealtimeInput_ActivityEnd() : super(fullyQualifiedName);

  factory BidiGenerateContentRealtimeInput_ActivityEnd.fromJson(Object? j) =>
      BidiGenerateContentRealtimeInput_ActivityEnd();

  @override
  Object toJson() => {};

  @override
  String toString() => 'ActivityEnd()';
}

/// Client generated response to a `ToolCall` received from the server.
/// Individual `FunctionResponse` objects are matched to the respective
/// `FunctionCall` objects by the `id` field.
///
/// Note that in the unary and server-streaming GenerateContent APIs function
/// calling happens by exchanging the `Content` parts, while in the bidi
/// GenerateContent APIs function calling happens over these dedicated set of
/// messages.
final class BidiGenerateContentToolResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BidiGenerateContentToolResponse';

  /// Optional. The response to the function calls.
  final List<FunctionResponse> functionResponses;

  BidiGenerateContentToolResponse({this.functionResponses = const []})
    : super(fullyQualifiedName);

  factory BidiGenerateContentToolResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BidiGenerateContentToolResponse(
      functionResponses: switch (json['functionResponses']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) FunctionResponse.fromJson(i)],
        _ => throw const FormatException('"functionResponses" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (functionResponses.isNotDefault)
      'functionResponses': encodeList(functionResponses),
  };

  @override
  String toString() => 'BidiGenerateContentToolResponse()';
}

/// Messages sent by the client in the BidiGenerateContent call.
final class BidiGenerateContentClientMessage extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BidiGenerateContentClientMessage';

  /// Optional. Session configuration sent only in the first client message.
  final BidiGenerateContentSetup? setup;

  /// Optional. Incremental update of the current conversation delivered from
  /// the client.
  final BidiGenerateContentClientContent? clientContent;

  /// Optional. User input that is sent in real time.
  final BidiGenerateContentRealtimeInput? realtimeInput;

  /// Optional. Response to a `ToolCallMessage` received from the server.
  final BidiGenerateContentToolResponse? toolResponse;

  BidiGenerateContentClientMessage({
    this.setup,
    this.clientContent,
    this.realtimeInput,
    this.toolResponse,
  }) : super(fullyQualifiedName);

  factory BidiGenerateContentClientMessage.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BidiGenerateContentClientMessage(
      setup: switch (json['setup']) {
        null => null,
        Object $1 => BidiGenerateContentSetup.fromJson($1),
      },
      clientContent: switch (json['clientContent']) {
        null => null,
        Object $1 => BidiGenerateContentClientContent.fromJson($1),
      },
      realtimeInput: switch (json['realtimeInput']) {
        null => null,
        Object $1 => BidiGenerateContentRealtimeInput.fromJson($1),
      },
      toolResponse: switch (json['toolResponse']) {
        null => null,
        Object $1 => BidiGenerateContentToolResponse.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (setup != null) 'setup': setup!.toJson(),
    if (clientContent != null) 'clientContent': clientContent!.toJson(),
    if (realtimeInput != null) 'realtimeInput': realtimeInput!.toJson(),
    if (toolResponse != null) 'toolResponse': toolResponse!.toJson(),
  };

  @override
  String toString() => 'BidiGenerateContentClientMessage()';
}

/// Sent in response to a `BidiGenerateContentSetup` message from the client.
final class BidiGenerateContentSetupComplete extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BidiGenerateContentSetupComplete';

  BidiGenerateContentSetupComplete() : super(fullyQualifiedName);

  factory BidiGenerateContentSetupComplete.fromJson(Object? j) =>
      BidiGenerateContentSetupComplete();

  @override
  Object toJson() => {};

  @override
  String toString() => 'BidiGenerateContentSetupComplete()';
}

/// Incremental server update generated by the model in response to client
/// messages.
///
/// Content is generated as quickly as possible, and not in real time. Clients
/// may choose to buffer and play it out in real time.
final class BidiGenerateContentServerContent extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BidiGenerateContentServerContent';

  /// Output only. The content that the model has generated as part of the
  /// current conversation with the user.
  final Content? modelTurn;

  /// Output only. If true, indicates that the model is done generating.
  ///
  /// When model is interrupted while generating there will be no
  /// 'generation_complete' message in interrupted turn, it will go through
  /// 'interrupted > turn_complete'.
  ///
  /// When model assumes realtime playback there will be delay between
  /// generation_complete and turn_complete that is caused by model waiting for
  /// playback to finish.
  final bool generationComplete;

  /// Output only. If true, indicates that the model has completed its turn.
  /// Generation will only start in response to additional client messages.
  final bool turnComplete;

  /// Output only. If true, indicates that a client message has interrupted
  /// current model generation. If the client is playing out the content in real
  /// time, this is a good signal to stop and empty the current playback queue.
  final bool interrupted;

  /// Output only. Grounding metadata for the generated content.
  final GroundingMetadata? groundingMetadata;

  /// Output only. Input audio transcription. The transcription is sent
  /// independently of the other server messages and there is no guaranteed
  /// ordering.
  final BidiGenerateContentTranscription? inputTranscription;

  /// Output only. Output audio transcription. These transcriptions are part of
  /// the Generation output of the server. The last output transcription of this
  /// turn is sent before either `generation_complete` or `interrupted`, which in
  /// turn are followed by `turn_complete`. There is no guaranteed exact ordering
  /// between transcriptions and other `model_turn` output but the server tries
  /// to send the transcripts close to the corresponding audio output.
  final BidiGenerateContentTranscription? outputTranscription;

  final UrlContextMetadata? urlContextMetadata;

  /// Output only. If true, indicates that the model is not generating content
  /// because it is waiting for more input from the user, e.g. because it expects
  /// the user to continue talking.
  final bool waitingForInput;

  BidiGenerateContentServerContent({
    this.modelTurn,
    this.generationComplete = false,
    this.turnComplete = false,
    this.interrupted = false,
    this.groundingMetadata,
    this.inputTranscription,
    this.outputTranscription,
    this.urlContextMetadata,
    this.waitingForInput = false,
  }) : super(fullyQualifiedName);

  factory BidiGenerateContentServerContent.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BidiGenerateContentServerContent(
      modelTurn: switch (json['modelTurn']) {
        null => null,
        Object $1 => Content.fromJson($1),
      },
      generationComplete: switch (json['generationComplete']) {
        null => false,
        Object $1 => decodeBool($1),
      },
      turnComplete: switch (json['turnComplete']) {
        null => false,
        Object $1 => decodeBool($1),
      },
      interrupted: switch (json['interrupted']) {
        null => false,
        Object $1 => decodeBool($1),
      },
      groundingMetadata: switch (json['groundingMetadata']) {
        null => null,
        Object $1 => GroundingMetadata.fromJson($1),
      },
      inputTranscription: switch (json['inputTranscription']) {
        null => null,
        Object $1 => BidiGenerateContentTranscription.fromJson($1),
      },
      outputTranscription: switch (json['outputTranscription']) {
        null => null,
        Object $1 => BidiGenerateContentTranscription.fromJson($1),
      },
      urlContextMetadata: switch (json['urlContextMetadata']) {
        null => null,
        Object $1 => UrlContextMetadata.fromJson($1),
      },
      waitingForInput: switch (json['waitingForInput']) {
        null => false,
        Object $1 => decodeBool($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (modelTurn != null) 'modelTurn': modelTurn!.toJson(),
    if (generationComplete.isNotDefault)
      'generationComplete': generationComplete,
    if (turnComplete.isNotDefault) 'turnComplete': turnComplete,
    if (interrupted.isNotDefault) 'interrupted': interrupted,
    if (groundingMetadata != null)
      'groundingMetadata': groundingMetadata!.toJson(),
    if (inputTranscription != null)
      'inputTranscription': inputTranscription!.toJson(),
    if (outputTranscription != null)
      'outputTranscription': outputTranscription!.toJson(),
    if (urlContextMetadata != null)
      'urlContextMetadata': urlContextMetadata!.toJson(),
    if (waitingForInput.isNotDefault) 'waitingForInput': waitingForInput,
  };

  @override
  String toString() {
    final contents = [
      'generationComplete=$generationComplete',
      'turnComplete=$turnComplete',
      'interrupted=$interrupted',
      'waitingForInput=$waitingForInput',
    ].join(',');
    return 'BidiGenerateContentServerContent($contents)';
  }
}

/// Request for the client to execute the `function_calls` and return the
/// responses with the matching `id`s.
final class BidiGenerateContentToolCall extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BidiGenerateContentToolCall';

  /// Output only. The function call to be executed.
  final List<FunctionCall> functionCalls;

  BidiGenerateContentToolCall({this.functionCalls = const []})
    : super(fullyQualifiedName);

  factory BidiGenerateContentToolCall.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BidiGenerateContentToolCall(
      functionCalls: switch (json['functionCalls']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) FunctionCall.fromJson(i)],
        _ => throw const FormatException('"functionCalls" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (functionCalls.isNotDefault) 'functionCalls': encodeList(functionCalls),
  };

  @override
  String toString() => 'BidiGenerateContentToolCall()';
}

/// Notification for the client that a previously issued `ToolCallMessage`
/// with the specified `id`s should not have been executed and should be
/// cancelled. If there were side-effects to those tool calls, clients may
/// attempt to undo the tool calls. This message occurs only in cases where the
/// clients interrupt server turns.
final class BidiGenerateContentToolCallCancellation extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BidiGenerateContentToolCallCancellation';

  /// Output only. The ids of the tool calls to be cancelled.
  final List<String> ids;

  BidiGenerateContentToolCallCancellation({this.ids = const []})
    : super(fullyQualifiedName);

  factory BidiGenerateContentToolCallCancellation.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BidiGenerateContentToolCallCancellation(
      ids: switch (json['ids']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) decodeString(i)],
        _ => throw const FormatException('"ids" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {if (ids.isNotDefault) 'ids': ids};

  @override
  String toString() => 'BidiGenerateContentToolCallCancellation()';
}

/// A notice that the server will soon disconnect.
final class GoAway extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GoAway';

  /// The remaining time before the connection will be terminated as ABORTED.
  ///
  /// This duration will never be less than a model-specific minimum, which will
  /// be specified together with the rate limits for the model.
  final Duration? timeLeft;

  GoAway({this.timeLeft}) : super(fullyQualifiedName);

  factory GoAway.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GoAway(
      timeLeft: switch (json['timeLeft']) {
        null => null,
        Object $1 => Duration.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {if (timeLeft != null) 'timeLeft': timeLeft!.toJson()};

  @override
  String toString() => 'GoAway()';
}

/// Update of the session resumption state.
///
/// Only sent if `BidiGenerateContentSetup.session_resumption` was set.
final class SessionResumptionUpdate extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.SessionResumptionUpdate';

  /// New handle that represents a state that can be resumed. Empty if
  /// `resumable`=false.
  final String newHandle;

  /// True if the current session can be resumed at this point.
  ///
  /// Resumption is not possible at some points in the session. For example, when
  /// the model is executing function calls or generating. Resuming the session
  /// (using a previous session token) in such a state will result in some data
  /// loss. In these cases, `new_handle` will be empty and `resumable` will be
  /// false.
  final bool resumable;

  SessionResumptionUpdate({this.newHandle = '', this.resumable = false})
    : super(fullyQualifiedName);

  factory SessionResumptionUpdate.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return SessionResumptionUpdate(
      newHandle: switch (json['newHandle']) {
        null => '',
        Object $1 => decodeString($1),
      },
      resumable: switch (json['resumable']) {
        null => false,
        Object $1 => decodeBool($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (newHandle.isNotDefault) 'newHandle': newHandle,
    if (resumable.isNotDefault) 'resumable': resumable,
  };

  @override
  String toString() {
    final contents = ['newHandle=$newHandle', 'resumable=$resumable'].join(',');
    return 'SessionResumptionUpdate($contents)';
  }
}

/// Transcription of audio (input or output).
final class BidiGenerateContentTranscription extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BidiGenerateContentTranscription';

  /// Transcription text.
  final String text;

  BidiGenerateContentTranscription({this.text = ''})
    : super(fullyQualifiedName);

  factory BidiGenerateContentTranscription.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BidiGenerateContentTranscription(
      text: switch (json['text']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {if (text.isNotDefault) 'text': text};

  @override
  String toString() {
    final contents = ['text=$text'].join(',');
    return 'BidiGenerateContentTranscription($contents)';
  }
}

/// Response message for the BidiGenerateContent call.
final class BidiGenerateContentServerMessage extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BidiGenerateContentServerMessage';

  /// Output only. Sent in response to a `BidiGenerateContentSetup` message
  /// from the client when setup is complete.
  final BidiGenerateContentSetupComplete? setupComplete;

  /// Output only. Content generated by the model in response to client
  /// messages.
  final BidiGenerateContentServerContent? serverContent;

  /// Output only. Request for the client to execute the `function_calls` and
  /// return the responses with the matching `id`s.
  final BidiGenerateContentToolCall? toolCall;

  /// Output only. Notification for the client that a previously issued
  /// `ToolCallMessage` with the specified `id`s should be cancelled.
  final BidiGenerateContentToolCallCancellation? toolCallCancellation;

  /// Output only. A notice that the server will soon disconnect.
  final GoAway? goAway;

  /// Output only. Update of the session resumption state.
  final SessionResumptionUpdate? sessionResumptionUpdate;

  /// Output only. Usage metadata about the response(s).
  final UsageMetadata? usageMetadata;

  BidiGenerateContentServerMessage({
    this.setupComplete,
    this.serverContent,
    this.toolCall,
    this.toolCallCancellation,
    this.goAway,
    this.sessionResumptionUpdate,
    this.usageMetadata,
  }) : super(fullyQualifiedName);

  factory BidiGenerateContentServerMessage.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BidiGenerateContentServerMessage(
      setupComplete: switch (json['setupComplete']) {
        null => null,
        Object $1 => BidiGenerateContentSetupComplete.fromJson($1),
      },
      serverContent: switch (json['serverContent']) {
        null => null,
        Object $1 => BidiGenerateContentServerContent.fromJson($1),
      },
      toolCall: switch (json['toolCall']) {
        null => null,
        Object $1 => BidiGenerateContentToolCall.fromJson($1),
      },
      toolCallCancellation: switch (json['toolCallCancellation']) {
        null => null,
        Object $1 => BidiGenerateContentToolCallCancellation.fromJson($1),
      },
      goAway: switch (json['goAway']) {
        null => null,
        Object $1 => GoAway.fromJson($1),
      },
      sessionResumptionUpdate: switch (json['sessionResumptionUpdate']) {
        null => null,
        Object $1 => SessionResumptionUpdate.fromJson($1),
      },
      usageMetadata: switch (json['usageMetadata']) {
        null => null,
        Object $1 => UsageMetadata.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (setupComplete != null) 'setupComplete': setupComplete!.toJson(),
    if (serverContent != null) 'serverContent': serverContent!.toJson(),
    if (toolCall != null) 'toolCall': toolCall!.toJson(),
    if (toolCallCancellation != null)
      'toolCallCancellation': toolCallCancellation!.toJson(),
    if (goAway != null) 'goAway': goAway!.toJson(),
    if (sessionResumptionUpdate != null)
      'sessionResumptionUpdate': sessionResumptionUpdate!.toJson(),
    if (usageMetadata != null) 'usageMetadata': usageMetadata!.toJson(),
  };

  @override
  String toString() => 'BidiGenerateContentServerMessage()';
}

/// Usage metadata about response(s).
final class UsageMetadata extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.UsageMetadata';

  /// Output only. Number of tokens in the prompt. When `cached_content` is set,
  /// this is still the total effective prompt size meaning this includes the
  /// number of tokens in the cached content.
  final int promptTokenCount;

  /// Number of tokens in the cached part of the prompt (the cached content)
  final int cachedContentTokenCount;

  /// Output only. Total number of tokens across all the generated response
  /// candidates.
  final int responseTokenCount;

  /// Output only. Number of tokens present in tool-use prompt(s).
  final int toolUsePromptTokenCount;

  /// Output only. Number of tokens of thoughts for thinking models.
  final int thoughtsTokenCount;

  /// Output only. Total token count for the generation request (prompt +
  /// response candidates).
  final int totalTokenCount;

  /// Output only. List of modalities that were processed in the request input.
  final List<ModalityTokenCount> promptTokensDetails;

  /// Output only. List of modalities of the cached content in the request input.
  final List<ModalityTokenCount> cacheTokensDetails;

  /// Output only. List of modalities that were returned in the response.
  final List<ModalityTokenCount> responseTokensDetails;

  /// Output only. List of modalities that were processed for tool-use request
  /// inputs.
  final List<ModalityTokenCount> toolUsePromptTokensDetails;

  UsageMetadata({
    this.promptTokenCount = 0,
    this.cachedContentTokenCount = 0,
    this.responseTokenCount = 0,
    this.toolUsePromptTokenCount = 0,
    this.thoughtsTokenCount = 0,
    this.totalTokenCount = 0,
    this.promptTokensDetails = const [],
    this.cacheTokensDetails = const [],
    this.responseTokensDetails = const [],
    this.toolUsePromptTokensDetails = const [],
  }) : super(fullyQualifiedName);

  factory UsageMetadata.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return UsageMetadata(
      promptTokenCount: switch (json['promptTokenCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      cachedContentTokenCount: switch (json['cachedContentTokenCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      responseTokenCount: switch (json['responseTokenCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      toolUsePromptTokenCount: switch (json['toolUsePromptTokenCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      thoughtsTokenCount: switch (json['thoughtsTokenCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      totalTokenCount: switch (json['totalTokenCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      promptTokensDetails: switch (json['promptTokensDetails']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) ModalityTokenCount.fromJson(i),
        ],
        _ => throw const FormatException('"promptTokensDetails" is not a list'),
      },
      cacheTokensDetails: switch (json['cacheTokensDetails']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) ModalityTokenCount.fromJson(i),
        ],
        _ => throw const FormatException('"cacheTokensDetails" is not a list'),
      },
      responseTokensDetails: switch (json['responseTokensDetails']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) ModalityTokenCount.fromJson(i),
        ],
        _ => throw const FormatException(
          '"responseTokensDetails" is not a list',
        ),
      },
      toolUsePromptTokensDetails: switch (json['toolUsePromptTokensDetails']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) ModalityTokenCount.fromJson(i),
        ],
        _ => throw const FormatException(
          '"toolUsePromptTokensDetails" is not a list',
        ),
      },
    );
  }

  @override
  Object toJson() => {
    if (promptTokenCount.isNotDefault) 'promptTokenCount': promptTokenCount,
    if (cachedContentTokenCount.isNotDefault)
      'cachedContentTokenCount': cachedContentTokenCount,
    if (responseTokenCount.isNotDefault)
      'responseTokenCount': responseTokenCount,
    if (toolUsePromptTokenCount.isNotDefault)
      'toolUsePromptTokenCount': toolUsePromptTokenCount,
    if (thoughtsTokenCount.isNotDefault)
      'thoughtsTokenCount': thoughtsTokenCount,
    if (totalTokenCount.isNotDefault) 'totalTokenCount': totalTokenCount,
    if (promptTokensDetails.isNotDefault)
      'promptTokensDetails': encodeList(promptTokensDetails),
    if (cacheTokensDetails.isNotDefault)
      'cacheTokensDetails': encodeList(cacheTokensDetails),
    if (responseTokensDetails.isNotDefault)
      'responseTokensDetails': encodeList(responseTokensDetails),
    if (toolUsePromptTokensDetails.isNotDefault)
      'toolUsePromptTokensDetails': encodeList(toolUsePromptTokensDetails),
  };

  @override
  String toString() {
    final contents = [
      'promptTokenCount=$promptTokenCount',
      'cachedContentTokenCount=$cachedContentTokenCount',
      'responseTokenCount=$responseTokenCount',
      'toolUsePromptTokenCount=$toolUsePromptTokenCount',
      'thoughtsTokenCount=$thoughtsTokenCount',
      'totalTokenCount=$totalTokenCount',
    ].join(',');
    return 'UsageMetadata($contents)';
  }
}

/// Information about a Generative Language Model.
final class Model extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Model';

  /// Required. The resource name of the `Model`. Refer to [Model
  /// variants](https://ai.google.dev/gemini-api/docs/models/gemini#model-variations)
  /// for all allowed values.
  ///
  /// Format: `models/{model}` with a `{model}` naming convention of:
  ///
  /// * "{base_model_id}-{version}"
  ///
  /// Examples:
  ///
  /// * `models/gemini-1.5-flash-001`
  final String name;

  /// Required. The name of the base model, pass this to the generation request.
  ///
  /// Examples:
  ///
  /// * `gemini-1.5-flash`
  final String baseModelId;

  /// Required. The version number of the model.
  ///
  /// This represents the major version (`1.0` or `1.5`)
  final String version;

  /// The human-readable name of the model. E.g. "Gemini 1.5 Flash".
  ///
  /// The name can be up to 128 characters long and can consist of any UTF-8
  /// characters.
  final String displayName;

  /// A short description of the model.
  final String description;

  /// Maximum number of input tokens allowed for this model.
  final int inputTokenLimit;

  /// Maximum number of output tokens available for this model.
  final int outputTokenLimit;

  /// The model's supported generation methods.
  ///
  /// The corresponding API method names are defined as Pascal case
  /// strings, such as `generateMessage` and `generateContent`.
  final List<String> supportedGenerationMethods;

  /// Controls the randomness of the output.
  ///
  /// Values can range over `[0.0,max_temperature]`, inclusive. A higher value
  /// will produce responses that are more varied, while a value closer to `0.0`
  /// will typically result in less surprising responses from the model.
  /// This value specifies default to be used by the backend while making the
  /// call to the model.
  final double? temperature;

  /// The maximum temperature this model can use.
  final double? maxTemperature;

  /// For [Nucleus
  /// sampling](https://ai.google.dev/gemini-api/docs/prompting-strategies#top-p).
  ///
  /// Nucleus sampling considers the smallest set of tokens whose probability
  /// sum is at least `top_p`.
  /// This value specifies default to be used by the backend while making the
  /// call to the model.
  final double? topP;

  /// For Top-k sampling.
  ///
  /// Top-k sampling considers the set of `top_k` most probable tokens.
  /// This value specifies default to be used by the backend while making the
  /// call to the model.
  /// If empty, indicates the model doesn't use top-k sampling, and `top_k` isn't
  /// allowed as a generation parameter.
  final int? topK;

  /// Whether the model supports thinking.
  final bool thinking;

  Model({
    required this.name,
    required this.baseModelId,
    required this.version,
    this.displayName = '',
    this.description = '',
    this.inputTokenLimit = 0,
    this.outputTokenLimit = 0,
    this.supportedGenerationMethods = const [],
    this.temperature,
    this.maxTemperature,
    this.topP,
    this.topK,
    this.thinking = false,
  }) : super(fullyQualifiedName);

  factory Model.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Model(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
      baseModelId: switch (json['baseModelId']) {
        null => '',
        Object $1 => decodeString($1),
      },
      version: switch (json['version']) {
        null => '',
        Object $1 => decodeString($1),
      },
      displayName: switch (json['displayName']) {
        null => '',
        Object $1 => decodeString($1),
      },
      description: switch (json['description']) {
        null => '',
        Object $1 => decodeString($1),
      },
      inputTokenLimit: switch (json['inputTokenLimit']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      outputTokenLimit: switch (json['outputTokenLimit']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      supportedGenerationMethods: switch (json['supportedGenerationMethods']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) decodeString(i)],
        _ => throw const FormatException(
          '"supportedGenerationMethods" is not a list',
        ),
      },
      temperature: switch (json['temperature']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      maxTemperature: switch (json['maxTemperature']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      topP: switch (json['topP']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      topK: switch (json['topK']) {
        null => null,
        Object $1 => decodeInt($1),
      },
      thinking: switch (json['thinking']) {
        null => false,
        Object $1 => decodeBool($1),
      },
    );
  }

  @override
  Object toJson() => {
    'name': name,
    'baseModelId': baseModelId,
    'version': version,
    if (displayName.isNotDefault) 'displayName': displayName,
    if (description.isNotDefault) 'description': description,
    if (inputTokenLimit.isNotDefault) 'inputTokenLimit': inputTokenLimit,
    if (outputTokenLimit.isNotDefault) 'outputTokenLimit': outputTokenLimit,
    if (supportedGenerationMethods.isNotDefault)
      'supportedGenerationMethods': supportedGenerationMethods,
    if (temperature != null) 'temperature': encodeDouble(temperature),
    if (maxTemperature != null) 'maxTemperature': encodeDouble(maxTemperature),
    if (topP != null) 'topP': encodeDouble(topP),
    if (topK != null) 'topK': topK,
    if (thinking.isNotDefault) 'thinking': thinking,
  };

  @override
  String toString() {
    final contents = [
      'name=$name',
      'baseModelId=$baseModelId',
      'version=$version',
      'displayName=$displayName',
      'description=$description',
      'inputTokenLimit=$inputTokenLimit',
      'outputTokenLimit=$outputTokenLimit',
      if (temperature != null) 'temperature=$temperature',
      if (maxTemperature != null) 'maxTemperature=$maxTemperature',
      if (topP != null) 'topP=$topP',
      if (topK != null) 'topK=$topK',
      'thinking=$thinking',
    ].join(',');
    return 'Model($contents)';
  }
}

/// Request for getting information about a specific Model.
final class GetModelRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GetModelRequest';

  /// Required. The resource name of the model.
  ///
  /// This name should match a model name returned by the `ListModels` method.
  ///
  /// Format: `models/{model}`
  final String name;

  GetModelRequest({required this.name}) : super(fullyQualifiedName);

  factory GetModelRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GetModelRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {'name': name};

  @override
  String toString() {
    final contents = ['name=$name'].join(',');
    return 'GetModelRequest($contents)';
  }
}

/// Request for listing all Models.
final class ListModelsRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ListModelsRequest';

  /// The maximum number of `Models` to return (per page).
  ///
  /// If unspecified, 50 models will be returned per page.
  /// This method returns at most 1000 models per page, even if you pass a larger
  /// page_size.
  final int pageSize;

  /// A page token, received from a previous `ListModels` call.
  ///
  /// Provide the `page_token` returned by one request as an argument to the next
  /// request to retrieve the next page.
  ///
  /// When paginating, all other parameters provided to `ListModels` must match
  /// the call that provided the page token.
  final String pageToken;

  ListModelsRequest({this.pageSize = 0, this.pageToken = ''})
    : super(fullyQualifiedName);

  factory ListModelsRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ListModelsRequest(
      pageSize: switch (json['pageSize']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      pageToken: switch (json['pageToken']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (pageSize.isNotDefault) 'pageSize': pageSize,
    if (pageToken.isNotDefault) 'pageToken': pageToken,
  };

  @override
  String toString() {
    final contents = ['pageSize=$pageSize', 'pageToken=$pageToken'].join(',');
    return 'ListModelsRequest($contents)';
  }
}

/// Response from `ListModel` containing a paginated list of Models.
final class ListModelsResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ListModelsResponse';

  /// The returned Models.
  final List<Model> models;

  /// A token, which can be sent as `page_token` to retrieve the next page.
  ///
  /// If this field is omitted, there are no more pages.
  final String nextPageToken;

  ListModelsResponse({this.models = const [], this.nextPageToken = ''})
    : super(fullyQualifiedName);

  factory ListModelsResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ListModelsResponse(
      models: switch (json['models']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Model.fromJson(i)],
        _ => throw const FormatException('"models" is not a list'),
      },
      nextPageToken: switch (json['nextPageToken']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (models.isNotDefault) 'models': encodeList(models),
    if (nextPageToken.isNotDefault) 'nextPageToken': nextPageToken,
  };

  @override
  String toString() {
    final contents = ['nextPageToken=$nextPageToken'].join(',');
    return 'ListModelsResponse($contents)';
  }
}

/// Request for getting information about a specific Model.
final class GetTunedModelRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GetTunedModelRequest';

  /// Required. The resource name of the model.
  ///
  /// Format: `tunedModels/my-model-id`
  final String name;

  GetTunedModelRequest({required this.name}) : super(fullyQualifiedName);

  factory GetTunedModelRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GetTunedModelRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {'name': name};

  @override
  String toString() {
    final contents = ['name=$name'].join(',');
    return 'GetTunedModelRequest($contents)';
  }
}

/// Request for listing TunedModels.
final class ListTunedModelsRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ListTunedModelsRequest';

  /// Optional. The maximum number of `TunedModels` to return (per page).
  /// The service may return fewer tuned models.
  ///
  /// If unspecified, at most 10 tuned models will be returned.
  /// This method returns at most 1000 models per page, even if you pass a larger
  /// page_size.
  final int pageSize;

  /// Optional. A page token, received from a previous `ListTunedModels` call.
  ///
  /// Provide the `page_token` returned by one request as an argument to the next
  /// request to retrieve the next page.
  ///
  /// When paginating, all other parameters provided to `ListTunedModels`
  /// must match the call that provided the page token.
  final String pageToken;

  /// Optional. A filter is a full text search over the tuned model's description
  /// and display name. By default, results will not include tuned models shared
  /// with everyone.
  ///
  /// Additional operators:
  ///   - owner:me
  ///   - writers:me
  ///   - readers:me
  ///   - readers:everyone
  ///
  /// Examples:
  ///   "owner:me" returns all tuned models to which caller has owner role
  ///   "readers:me" returns all tuned models to which caller has reader role
  ///   "readers:everyone" returns all tuned models that are shared with everyone
  final String filter;

  ListTunedModelsRequest({
    this.pageSize = 0,
    this.pageToken = '',
    this.filter = '',
  }) : super(fullyQualifiedName);

  factory ListTunedModelsRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ListTunedModelsRequest(
      pageSize: switch (json['pageSize']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      pageToken: switch (json['pageToken']) {
        null => '',
        Object $1 => decodeString($1),
      },
      filter: switch (json['filter']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (pageSize.isNotDefault) 'pageSize': pageSize,
    if (pageToken.isNotDefault) 'pageToken': pageToken,
    if (filter.isNotDefault) 'filter': filter,
  };

  @override
  String toString() {
    final contents = [
      'pageSize=$pageSize',
      'pageToken=$pageToken',
      'filter=$filter',
    ].join(',');
    return 'ListTunedModelsRequest($contents)';
  }
}

/// Response from `ListTunedModels` containing a paginated list of Models.
final class ListTunedModelsResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ListTunedModelsResponse';

  /// The returned Models.
  final List<TunedModel> tunedModels;

  /// A token, which can be sent as `page_token` to retrieve the next page.
  ///
  /// If this field is omitted, there are no more pages.
  final String nextPageToken;

  ListTunedModelsResponse({
    this.tunedModels = const [],
    this.nextPageToken = '',
  }) : super(fullyQualifiedName);

  factory ListTunedModelsResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ListTunedModelsResponse(
      tunedModels: switch (json['tunedModels']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) TunedModel.fromJson(i)],
        _ => throw const FormatException('"tunedModels" is not a list'),
      },
      nextPageToken: switch (json['nextPageToken']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (tunedModels.isNotDefault) 'tunedModels': encodeList(tunedModels),
    if (nextPageToken.isNotDefault) 'nextPageToken': nextPageToken,
  };

  @override
  String toString() {
    final contents = ['nextPageToken=$nextPageToken'].join(',');
    return 'ListTunedModelsResponse($contents)';
  }
}

/// Request to create a TunedModel.
final class CreateTunedModelRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CreateTunedModelRequest';

  /// Optional. The unique id for the tuned model if specified.
  /// This value should be up to 40 characters, the first character must be a
  /// letter, the last could be a letter or a number. The id must match the
  /// regular expression: `[a-z]([a-z0-9-]{0,38}[a-z0-9])?`.
  final String? tunedModelId;

  /// Required. The tuned model to create.
  final TunedModel? tunedModel;

  CreateTunedModelRequest({this.tunedModelId, required this.tunedModel})
    : super(fullyQualifiedName);

  factory CreateTunedModelRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CreateTunedModelRequest(
      tunedModelId: switch (json['tunedModelId']) {
        null => null,
        Object $1 => decodeString($1),
      },
      tunedModel: switch (json['tunedModel']) {
        null => null,
        Object $1 => TunedModel.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (tunedModelId != null) 'tunedModelId': tunedModelId,
    if (tunedModel != null) 'tunedModel': tunedModel!.toJson(),
  };

  @override
  String toString() {
    final contents = [
      if (tunedModelId != null) 'tunedModelId=$tunedModelId',
    ].join(',');
    return 'CreateTunedModelRequest($contents)';
  }
}

/// Metadata about the state and progress of creating a tuned model returned from
/// the long-running operation
final class CreateTunedModelMetadata extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CreateTunedModelMetadata';

  /// Name of the tuned model associated with the tuning operation.
  final String tunedModel;

  /// The total number of tuning steps.
  final int totalSteps;

  /// The number of steps completed.
  final int completedSteps;

  /// The completed percentage for the tuning operation.
  final double completedPercent;

  /// Metrics collected during tuning.
  final List<TuningSnapshot> snapshots;

  CreateTunedModelMetadata({
    this.tunedModel = '',
    this.totalSteps = 0,
    this.completedSteps = 0,
    this.completedPercent = 0,
    this.snapshots = const [],
  }) : super(fullyQualifiedName);

  factory CreateTunedModelMetadata.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CreateTunedModelMetadata(
      tunedModel: switch (json['tunedModel']) {
        null => '',
        Object $1 => decodeString($1),
      },
      totalSteps: switch (json['totalSteps']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      completedSteps: switch (json['completedSteps']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      completedPercent: switch (json['completedPercent']) {
        null => 0,
        Object $1 => decodeDouble($1),
      },
      snapshots: switch (json['snapshots']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) TuningSnapshot.fromJson(i)],
        _ => throw const FormatException('"snapshots" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (tunedModel.isNotDefault) 'tunedModel': tunedModel,
    if (totalSteps.isNotDefault) 'totalSteps': totalSteps,
    if (completedSteps.isNotDefault) 'completedSteps': completedSteps,
    if (completedPercent.isNotDefault)
      'completedPercent': encodeDouble(completedPercent),
    if (snapshots.isNotDefault) 'snapshots': encodeList(snapshots),
  };

  @override
  String toString() {
    final contents = [
      'tunedModel=$tunedModel',
      'totalSteps=$totalSteps',
      'completedSteps=$completedSteps',
      'completedPercent=$completedPercent',
    ].join(',');
    return 'CreateTunedModelMetadata($contents)';
  }
}

/// Request to update a TunedModel.
final class UpdateTunedModelRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.UpdateTunedModelRequest';

  /// Required. The tuned model to update.
  final TunedModel? tunedModel;

  /// Optional. The list of fields to update.
  final FieldMask? updateMask;

  UpdateTunedModelRequest({required this.tunedModel, this.updateMask})
    : super(fullyQualifiedName);

  factory UpdateTunedModelRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return UpdateTunedModelRequest(
      tunedModel: switch (json['tunedModel']) {
        null => null,
        Object $1 => TunedModel.fromJson($1),
      },
      updateMask: switch (json['updateMask']) {
        null => null,
        Object $1 => FieldMask.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (tunedModel != null) 'tunedModel': tunedModel!.toJson(),
    if (updateMask != null) 'updateMask': updateMask!.toJson(),
  };

  @override
  String toString() => 'UpdateTunedModelRequest()';
}

/// Request to delete a TunedModel.
final class DeleteTunedModelRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.DeleteTunedModelRequest';

  /// Required. The resource name of the model.
  /// Format: `tunedModels/my-model-id`
  final String name;

  DeleteTunedModelRequest({required this.name}) : super(fullyQualifiedName);

  factory DeleteTunedModelRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return DeleteTunedModelRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {'name': name};

  @override
  String toString() {
    final contents = ['name=$name'].join(',');
    return 'DeleteTunedModelRequest($contents)';
  }
}

/// Permission resource grants user, group or the rest of the world access to the
/// PaLM API resource (e.g. a tuned model, corpus).
///
/// A role is a collection of permitted operations that allows users to perform
/// specific actions on PaLM API resources. To make them available to users,
/// groups, or service accounts, you assign roles. When you assign a role, you
/// grant permissions that the role contains.
///
/// There are three concentric roles. Each role is a superset of the previous
/// role's permitted operations:
///
/// - reader can use the resource (e.g. tuned model, corpus) for inference
/// - writer has reader's permissions and additionally can edit and share
/// - owner has writer's permissions and additionally can delete
final class Permission extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Permission';

  /// Output only. Identifier. The permission name. A unique name will be
  /// generated on create. Examples:
  ///     tunedModels/{tuned_model}/permissions/{permission}
  ///     corpora/{corpus}/permissions/{permission}
  /// Output only.
  final String name;

  /// Optional. Immutable. The type of the grantee.
  final Permission_GranteeType? granteeType;

  /// Optional. Immutable. The email address of the user of group which this
  /// permission refers. Field is not set when permission's grantee type is
  /// EVERYONE.
  final String? emailAddress;

  /// Required. The role granted by this permission.
  final Permission_Role? role;

  Permission({
    this.name = '',
    this.granteeType,
    this.emailAddress,
    required this.role,
  }) : super(fullyQualifiedName);

  factory Permission.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Permission(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
      granteeType: switch (json['granteeType']) {
        null => null,
        Object $1 => Permission_GranteeType.fromJson($1),
      },
      emailAddress: switch (json['emailAddress']) {
        null => null,
        Object $1 => decodeString($1),
      },
      role: switch (json['role']) {
        null => null,
        Object $1 => Permission_Role.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (name.isNotDefault) 'name': name,
    if (granteeType != null) 'granteeType': granteeType!.toJson(),
    if (emailAddress != null) 'emailAddress': emailAddress,
    if (role != null) 'role': role!.toJson(),
  };

  @override
  String toString() {
    final contents = [
      'name=$name',
      if (granteeType != null) 'granteeType=$granteeType',
      if (emailAddress != null) 'emailAddress=$emailAddress',
      if (role != null) 'role=$role',
    ].join(',');
    return 'Permission($contents)';
  }
}

/// Defines types of the grantee of this permission.
final class Permission_GranteeType extends ProtoEnum {
  /// The default value. This value is unused.
  static const granteeTypeUnspecified = Permission_GranteeType(
    'GRANTEE_TYPE_UNSPECIFIED',
  );

  /// Represents a user. When set, you must provide email_address for the user.
  static const user = Permission_GranteeType('USER');

  /// Represents a group. When set, you must provide email_address for the
  /// group.
  static const group = Permission_GranteeType('GROUP');

  /// Represents access to everyone. No extra information is required.
  static const everyone = Permission_GranteeType('EVERYONE');

  /// The default value for [Permission_GranteeType].
  static const $default = granteeTypeUnspecified;

  const Permission_GranteeType(super.value);

  factory Permission_GranteeType.fromJson(Object? json) =>
      Permission_GranteeType(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'GranteeType.$value';
}

/// Defines the role granted by this permission.
final class Permission_Role extends ProtoEnum {
  /// The default value. This value is unused.
  static const roleUnspecified = Permission_Role('ROLE_UNSPECIFIED');

  /// Owner can use, update, share and delete the resource.
  static const owner = Permission_Role('OWNER');

  /// Writer can use, update and share the resource.
  static const writer = Permission_Role('WRITER');

  /// Reader can use the resource.
  static const reader = Permission_Role('READER');

  /// The default value for [Permission_Role].
  static const $default = roleUnspecified;

  const Permission_Role(super.value);

  factory Permission_Role.fromJson(Object? json) =>
      Permission_Role(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'Role.$value';
}

/// Request to create a `Permission`.
final class CreatePermissionRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CreatePermissionRequest';

  /// Required. The parent resource of the `Permission`.
  /// Formats:
  ///    `tunedModels/{tuned_model}`
  ///    `corpora/{corpus}`
  final String parent;

  /// Required. The permission to create.
  final Permission? permission;

  CreatePermissionRequest({required this.parent, required this.permission})
    : super(fullyQualifiedName);

  factory CreatePermissionRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CreatePermissionRequest(
      parent: switch (json['parent']) {
        null => '',
        Object $1 => decodeString($1),
      },
      permission: switch (json['permission']) {
        null => null,
        Object $1 => Permission.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    'parent': parent,
    if (permission != null) 'permission': permission!.toJson(),
  };

  @override
  String toString() {
    final contents = ['parent=$parent'].join(',');
    return 'CreatePermissionRequest($contents)';
  }
}

/// Request for getting information about a specific `Permission`.
final class GetPermissionRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GetPermissionRequest';

  /// Required. The resource name of the permission.
  ///
  /// Formats:
  ///    `tunedModels/{tuned_model}/permissions/{permission}`
  ///    `corpora/{corpus}/permissions/{permission}`
  final String name;

  GetPermissionRequest({required this.name}) : super(fullyQualifiedName);

  factory GetPermissionRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GetPermissionRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {'name': name};

  @override
  String toString() {
    final contents = ['name=$name'].join(',');
    return 'GetPermissionRequest($contents)';
  }
}

/// Request for listing permissions.
final class ListPermissionsRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ListPermissionsRequest';

  /// Required. The parent resource of the permissions.
  /// Formats:
  ///    `tunedModels/{tuned_model}`
  ///    `corpora/{corpus}`
  final String parent;

  /// Optional. The maximum number of `Permission`s to return (per page).
  /// The service may return fewer permissions.
  ///
  /// If unspecified, at most 10 permissions will be returned.
  /// This method returns at most 1000 permissions per page, even if you pass
  /// larger page_size.
  final int pageSize;

  /// Optional. A page token, received from a previous `ListPermissions` call.
  ///
  /// Provide the `page_token` returned by one request as an argument to the
  /// next request to retrieve the next page.
  ///
  /// When paginating, all other parameters provided to `ListPermissions`
  /// must match the call that provided the page token.
  final String pageToken;

  ListPermissionsRequest({
    required this.parent,
    this.pageSize = 0,
    this.pageToken = '',
  }) : super(fullyQualifiedName);

  factory ListPermissionsRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ListPermissionsRequest(
      parent: switch (json['parent']) {
        null => '',
        Object $1 => decodeString($1),
      },
      pageSize: switch (json['pageSize']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      pageToken: switch (json['pageToken']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    'parent': parent,
    if (pageSize.isNotDefault) 'pageSize': pageSize,
    if (pageToken.isNotDefault) 'pageToken': pageToken,
  };

  @override
  String toString() {
    final contents = [
      'parent=$parent',
      'pageSize=$pageSize',
      'pageToken=$pageToken',
    ].join(',');
    return 'ListPermissionsRequest($contents)';
  }
}

/// Response from `ListPermissions` containing a paginated list of
/// permissions.
final class ListPermissionsResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ListPermissionsResponse';

  /// Returned permissions.
  final List<Permission> permissions;

  /// A token, which can be sent as `page_token` to retrieve the next page.
  ///
  /// If this field is omitted, there are no more pages.
  final String nextPageToken;

  ListPermissionsResponse({
    this.permissions = const [],
    this.nextPageToken = '',
  }) : super(fullyQualifiedName);

  factory ListPermissionsResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ListPermissionsResponse(
      permissions: switch (json['permissions']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Permission.fromJson(i)],
        _ => throw const FormatException('"permissions" is not a list'),
      },
      nextPageToken: switch (json['nextPageToken']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (permissions.isNotDefault) 'permissions': encodeList(permissions),
    if (nextPageToken.isNotDefault) 'nextPageToken': nextPageToken,
  };

  @override
  String toString() {
    final contents = ['nextPageToken=$nextPageToken'].join(',');
    return 'ListPermissionsResponse($contents)';
  }
}

/// Request to update the `Permission`.
final class UpdatePermissionRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.UpdatePermissionRequest';

  /// Required. The permission to update.
  ///
  /// The permission's `name` field is used to identify the permission to update.
  final Permission? permission;

  /// Required. The list of fields to update. Accepted ones:
  ///  - role (`Permission.role` field)
  final FieldMask? updateMask;

  UpdatePermissionRequest({required this.permission, required this.updateMask})
    : super(fullyQualifiedName);

  factory UpdatePermissionRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return UpdatePermissionRequest(
      permission: switch (json['permission']) {
        null => null,
        Object $1 => Permission.fromJson($1),
      },
      updateMask: switch (json['updateMask']) {
        null => null,
        Object $1 => FieldMask.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (permission != null) 'permission': permission!.toJson(),
    if (updateMask != null) 'updateMask': updateMask!.toJson(),
  };

  @override
  String toString() => 'UpdatePermissionRequest()';
}

/// Request to delete the `Permission`.
final class DeletePermissionRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.DeletePermissionRequest';

  /// Required. The resource name of the permission.
  /// Formats:
  ///    `tunedModels/{tuned_model}/permissions/{permission}`
  ///    `corpora/{corpus}/permissions/{permission}`
  final String name;

  DeletePermissionRequest({required this.name}) : super(fullyQualifiedName);

  factory DeletePermissionRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return DeletePermissionRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {'name': name};

  @override
  String toString() {
    final contents = ['name=$name'].join(',');
    return 'DeletePermissionRequest($contents)';
  }
}

/// Request to transfer the ownership of the tuned model.
final class TransferOwnershipRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.TransferOwnershipRequest';

  /// Required. The resource name of the tuned model to transfer ownership.
  ///
  /// Format: `tunedModels/my-model-id`
  final String name;

  /// Required. The email address of the user to whom the tuned model is being
  /// transferred to.
  final String emailAddress;

  TransferOwnershipRequest({required this.name, required this.emailAddress})
    : super(fullyQualifiedName);

  factory TransferOwnershipRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return TransferOwnershipRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
      emailAddress: switch (json['emailAddress']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {'name': name, 'emailAddress': emailAddress};

  @override
  String toString() {
    final contents = ['name=$name', 'emailAddress=$emailAddress'].join(',');
    return 'TransferOwnershipRequest($contents)';
  }
}

/// Response from `TransferOwnership`.
final class TransferOwnershipResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.TransferOwnershipResponse';

  TransferOwnershipResponse() : super(fullyQualifiedName);

  factory TransferOwnershipResponse.fromJson(Object? j) =>
      TransferOwnershipResponse();

  @override
  Object toJson() => {};

  @override
  String toString() => 'TransferOwnershipResponse()';
}

/// Request message for
/// `PredictionService.Predict`.
final class PredictRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.PredictRequest';

  /// Required. The name of the model for prediction.
  /// Format: `name=models/{model}`.
  final String model;

  /// Required. The instances that are the input to the prediction call.
  final List<Value> instances;

  /// Optional. The parameters that govern the prediction call.
  final Value? parameters;

  PredictRequest({
    required this.model,
    required this.instances,
    this.parameters,
  }) : super(fullyQualifiedName);

  factory PredictRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return PredictRequest(
      model: switch (json['model']) {
        null => '',
        Object $1 => decodeString($1),
      },
      instances: switch (json['instances']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Value.fromJson(i)],
        _ => throw const FormatException('"instances" is not a list'),
      },
      parameters: switch (json['parameters']) {
        null => null,
        Object $1 => Value.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    'model': model,
    'instances': encodeList(instances),
    if (parameters != null) 'parameters': parameters!.toJson(),
  };

  @override
  String toString() {
    final contents = ['model=$model'].join(',');
    return 'PredictRequest($contents)';
  }
}

/// Request message for [PredictionService.PredictLongRunning].
final class PredictLongRunningRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.PredictLongRunningRequest';

  /// Required. The name of the model for prediction.
  /// Format: `name=models/{model}`.
  final String model;

  /// Required. The instances that are the input to the prediction call.
  final List<Value> instances;

  /// Optional. The parameters that govern the prediction call.
  final Value? parameters;

  PredictLongRunningRequest({
    required this.model,
    required this.instances,
    this.parameters,
  }) : super(fullyQualifiedName);

  factory PredictLongRunningRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return PredictLongRunningRequest(
      model: switch (json['model']) {
        null => '',
        Object $1 => decodeString($1),
      },
      instances: switch (json['instances']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Value.fromJson(i)],
        _ => throw const FormatException('"instances" is not a list'),
      },
      parameters: switch (json['parameters']) {
        null => null,
        Object $1 => Value.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    'model': model,
    'instances': encodeList(instances),
    if (parameters != null) 'parameters': parameters!.toJson(),
  };

  @override
  String toString() {
    final contents = ['model=$model'].join(',');
    return 'PredictLongRunningRequest($contents)';
  }
}

/// Response message for [PredictionService.Predict].
final class PredictResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.PredictResponse';

  /// The outputs of the prediction call.
  final List<Value> predictions;

  PredictResponse({this.predictions = const []}) : super(fullyQualifiedName);

  factory PredictResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return PredictResponse(
      predictions: switch (json['predictions']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Value.fromJson(i)],
        _ => throw const FormatException('"predictions" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (predictions.isNotDefault) 'predictions': encodeList(predictions),
  };

  @override
  String toString() => 'PredictResponse()';
}

/// Response message for [PredictionService.PredictLongRunning]
final class PredictLongRunningResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.PredictLongRunningResponse';

  /// The response of the video generation prediction.
  final PredictLongRunningGeneratedVideoResponse? generateVideoResponse;

  PredictLongRunningResponse({this.generateVideoResponse})
    : super(fullyQualifiedName);

  factory PredictLongRunningResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return PredictLongRunningResponse(
      generateVideoResponse: switch (json['generateVideoResponse']) {
        null => null,
        Object $1 => PredictLongRunningGeneratedVideoResponse.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (generateVideoResponse != null)
      'generateVideoResponse': generateVideoResponse!.toJson(),
  };

  @override
  String toString() => 'PredictLongRunningResponse()';
}

/// Metadata for PredictLongRunning long running operations.
final class PredictLongRunningMetadata extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.PredictLongRunningMetadata';

  PredictLongRunningMetadata() : super(fullyQualifiedName);

  factory PredictLongRunningMetadata.fromJson(Object? j) =>
      PredictLongRunningMetadata();

  @override
  Object toJson() => {};

  @override
  String toString() => 'PredictLongRunningMetadata()';
}

/// A proto encapsulate various type of media.
final class Media extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Media';

  /// Video as the only one for now.  This is mimicking Vertex proto.
  final Video? video;

  Media({this.video}) : super(fullyQualifiedName);

  factory Media.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Media(
      video: switch (json['video']) {
        null => null,
        Object $1 => Video.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {if (video != null) 'video': video!.toJson()};

  @override
  String toString() => 'Media()';
}

/// Representation of a video.
final class Video extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Video';

  /// Raw bytes.
  final Uint8List? video;

  /// Path to another storage.
  final String? uri;

  Video({this.video, this.uri}) : super(fullyQualifiedName);

  factory Video.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Video(
      video: switch (json['video']) {
        null => null,
        Object $1 => decodeBytes($1),
      },
      uri: switch (json['uri']) {
        null => null,
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (video != null) 'video': encodeBytes(video),
    if (uri != null) 'uri': uri,
  };

  @override
  String toString() {
    final contents = [
      if (video != null) 'video=$video',
      if (uri != null) 'uri=$uri',
    ].join(',');
    return 'Video($contents)';
  }
}

/// Veo response.
final class PredictLongRunningGeneratedVideoResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.PredictLongRunningGeneratedVideoResponse';

  /// The generated samples.
  final List<Media> generatedSamples;

  /// Returns if any videos were filtered due to RAI policies.
  final int raiMediaFilteredCount;

  /// Returns rai failure reasons if any.
  final List<String> raiMediaFilteredReasons;

  PredictLongRunningGeneratedVideoResponse({
    this.generatedSamples = const [],
    this.raiMediaFilteredCount = 0,
    this.raiMediaFilteredReasons = const [],
  }) : super(fullyQualifiedName);

  factory PredictLongRunningGeneratedVideoResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return PredictLongRunningGeneratedVideoResponse(
      generatedSamples: switch (json['generatedSamples']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Media.fromJson(i)],
        _ => throw const FormatException('"generatedSamples" is not a list'),
      },
      raiMediaFilteredCount: switch (json['raiMediaFilteredCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      raiMediaFilteredReasons: switch (json['raiMediaFilteredReasons']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) decodeString(i)],
        _ => throw const FormatException(
          '"raiMediaFilteredReasons" is not a list',
        ),
      },
    );
  }

  @override
  Object toJson() => {
    if (generatedSamples.isNotDefault)
      'generatedSamples': encodeList(generatedSamples),
    if (raiMediaFilteredCount.isNotDefault)
      'raiMediaFilteredCount': raiMediaFilteredCount,
    if (raiMediaFilteredReasons.isNotDefault)
      'raiMediaFilteredReasons': raiMediaFilteredReasons,
  };

  @override
  String toString() {
    final contents = ['raiMediaFilteredCount=$raiMediaFilteredCount'].join(',');
    return 'PredictLongRunningGeneratedVideoResponse($contents)';
  }
}

/// A `Corpus` is a collection of `Document`s.
/// A project can create up to 5 corpora.
final class Corpus extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Corpus';

  /// Immutable. Identifier. The `Corpus` resource name. The ID (name excluding
  /// the "corpora/" prefix) can contain up to 40 characters that are lowercase
  /// alphanumeric or dashes
  /// (-). The ID cannot start or end with a dash. If the name is empty on
  /// create, a unique name will be derived from `display_name` along with a 12
  /// character random suffix.
  /// Example: `corpora/my-awesome-corpora-123a456b789c`
  final String name;

  /// Optional. The human-readable display name for the `Corpus`. The display
  /// name must be no more than 512 characters in length, including spaces.
  /// Example: "Docs on Semantic Retriever"
  final String displayName;

  /// Output only. The Timestamp of when the `Corpus` was created.
  final Timestamp? createTime;

  /// Output only. The Timestamp of when the `Corpus` was last updated.
  final Timestamp? updateTime;

  Corpus({
    this.name = '',
    this.displayName = '',
    this.createTime,
    this.updateTime,
  }) : super(fullyQualifiedName);

  factory Corpus.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Corpus(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
      displayName: switch (json['displayName']) {
        null => '',
        Object $1 => decodeString($1),
      },
      createTime: switch (json['createTime']) {
        null => null,
        Object $1 => Timestamp.fromJson($1),
      },
      updateTime: switch (json['updateTime']) {
        null => null,
        Object $1 => Timestamp.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (name.isNotDefault) 'name': name,
    if (displayName.isNotDefault) 'displayName': displayName,
    if (createTime != null) 'createTime': createTime!.toJson(),
    if (updateTime != null) 'updateTime': updateTime!.toJson(),
  };

  @override
  String toString() {
    final contents = ['name=$name', 'displayName=$displayName'].join(',');
    return 'Corpus($contents)';
  }
}

/// A `Document` is a collection of `Chunk`s.
/// A `Corpus` can have a maximum of 10,000 `Document`s.
final class Document extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Document';

  /// Immutable. Identifier. The `Document` resource name. The ID (name excluding
  /// the "corpora/*/documents/" prefix) can contain up to 40 characters that are
  /// lowercase alphanumeric or dashes (-). The ID cannot start or end with a
  /// dash. If the name is empty on create, a unique name will be derived from
  /// `display_name` along with a 12 character random suffix.
  /// Example: `corpora/{corpus_id}/documents/my-awesome-doc-123a456b789c`
  final String name;

  /// Optional. The human-readable display name for the `Document`. The display
  /// name must be no more than 512 characters in length, including spaces.
  /// Example: "Semantic Retriever Documentation"
  final String displayName;

  /// Optional. User provided custom metadata stored as key-value pairs used for
  /// querying. A `Document` can have a maximum of 20 `CustomMetadata`.
  final List<CustomMetadata> customMetadata;

  /// Output only. The Timestamp of when the `Document` was last updated.
  final Timestamp? updateTime;

  /// Output only. The Timestamp of when the `Document` was created.
  final Timestamp? createTime;

  Document({
    this.name = '',
    this.displayName = '',
    this.customMetadata = const [],
    this.updateTime,
    this.createTime,
  }) : super(fullyQualifiedName);

  factory Document.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Document(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
      displayName: switch (json['displayName']) {
        null => '',
        Object $1 => decodeString($1),
      },
      customMetadata: switch (json['customMetadata']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) CustomMetadata.fromJson(i)],
        _ => throw const FormatException('"customMetadata" is not a list'),
      },
      updateTime: switch (json['updateTime']) {
        null => null,
        Object $1 => Timestamp.fromJson($1),
      },
      createTime: switch (json['createTime']) {
        null => null,
        Object $1 => Timestamp.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (name.isNotDefault) 'name': name,
    if (displayName.isNotDefault) 'displayName': displayName,
    if (customMetadata.isNotDefault)
      'customMetadata': encodeList(customMetadata),
    if (updateTime != null) 'updateTime': updateTime!.toJson(),
    if (createTime != null) 'createTime': createTime!.toJson(),
  };

  @override
  String toString() {
    final contents = ['name=$name', 'displayName=$displayName'].join(',');
    return 'Document($contents)';
  }
}

/// User provided string values assigned to a single metadata key.
final class StringList extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.StringList';

  /// The string values of the metadata to store.
  final List<String> values;

  StringList({this.values = const []}) : super(fullyQualifiedName);

  factory StringList.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return StringList(
      values: switch (json['values']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) decodeString(i)],
        _ => throw const FormatException('"values" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {if (values.isNotDefault) 'values': values};

  @override
  String toString() => 'StringList()';
}

/// User provided metadata stored as key-value pairs.
final class CustomMetadata extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CustomMetadata';

  /// The string value of the metadata to store.
  final String? stringValue;

  /// The StringList value of the metadata to store.
  final StringList? stringListValue;

  /// The numeric value of the metadata to store.
  final double? numericValue;

  /// Required. The key of the metadata to store.
  final String key;

  CustomMetadata({
    this.stringValue,
    this.stringListValue,
    this.numericValue,
    required this.key,
  }) : super(fullyQualifiedName);

  factory CustomMetadata.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CustomMetadata(
      stringValue: switch (json['stringValue']) {
        null => null,
        Object $1 => decodeString($1),
      },
      stringListValue: switch (json['stringListValue']) {
        null => null,
        Object $1 => StringList.fromJson($1),
      },
      numericValue: switch (json['numericValue']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      key: switch (json['key']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (stringValue != null) 'stringValue': stringValue,
    if (stringListValue != null) 'stringListValue': stringListValue!.toJson(),
    if (numericValue != null) 'numericValue': encodeDouble(numericValue),
    'key': key,
  };

  @override
  String toString() {
    final contents = [
      if (stringValue != null) 'stringValue=$stringValue',
      if (numericValue != null) 'numericValue=$numericValue',
      'key=$key',
    ].join(',');
    return 'CustomMetadata($contents)';
  }
}

/// User provided filter to limit retrieval based on `Chunk` or `Document` level
/// metadata values.
/// Example (genre = drama OR genre = action):
///   key = "document.custom_metadata.genre"
///   conditions = [{string_value = "drama", operation = EQUAL},
///                 {string_value = "action", operation = EQUAL}]
final class MetadataFilter extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.MetadataFilter';

  /// Required. The key of the metadata to filter on.
  final String key;

  /// Required. The `Condition`s for the given key that will trigger this filter.
  /// Multiple `Condition`s are joined by logical ORs.
  final List<Condition> conditions;

  MetadataFilter({required this.key, required this.conditions})
    : super(fullyQualifiedName);

  factory MetadataFilter.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return MetadataFilter(
      key: switch (json['key']) {
        null => '',
        Object $1 => decodeString($1),
      },
      conditions: switch (json['conditions']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Condition.fromJson(i)],
        _ => throw const FormatException('"conditions" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {'key': key, 'conditions': encodeList(conditions)};

  @override
  String toString() {
    final contents = ['key=$key'].join(',');
    return 'MetadataFilter($contents)';
  }
}

/// Filter condition applicable to a single key.
final class Condition extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Condition';

  /// The string value to filter the metadata on.
  final String? stringValue;

  /// The numeric value to filter the metadata on.
  final double? numericValue;

  /// Required. Operator applied to the given key-value pair to trigger the
  /// condition.
  final Condition_Operator operation;

  Condition({this.stringValue, this.numericValue, required this.operation})
    : super(fullyQualifiedName);

  factory Condition.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Condition(
      stringValue: switch (json['stringValue']) {
        null => null,
        Object $1 => decodeString($1),
      },
      numericValue: switch (json['numericValue']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      operation: switch (json['operation']) {
        null => Condition_Operator.$default,
        Object $1 => Condition_Operator.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (stringValue != null) 'stringValue': stringValue,
    if (numericValue != null) 'numericValue': encodeDouble(numericValue),
    'operation': operation.toJson(),
  };

  @override
  String toString() {
    final contents = [
      if (stringValue != null) 'stringValue=$stringValue',
      if (numericValue != null) 'numericValue=$numericValue',
      'operation=$operation',
    ].join(',');
    return 'Condition($contents)';
  }
}

/// Defines the valid operators that can be applied to a key-value pair.
final class Condition_Operator extends ProtoEnum {
  /// The default value. This value is unused.
  static const operatorUnspecified = Condition_Operator('OPERATOR_UNSPECIFIED');

  /// Supported by numeric.
  static const less = Condition_Operator('LESS');

  /// Supported by numeric.
  static const lessEqual = Condition_Operator('LESS_EQUAL');

  /// Supported by numeric & string.
  static const equal = Condition_Operator('EQUAL');

  /// Supported by numeric.
  static const greaterEqual = Condition_Operator('GREATER_EQUAL');

  /// Supported by numeric.
  static const greater = Condition_Operator('GREATER');

  /// Supported by numeric & string.
  static const notEqual = Condition_Operator('NOT_EQUAL');

  /// Supported by string only when `CustomMetadata` value type for the given
  /// key has a `string_list_value`.
  static const includes = Condition_Operator('INCLUDES');

  /// Supported by string only when `CustomMetadata` value type for the given
  /// key has a `string_list_value`.
  static const excludes = Condition_Operator('EXCLUDES');

  /// The default value for [Condition_Operator].
  static const $default = operatorUnspecified;

  const Condition_Operator(super.value);

  factory Condition_Operator.fromJson(Object? json) =>
      Condition_Operator(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'Operator.$value';
}

/// A `Chunk` is a subpart of a `Document` that is treated as an independent unit
/// for the purposes of vector representation and storage.
/// A `Corpus` can have a maximum of 1 million `Chunk`s.
final class Chunk extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Chunk';

  /// Immutable. Identifier. The `Chunk` resource name. The ID (name excluding
  /// the "corpora/*/documents/*/chunks/" prefix) can contain up to 40 characters
  /// that are lowercase alphanumeric or dashes (-). The ID cannot start or end
  /// with a dash. If the name is empty on create, a random 12-character unique
  /// ID will be generated.
  /// Example: `corpora/{corpus_id}/documents/{document_id}/chunks/123a456b789c`
  final String name;

  /// Required. The content for the `Chunk`, such as the text string.
  /// The maximum number of tokens per chunk is 2043.
  final ChunkData? data;

  /// Optional. User provided custom metadata stored as key-value pairs.
  /// The maximum number of `CustomMetadata` per chunk is 20.
  final List<CustomMetadata> customMetadata;

  /// Output only. The Timestamp of when the `Chunk` was created.
  final Timestamp? createTime;

  /// Output only. The Timestamp of when the `Chunk` was last updated.
  final Timestamp? updateTime;

  /// Output only. Current state of the `Chunk`.
  final Chunk_State state;

  Chunk({
    this.name = '',
    required this.data,
    this.customMetadata = const [],
    this.createTime,
    this.updateTime,
    this.state = Chunk_State.$default,
  }) : super(fullyQualifiedName);

  factory Chunk.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Chunk(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
      data: switch (json['data']) {
        null => null,
        Object $1 => ChunkData.fromJson($1),
      },
      customMetadata: switch (json['customMetadata']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) CustomMetadata.fromJson(i)],
        _ => throw const FormatException('"customMetadata" is not a list'),
      },
      createTime: switch (json['createTime']) {
        null => null,
        Object $1 => Timestamp.fromJson($1),
      },
      updateTime: switch (json['updateTime']) {
        null => null,
        Object $1 => Timestamp.fromJson($1),
      },
      state: switch (json['state']) {
        null => Chunk_State.$default,
        Object $1 => Chunk_State.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (name.isNotDefault) 'name': name,
    if (data != null) 'data': data!.toJson(),
    if (customMetadata.isNotDefault)
      'customMetadata': encodeList(customMetadata),
    if (createTime != null) 'createTime': createTime!.toJson(),
    if (updateTime != null) 'updateTime': updateTime!.toJson(),
    if (state.isNotDefault) 'state': state.toJson(),
  };

  @override
  String toString() {
    final contents = ['name=$name', 'state=$state'].join(',');
    return 'Chunk($contents)';
  }
}

/// States for the lifecycle of a `Chunk`.
final class Chunk_State extends ProtoEnum {
  /// The default value. This value is used if the state is omitted.
  static const stateUnspecified = Chunk_State('STATE_UNSPECIFIED');

  /// `Chunk` is being processed (embedding and vector storage).
  static const statePendingProcessing = Chunk_State('STATE_PENDING_PROCESSING');

  /// `Chunk` is processed and available for querying.
  static const stateActive = Chunk_State('STATE_ACTIVE');

  /// `Chunk` failed processing.
  static const stateFailed = Chunk_State('STATE_FAILED');

  /// The default value for [Chunk_State].
  static const $default = stateUnspecified;

  const Chunk_State(super.value);

  factory Chunk_State.fromJson(Object? json) => Chunk_State(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'State.$value';
}

/// Extracted data that represents the `Chunk` content.
final class ChunkData extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ChunkData';

  /// The `Chunk` content as a string.
  /// The maximum number of tokens per chunk is 2043.
  final String? stringValue;

  ChunkData({this.stringValue}) : super(fullyQualifiedName);

  factory ChunkData.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ChunkData(
      stringValue: switch (json['stringValue']) {
        null => null,
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {if (stringValue != null) 'stringValue': stringValue};

  @override
  String toString() {
    final contents = [
      if (stringValue != null) 'stringValue=$stringValue',
    ].join(',');
    return 'ChunkData($contents)';
  }
}

/// Request to create a `Corpus`.
final class CreateCorpusRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CreateCorpusRequest';

  /// Required. The `Corpus` to create.
  final Corpus? corpus;

  CreateCorpusRequest({required this.corpus}) : super(fullyQualifiedName);

  factory CreateCorpusRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CreateCorpusRequest(
      corpus: switch (json['corpus']) {
        null => null,
        Object $1 => Corpus.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {if (corpus != null) 'corpus': corpus!.toJson()};

  @override
  String toString() => 'CreateCorpusRequest()';
}

/// Request for getting information about a specific `Corpus`.
final class GetCorpusRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GetCorpusRequest';

  /// Required. The name of the `Corpus`.
  /// Example: `corpora/my-corpus-123`
  final String name;

  GetCorpusRequest({required this.name}) : super(fullyQualifiedName);

  factory GetCorpusRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GetCorpusRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {'name': name};

  @override
  String toString() {
    final contents = ['name=$name'].join(',');
    return 'GetCorpusRequest($contents)';
  }
}

/// Request to update a `Corpus`.
final class UpdateCorpusRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.UpdateCorpusRequest';

  /// Required. The `Corpus` to update.
  final Corpus? corpus;

  /// Required. The list of fields to update.
  /// Currently, this only supports updating `display_name`.
  final FieldMask? updateMask;

  UpdateCorpusRequest({required this.corpus, required this.updateMask})
    : super(fullyQualifiedName);

  factory UpdateCorpusRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return UpdateCorpusRequest(
      corpus: switch (json['corpus']) {
        null => null,
        Object $1 => Corpus.fromJson($1),
      },
      updateMask: switch (json['updateMask']) {
        null => null,
        Object $1 => FieldMask.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (corpus != null) 'corpus': corpus!.toJson(),
    if (updateMask != null) 'updateMask': updateMask!.toJson(),
  };

  @override
  String toString() => 'UpdateCorpusRequest()';
}

/// Request to delete a `Corpus`.
final class DeleteCorpusRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.DeleteCorpusRequest';

  /// Required. The resource name of the `Corpus`.
  /// Example: `corpora/my-corpus-123`
  final String name;

  /// Optional. If set to true, any `Document`s and objects related to this
  /// `Corpus` will also be deleted.
  ///
  /// If false (the default), a `FAILED_PRECONDITION` error will be returned if
  /// `Corpus` contains any `Document`s.
  final bool force;

  DeleteCorpusRequest({required this.name, this.force = false})
    : super(fullyQualifiedName);

  factory DeleteCorpusRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return DeleteCorpusRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
      force: switch (json['force']) {
        null => false,
        Object $1 => decodeBool($1),
      },
    );
  }

  @override
  Object toJson() => {'name': name, if (force.isNotDefault) 'force': force};

  @override
  String toString() {
    final contents = ['name=$name', 'force=$force'].join(',');
    return 'DeleteCorpusRequest($contents)';
  }
}

/// Request for listing `Corpora`.
final class ListCorporaRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ListCorporaRequest';

  /// Optional. The maximum number of `Corpora` to return (per page).
  /// The service may return fewer `Corpora`.
  ///
  /// If unspecified, at most 10 `Corpora` will be returned.
  /// The maximum size limit is 20 `Corpora` per page.
  final int pageSize;

  /// Optional. A page token, received from a previous `ListCorpora` call.
  ///
  /// Provide the `next_page_token` returned in the response as an argument to
  /// the next request to retrieve the next page.
  ///
  /// When paginating, all other parameters provided to `ListCorpora`
  /// must match the call that provided the page token.
  final String pageToken;

  ListCorporaRequest({this.pageSize = 0, this.pageToken = ''})
    : super(fullyQualifiedName);

  factory ListCorporaRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ListCorporaRequest(
      pageSize: switch (json['pageSize']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      pageToken: switch (json['pageToken']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (pageSize.isNotDefault) 'pageSize': pageSize,
    if (pageToken.isNotDefault) 'pageToken': pageToken,
  };

  @override
  String toString() {
    final contents = ['pageSize=$pageSize', 'pageToken=$pageToken'].join(',');
    return 'ListCorporaRequest($contents)';
  }
}

/// Response from `ListCorpora` containing a paginated list of `Corpora`.
/// The results are sorted by ascending `corpus.create_time`.
final class ListCorporaResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ListCorporaResponse';

  /// The returned corpora.
  final List<Corpus> corpora;

  /// A token, which can be sent as `page_token` to retrieve the next page.
  /// If this field is omitted, there are no more pages.
  final String nextPageToken;

  ListCorporaResponse({this.corpora = const [], this.nextPageToken = ''})
    : super(fullyQualifiedName);

  factory ListCorporaResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ListCorporaResponse(
      corpora: switch (json['corpora']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Corpus.fromJson(i)],
        _ => throw const FormatException('"corpora" is not a list'),
      },
      nextPageToken: switch (json['nextPageToken']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (corpora.isNotDefault) 'corpora': encodeList(corpora),
    if (nextPageToken.isNotDefault) 'nextPageToken': nextPageToken,
  };

  @override
  String toString() {
    final contents = ['nextPageToken=$nextPageToken'].join(',');
    return 'ListCorporaResponse($contents)';
  }
}

/// Request for querying a `Corpus`.
final class QueryCorpusRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.QueryCorpusRequest';

  /// Required. The name of the `Corpus` to query.
  /// Example: `corpora/my-corpus-123`
  final String name;

  /// Required. Query string to perform semantic search.
  final String query;

  /// Optional. Filter for `Chunk` and `Document` metadata. Each `MetadataFilter`
  /// object should correspond to a unique key. Multiple `MetadataFilter` objects
  /// are joined by logical "AND"s.
  ///
  /// Example query at document level:
  /// (year >= 2020 OR year < 2010) AND (genre = drama OR genre = action)
  ///
  /// `MetadataFilter` object list:
  ///  metadata_filters = [
  ///  {key = "document.custom_metadata.year"
  ///   conditions = [{int_value = 2020, operation = GREATER_EQUAL},
  ///                 {int_value = 2010, operation = LESS}]},
  ///  {key = "document.custom_metadata.year"
  ///   conditions = [{int_value = 2020, operation = GREATER_EQUAL},
  ///                 {int_value = 2010, operation = LESS}]},
  ///  {key = "document.custom_metadata.genre"
  ///   conditions = [{string_value = "drama", operation = EQUAL},
  ///                 {string_value = "action", operation = EQUAL}]}]
  ///
  /// Example query at chunk level for a numeric range of values:
  /// (year > 2015 AND year <= 2020)
  ///
  /// `MetadataFilter` object list:
  ///  metadata_filters = [
  ///  {key = "chunk.custom_metadata.year"
  ///   conditions = [{int_value = 2015, operation = GREATER}]},
  ///  {key = "chunk.custom_metadata.year"
  ///   conditions = [{int_value = 2020, operation = LESS_EQUAL}]}]
  ///
  /// Note: "AND"s for the same key are only supported for numeric values. String
  /// values only support "OR"s for the same key.
  final List<MetadataFilter> metadataFilters;

  /// Optional. The maximum number of `Chunk`s to return.
  /// The service may return fewer `Chunk`s.
  ///
  /// If unspecified, at most 10 `Chunk`s will be returned.
  /// The maximum specified result count is 100.
  final int resultsCount;

  QueryCorpusRequest({
    required this.name,
    required this.query,
    this.metadataFilters = const [],
    this.resultsCount = 0,
  }) : super(fullyQualifiedName);

  factory QueryCorpusRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return QueryCorpusRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
      query: switch (json['query']) {
        null => '',
        Object $1 => decodeString($1),
      },
      metadataFilters: switch (json['metadataFilters']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) MetadataFilter.fromJson(i)],
        _ => throw const FormatException('"metadataFilters" is not a list'),
      },
      resultsCount: switch (json['resultsCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
    );
  }

  @override
  Object toJson() => {
    'name': name,
    'query': query,
    if (metadataFilters.isNotDefault)
      'metadataFilters': encodeList(metadataFilters),
    if (resultsCount.isNotDefault) 'resultsCount': resultsCount,
  };

  @override
  String toString() {
    final contents = [
      'name=$name',
      'query=$query',
      'resultsCount=$resultsCount',
    ].join(',');
    return 'QueryCorpusRequest($contents)';
  }
}

/// Response from `QueryCorpus` containing a list of relevant chunks.
final class QueryCorpusResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.QueryCorpusResponse';

  /// The relevant chunks.
  final List<RelevantChunk> relevantChunks;

  QueryCorpusResponse({this.relevantChunks = const []})
    : super(fullyQualifiedName);

  factory QueryCorpusResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return QueryCorpusResponse(
      relevantChunks: switch (json['relevantChunks']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) RelevantChunk.fromJson(i)],
        _ => throw const FormatException('"relevantChunks" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (relevantChunks.isNotDefault)
      'relevantChunks': encodeList(relevantChunks),
  };

  @override
  String toString() => 'QueryCorpusResponse()';
}

/// The information for a chunk relevant to a query.
final class RelevantChunk extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.RelevantChunk';

  /// `Chunk` relevance to the query.
  final double chunkRelevanceScore;

  /// `Chunk` associated with the query.
  final Chunk? chunk;

  /// `Document` associated with the chunk.
  final Document? document;

  RelevantChunk({this.chunkRelevanceScore = 0, this.chunk, this.document})
    : super(fullyQualifiedName);

  factory RelevantChunk.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return RelevantChunk(
      chunkRelevanceScore: switch (json['chunkRelevanceScore']) {
        null => 0,
        Object $1 => decodeDouble($1),
      },
      chunk: switch (json['chunk']) {
        null => null,
        Object $1 => Chunk.fromJson($1),
      },
      document: switch (json['document']) {
        null => null,
        Object $1 => Document.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (chunkRelevanceScore.isNotDefault)
      'chunkRelevanceScore': encodeDouble(chunkRelevanceScore),
    if (chunk != null) 'chunk': chunk!.toJson(),
    if (document != null) 'document': document!.toJson(),
  };

  @override
  String toString() {
    final contents = ['chunkRelevanceScore=$chunkRelevanceScore'].join(',');
    return 'RelevantChunk($contents)';
  }
}

/// Request to create a `Document`.
final class CreateDocumentRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CreateDocumentRequest';

  /// Required. The name of the `Corpus` where this `Document` will be created.
  /// Example: `corpora/my-corpus-123`
  final String parent;

  /// Required. The `Document` to create.
  final Document? document;

  CreateDocumentRequest({required this.parent, required this.document})
    : super(fullyQualifiedName);

  factory CreateDocumentRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CreateDocumentRequest(
      parent: switch (json['parent']) {
        null => '',
        Object $1 => decodeString($1),
      },
      document: switch (json['document']) {
        null => null,
        Object $1 => Document.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    'parent': parent,
    if (document != null) 'document': document!.toJson(),
  };

  @override
  String toString() {
    final contents = ['parent=$parent'].join(',');
    return 'CreateDocumentRequest($contents)';
  }
}

/// Request for getting information about a specific `Document`.
final class GetDocumentRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GetDocumentRequest';

  /// Required. The name of the `Document` to retrieve.
  /// Example: `corpora/my-corpus-123/documents/the-doc-abc`
  final String name;

  GetDocumentRequest({required this.name}) : super(fullyQualifiedName);

  factory GetDocumentRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GetDocumentRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {'name': name};

  @override
  String toString() {
    final contents = ['name=$name'].join(',');
    return 'GetDocumentRequest($contents)';
  }
}

/// Request to update a `Document`.
final class UpdateDocumentRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.UpdateDocumentRequest';

  /// Required. The `Document` to update.
  final Document? document;

  /// Required. The list of fields to update.
  /// Currently, this only supports updating `display_name` and
  /// `custom_metadata`.
  final FieldMask? updateMask;

  UpdateDocumentRequest({required this.document, required this.updateMask})
    : super(fullyQualifiedName);

  factory UpdateDocumentRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return UpdateDocumentRequest(
      document: switch (json['document']) {
        null => null,
        Object $1 => Document.fromJson($1),
      },
      updateMask: switch (json['updateMask']) {
        null => null,
        Object $1 => FieldMask.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (document != null) 'document': document!.toJson(),
    if (updateMask != null) 'updateMask': updateMask!.toJson(),
  };

  @override
  String toString() => 'UpdateDocumentRequest()';
}

/// Request to delete a `Document`.
final class DeleteDocumentRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.DeleteDocumentRequest';

  /// Required. The resource name of the `Document` to delete.
  /// Example: `corpora/my-corpus-123/documents/the-doc-abc`
  final String name;

  /// Optional. If set to true, any `Chunk`s and objects related to this
  /// `Document` will also be deleted.
  ///
  /// If false (the default), a `FAILED_PRECONDITION` error will be returned if
  /// `Document` contains any `Chunk`s.
  final bool force;

  DeleteDocumentRequest({required this.name, this.force = false})
    : super(fullyQualifiedName);

  factory DeleteDocumentRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return DeleteDocumentRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
      force: switch (json['force']) {
        null => false,
        Object $1 => decodeBool($1),
      },
    );
  }

  @override
  Object toJson() => {'name': name, if (force.isNotDefault) 'force': force};

  @override
  String toString() {
    final contents = ['name=$name', 'force=$force'].join(',');
    return 'DeleteDocumentRequest($contents)';
  }
}

/// Request for listing `Document`s.
final class ListDocumentsRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ListDocumentsRequest';

  /// Required. The name of the `Corpus` containing `Document`s.
  /// Example: `corpora/my-corpus-123`
  final String parent;

  /// Optional. The maximum number of `Document`s to return (per page).
  /// The service may return fewer `Document`s.
  ///
  /// If unspecified, at most 10 `Document`s will be returned.
  /// The maximum size limit is 20 `Document`s per page.
  final int pageSize;

  /// Optional. A page token, received from a previous `ListDocuments` call.
  ///
  /// Provide the `next_page_token` returned in the response as an argument to
  /// the next request to retrieve the next page.
  ///
  /// When paginating, all other parameters provided to `ListDocuments`
  /// must match the call that provided the page token.
  final String pageToken;

  ListDocumentsRequest({
    required this.parent,
    this.pageSize = 0,
    this.pageToken = '',
  }) : super(fullyQualifiedName);

  factory ListDocumentsRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ListDocumentsRequest(
      parent: switch (json['parent']) {
        null => '',
        Object $1 => decodeString($1),
      },
      pageSize: switch (json['pageSize']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      pageToken: switch (json['pageToken']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    'parent': parent,
    if (pageSize.isNotDefault) 'pageSize': pageSize,
    if (pageToken.isNotDefault) 'pageToken': pageToken,
  };

  @override
  String toString() {
    final contents = [
      'parent=$parent',
      'pageSize=$pageSize',
      'pageToken=$pageToken',
    ].join(',');
    return 'ListDocumentsRequest($contents)';
  }
}

/// Response from `ListDocuments` containing a paginated list of `Document`s.
/// The `Document`s are sorted by ascending `document.create_time`.
final class ListDocumentsResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ListDocumentsResponse';

  /// The returned `Document`s.
  final List<Document> documents;

  /// A token, which can be sent as `page_token` to retrieve the next page.
  /// If this field is omitted, there are no more pages.
  final String nextPageToken;

  ListDocumentsResponse({this.documents = const [], this.nextPageToken = ''})
    : super(fullyQualifiedName);

  factory ListDocumentsResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ListDocumentsResponse(
      documents: switch (json['documents']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Document.fromJson(i)],
        _ => throw const FormatException('"documents" is not a list'),
      },
      nextPageToken: switch (json['nextPageToken']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (documents.isNotDefault) 'documents': encodeList(documents),
    if (nextPageToken.isNotDefault) 'nextPageToken': nextPageToken,
  };

  @override
  String toString() {
    final contents = ['nextPageToken=$nextPageToken'].join(',');
    return 'ListDocumentsResponse($contents)';
  }
}

/// Request for querying a `Document`.
final class QueryDocumentRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.QueryDocumentRequest';

  /// Required. The name of the `Document` to query.
  /// Example: `corpora/my-corpus-123/documents/the-doc-abc`
  final String name;

  /// Required. Query string to perform semantic search.
  final String query;

  /// Optional. The maximum number of `Chunk`s to return.
  /// The service may return fewer `Chunk`s.
  ///
  /// If unspecified, at most 10 `Chunk`s will be returned.
  /// The maximum specified result count is 100.
  final int resultsCount;

  /// Optional. Filter for `Chunk` metadata. Each `MetadataFilter` object should
  /// correspond to a unique key. Multiple `MetadataFilter` objects are joined by
  /// logical "AND"s.
  ///
  /// Note: `Document`-level filtering is not supported for this request because
  /// a `Document` name is already specified.
  ///
  /// Example query:
  /// (year >= 2020 OR year < 2010) AND (genre = drama OR genre = action)
  ///
  /// `MetadataFilter` object list:
  ///  metadata_filters = [
  ///  {key = "chunk.custom_metadata.year"
  ///   conditions = [{int_value = 2020, operation = GREATER_EQUAL},
  ///                 {int_value = 2010, operation = LESS}},
  ///  {key = "chunk.custom_metadata.genre"
  ///   conditions = [{string_value = "drama", operation = EQUAL},
  ///                 {string_value = "action", operation = EQUAL}}]
  ///
  /// Example query for a numeric range of values:
  /// (year > 2015 AND year <= 2020)
  ///
  /// `MetadataFilter` object list:
  ///  metadata_filters = [
  ///  {key = "chunk.custom_metadata.year"
  ///   conditions = [{int_value = 2015, operation = GREATER}]},
  ///  {key = "chunk.custom_metadata.year"
  ///   conditions = [{int_value = 2020, operation = LESS_EQUAL}]}]
  ///
  /// Note: "AND"s for the same key are only supported for numeric values. String
  /// values only support "OR"s for the same key.
  final List<MetadataFilter> metadataFilters;

  QueryDocumentRequest({
    required this.name,
    required this.query,
    this.resultsCount = 0,
    this.metadataFilters = const [],
  }) : super(fullyQualifiedName);

  factory QueryDocumentRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return QueryDocumentRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
      query: switch (json['query']) {
        null => '',
        Object $1 => decodeString($1),
      },
      resultsCount: switch (json['resultsCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      metadataFilters: switch (json['metadataFilters']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) MetadataFilter.fromJson(i)],
        _ => throw const FormatException('"metadataFilters" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    'name': name,
    'query': query,
    if (resultsCount.isNotDefault) 'resultsCount': resultsCount,
    if (metadataFilters.isNotDefault)
      'metadataFilters': encodeList(metadataFilters),
  };

  @override
  String toString() {
    final contents = [
      'name=$name',
      'query=$query',
      'resultsCount=$resultsCount',
    ].join(',');
    return 'QueryDocumentRequest($contents)';
  }
}

/// Response from `QueryDocument` containing a list of relevant chunks.
final class QueryDocumentResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.QueryDocumentResponse';

  /// The returned relevant chunks.
  final List<RelevantChunk> relevantChunks;

  QueryDocumentResponse({this.relevantChunks = const []})
    : super(fullyQualifiedName);

  factory QueryDocumentResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return QueryDocumentResponse(
      relevantChunks: switch (json['relevantChunks']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) RelevantChunk.fromJson(i)],
        _ => throw const FormatException('"relevantChunks" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (relevantChunks.isNotDefault)
      'relevantChunks': encodeList(relevantChunks),
  };

  @override
  String toString() => 'QueryDocumentResponse()';
}

/// Request to create a `Chunk`.
final class CreateChunkRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CreateChunkRequest';

  /// Required. The name of the `Document` where this `Chunk` will be created.
  /// Example: `corpora/my-corpus-123/documents/the-doc-abc`
  final String parent;

  /// Required. The `Chunk` to create.
  final Chunk? chunk;

  CreateChunkRequest({required this.parent, required this.chunk})
    : super(fullyQualifiedName);

  factory CreateChunkRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CreateChunkRequest(
      parent: switch (json['parent']) {
        null => '',
        Object $1 => decodeString($1),
      },
      chunk: switch (json['chunk']) {
        null => null,
        Object $1 => Chunk.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    'parent': parent,
    if (chunk != null) 'chunk': chunk!.toJson(),
  };

  @override
  String toString() {
    final contents = ['parent=$parent'].join(',');
    return 'CreateChunkRequest($contents)';
  }
}

/// Request to batch create `Chunk`s.
final class BatchCreateChunksRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BatchCreateChunksRequest';

  /// Optional. The name of the `Document` where this batch of `Chunk`s will be
  /// created. The parent field in every `CreateChunkRequest` must match this
  /// value. Example: `corpora/my-corpus-123/documents/the-doc-abc`
  final String parent;

  /// Required. The request messages specifying the `Chunk`s to create.
  /// A maximum of 100 `Chunk`s can be created in a batch.
  final List<CreateChunkRequest> requests;

  BatchCreateChunksRequest({this.parent = '', required this.requests})
    : super(fullyQualifiedName);

  factory BatchCreateChunksRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BatchCreateChunksRequest(
      parent: switch (json['parent']) {
        null => '',
        Object $1 => decodeString($1),
      },
      requests: switch (json['requests']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) CreateChunkRequest.fromJson(i),
        ],
        _ => throw const FormatException('"requests" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (parent.isNotDefault) 'parent': parent,
    'requests': encodeList(requests),
  };

  @override
  String toString() {
    final contents = ['parent=$parent'].join(',');
    return 'BatchCreateChunksRequest($contents)';
  }
}

/// Response from `BatchCreateChunks` containing a list of created `Chunk`s.
final class BatchCreateChunksResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BatchCreateChunksResponse';

  /// `Chunk`s created.
  final List<Chunk> chunks;

  BatchCreateChunksResponse({this.chunks = const []})
    : super(fullyQualifiedName);

  factory BatchCreateChunksResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BatchCreateChunksResponse(
      chunks: switch (json['chunks']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Chunk.fromJson(i)],
        _ => throw const FormatException('"chunks" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {if (chunks.isNotDefault) 'chunks': encodeList(chunks)};

  @override
  String toString() => 'BatchCreateChunksResponse()';
}

/// Request for getting information about a specific `Chunk`.
final class GetChunkRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GetChunkRequest';

  /// Required. The name of the `Chunk` to retrieve.
  /// Example: `corpora/my-corpus-123/documents/the-doc-abc/chunks/some-chunk`
  final String name;

  GetChunkRequest({required this.name}) : super(fullyQualifiedName);

  factory GetChunkRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GetChunkRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {'name': name};

  @override
  String toString() {
    final contents = ['name=$name'].join(',');
    return 'GetChunkRequest($contents)';
  }
}

/// Request to update a `Chunk`.
final class UpdateChunkRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.UpdateChunkRequest';

  /// Required. The `Chunk` to update.
  final Chunk? chunk;

  /// Required. The list of fields to update.
  /// Currently, this only supports updating `custom_metadata` and `data`.
  final FieldMask? updateMask;

  UpdateChunkRequest({required this.chunk, required this.updateMask})
    : super(fullyQualifiedName);

  factory UpdateChunkRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return UpdateChunkRequest(
      chunk: switch (json['chunk']) {
        null => null,
        Object $1 => Chunk.fromJson($1),
      },
      updateMask: switch (json['updateMask']) {
        null => null,
        Object $1 => FieldMask.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (chunk != null) 'chunk': chunk!.toJson(),
    if (updateMask != null) 'updateMask': updateMask!.toJson(),
  };

  @override
  String toString() => 'UpdateChunkRequest()';
}

/// Request to batch update `Chunk`s.
final class BatchUpdateChunksRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BatchUpdateChunksRequest';

  /// Optional. The name of the `Document` containing the `Chunk`s to update.
  /// The parent field in every `UpdateChunkRequest` must match this value.
  /// Example: `corpora/my-corpus-123/documents/the-doc-abc`
  final String parent;

  /// Required. The request messages specifying the `Chunk`s to update.
  /// A maximum of 100 `Chunk`s can be updated in a batch.
  final List<UpdateChunkRequest> requests;

  BatchUpdateChunksRequest({this.parent = '', required this.requests})
    : super(fullyQualifiedName);

  factory BatchUpdateChunksRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BatchUpdateChunksRequest(
      parent: switch (json['parent']) {
        null => '',
        Object $1 => decodeString($1),
      },
      requests: switch (json['requests']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) UpdateChunkRequest.fromJson(i),
        ],
        _ => throw const FormatException('"requests" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (parent.isNotDefault) 'parent': parent,
    'requests': encodeList(requests),
  };

  @override
  String toString() {
    final contents = ['parent=$parent'].join(',');
    return 'BatchUpdateChunksRequest($contents)';
  }
}

/// Response from `BatchUpdateChunks` containing a list of updated `Chunk`s.
final class BatchUpdateChunksResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BatchUpdateChunksResponse';

  /// `Chunk`s updated.
  final List<Chunk> chunks;

  BatchUpdateChunksResponse({this.chunks = const []})
    : super(fullyQualifiedName);

  factory BatchUpdateChunksResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BatchUpdateChunksResponse(
      chunks: switch (json['chunks']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Chunk.fromJson(i)],
        _ => throw const FormatException('"chunks" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {if (chunks.isNotDefault) 'chunks': encodeList(chunks)};

  @override
  String toString() => 'BatchUpdateChunksResponse()';
}

/// Request to delete a `Chunk`.
final class DeleteChunkRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.DeleteChunkRequest';

  /// Required. The resource name of the `Chunk` to delete.
  /// Example: `corpora/my-corpus-123/documents/the-doc-abc/chunks/some-chunk`
  final String name;

  DeleteChunkRequest({required this.name}) : super(fullyQualifiedName);

  factory DeleteChunkRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return DeleteChunkRequest(
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {'name': name};

  @override
  String toString() {
    final contents = ['name=$name'].join(',');
    return 'DeleteChunkRequest($contents)';
  }
}

/// Request to batch delete `Chunk`s.
final class BatchDeleteChunksRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BatchDeleteChunksRequest';

  /// Optional. The name of the `Document` containing the `Chunk`s to delete.
  /// The parent field in every `DeleteChunkRequest` must match this value.
  /// Example: `corpora/my-corpus-123/documents/the-doc-abc`
  final String parent;

  /// Required. The request messages specifying the `Chunk`s to delete.
  final List<DeleteChunkRequest> requests;

  BatchDeleteChunksRequest({this.parent = '', required this.requests})
    : super(fullyQualifiedName);

  factory BatchDeleteChunksRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BatchDeleteChunksRequest(
      parent: switch (json['parent']) {
        null => '',
        Object $1 => decodeString($1),
      },
      requests: switch (json['requests']) {
        null => [],
        List<Object?> $1 => [
          for (final i in $1) DeleteChunkRequest.fromJson(i),
        ],
        _ => throw const FormatException('"requests" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (parent.isNotDefault) 'parent': parent,
    'requests': encodeList(requests),
  };

  @override
  String toString() {
    final contents = ['parent=$parent'].join(',');
    return 'BatchDeleteChunksRequest($contents)';
  }
}

/// Request for listing `Chunk`s.
final class ListChunksRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ListChunksRequest';

  /// Required. The name of the `Document` containing `Chunk`s.
  /// Example: `corpora/my-corpus-123/documents/the-doc-abc`
  final String parent;

  /// Optional. The maximum number of `Chunk`s to return (per page).
  /// The service may return fewer `Chunk`s.
  ///
  /// If unspecified, at most 10 `Chunk`s will be returned.
  /// The maximum size limit is 100 `Chunk`s per page.
  final int pageSize;

  /// Optional. A page token, received from a previous `ListChunks` call.
  ///
  /// Provide the `next_page_token` returned in the response as an argument to
  /// the next request to retrieve the next page.
  ///
  /// When paginating, all other parameters provided to `ListChunks`
  /// must match the call that provided the page token.
  final String pageToken;

  ListChunksRequest({
    required this.parent,
    this.pageSize = 0,
    this.pageToken = '',
  }) : super(fullyQualifiedName);

  factory ListChunksRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ListChunksRequest(
      parent: switch (json['parent']) {
        null => '',
        Object $1 => decodeString($1),
      },
      pageSize: switch (json['pageSize']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      pageToken: switch (json['pageToken']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    'parent': parent,
    if (pageSize.isNotDefault) 'pageSize': pageSize,
    if (pageToken.isNotDefault) 'pageToken': pageToken,
  };

  @override
  String toString() {
    final contents = [
      'parent=$parent',
      'pageSize=$pageSize',
      'pageToken=$pageToken',
    ].join(',');
    return 'ListChunksRequest($contents)';
  }
}

/// Response from `ListChunks` containing a paginated list of `Chunk`s.
/// The `Chunk`s are sorted by ascending `chunk.create_time`.
final class ListChunksResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ListChunksResponse';

  /// The returned `Chunk`s.
  final List<Chunk> chunks;

  /// A token, which can be sent as `page_token` to retrieve the next page.
  /// If this field is omitted, there are no more pages.
  final String nextPageToken;

  ListChunksResponse({this.chunks = const [], this.nextPageToken = ''})
    : super(fullyQualifiedName);

  factory ListChunksResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ListChunksResponse(
      chunks: switch (json['chunks']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Chunk.fromJson(i)],
        _ => throw const FormatException('"chunks" is not a list'),
      },
      nextPageToken: switch (json['nextPageToken']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (chunks.isNotDefault) 'chunks': encodeList(chunks),
    if (nextPageToken.isNotDefault) 'nextPageToken': nextPageToken,
  };

  @override
  String toString() {
    final contents = ['nextPageToken=$nextPageToken'].join(',');
    return 'ListChunksResponse($contents)';
  }
}

/// Content filtering metadata associated with processing a single request.
///
/// ContentFilter contains a reason and an optional supporting string. The reason
/// may be unspecified.
final class ContentFilter extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.ContentFilter';

  /// The reason content was blocked during request processing.
  final ContentFilter_BlockedReason reason;

  /// A string that describes the filtering behavior in more detail.
  final String? message;

  ContentFilter({
    this.reason = ContentFilter_BlockedReason.$default,
    this.message,
  }) : super(fullyQualifiedName);

  factory ContentFilter.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return ContentFilter(
      reason: switch (json['reason']) {
        null => ContentFilter_BlockedReason.$default,
        Object $1 => ContentFilter_BlockedReason.fromJson($1),
      },
      message: switch (json['message']) {
        null => null,
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (reason.isNotDefault) 'reason': reason.toJson(),
    if (message != null) 'message': message,
  };

  @override
  String toString() {
    final contents = [
      'reason=$reason',
      if (message != null) 'message=$message',
    ].join(',');
    return 'ContentFilter($contents)';
  }
}

/// A list of reasons why content may have been blocked.
final class ContentFilter_BlockedReason extends ProtoEnum {
  /// A blocked reason was not specified.
  static const blockedReasonUnspecified = ContentFilter_BlockedReason(
    'BLOCKED_REASON_UNSPECIFIED',
  );

  /// Content was blocked by safety settings.
  static const safety = ContentFilter_BlockedReason('SAFETY');

  /// Content was blocked, but the reason is uncategorized.
  static const other = ContentFilter_BlockedReason('OTHER');

  /// The default value for [ContentFilter_BlockedReason].
  static const $default = blockedReasonUnspecified;

  const ContentFilter_BlockedReason(super.value);

  factory ContentFilter_BlockedReason.fromJson(Object? json) =>
      ContentFilter_BlockedReason(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'BlockedReason.$value';
}

/// Safety feedback for an entire request.
///
/// This field is populated if content in the input and/or response is blocked
/// due to safety settings. SafetyFeedback may not exist for every HarmCategory.
/// Each SafetyFeedback will return the safety settings used by the request as
/// well as the lowest HarmProbability that should be allowed in order to return
/// a result.
final class SafetyFeedback extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.SafetyFeedback';

  /// Safety rating evaluated from content.
  final SafetyRating? rating;

  /// Safety settings applied to the request.
  final SafetySetting? setting;

  SafetyFeedback({this.rating, this.setting}) : super(fullyQualifiedName);

  factory SafetyFeedback.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return SafetyFeedback(
      rating: switch (json['rating']) {
        null => null,
        Object $1 => SafetyRating.fromJson($1),
      },
      setting: switch (json['setting']) {
        null => null,
        Object $1 => SafetySetting.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (rating != null) 'rating': rating!.toJson(),
    if (setting != null) 'setting': setting!.toJson(),
  };

  @override
  String toString() => 'SafetyFeedback()';
}

/// Safety rating for a piece of content.
///
/// The safety rating contains the category of harm and the
/// harm probability level in that category for a piece of content.
/// Content is classified for safety across a number of
/// harm categories and the probability of the harm classification is included
/// here.
final class SafetyRating extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.SafetyRating';

  /// Required. The category for this rating.
  final HarmCategory category;

  /// Required. The probability of harm for this content.
  final SafetyRating_HarmProbability probability;

  /// Was this content blocked because of this rating?
  final bool blocked;

  SafetyRating({
    required this.category,
    required this.probability,
    this.blocked = false,
  }) : super(fullyQualifiedName);

  factory SafetyRating.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return SafetyRating(
      category: switch (json['category']) {
        null => HarmCategory.$default,
        Object $1 => HarmCategory.fromJson($1),
      },
      probability: switch (json['probability']) {
        null => SafetyRating_HarmProbability.$default,
        Object $1 => SafetyRating_HarmProbability.fromJson($1),
      },
      blocked: switch (json['blocked']) {
        null => false,
        Object $1 => decodeBool($1),
      },
    );
  }

  @override
  Object toJson() => {
    'category': category.toJson(),
    'probability': probability.toJson(),
    if (blocked.isNotDefault) 'blocked': blocked,
  };

  @override
  String toString() {
    final contents = [
      'category=$category',
      'probability=$probability',
      'blocked=$blocked',
    ].join(',');
    return 'SafetyRating($contents)';
  }
}

/// The probability that a piece of content is harmful.
///
/// The classification system gives the probability of the content being
/// unsafe. This does not indicate the severity of harm for a piece of content.
final class SafetyRating_HarmProbability extends ProtoEnum {
  /// Probability is unspecified.
  static const harmProbabilityUnspecified = SafetyRating_HarmProbability(
    'HARM_PROBABILITY_UNSPECIFIED',
  );

  /// Content has a negligible chance of being unsafe.
  static const negligible = SafetyRating_HarmProbability('NEGLIGIBLE');

  /// Content has a low chance of being unsafe.
  static const low = SafetyRating_HarmProbability('LOW');

  /// Content has a medium chance of being unsafe.
  static const medium = SafetyRating_HarmProbability('MEDIUM');

  /// Content has a high chance of being unsafe.
  static const high = SafetyRating_HarmProbability('HIGH');

  /// The default value for [SafetyRating_HarmProbability].
  static const $default = harmProbabilityUnspecified;

  const SafetyRating_HarmProbability(super.value);

  factory SafetyRating_HarmProbability.fromJson(Object? json) =>
      SafetyRating_HarmProbability(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'HarmProbability.$value';
}

/// Safety setting, affecting the safety-blocking behavior.
///
/// Passing a safety setting for a category changes the allowed probability that
/// content is blocked.
final class SafetySetting extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.SafetySetting';

  /// Required. The category for this setting.
  final HarmCategory category;

  /// Required. Controls the probability threshold at which harm is blocked.
  final SafetySetting_HarmBlockThreshold threshold;

  SafetySetting({required this.category, required this.threshold})
    : super(fullyQualifiedName);

  factory SafetySetting.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return SafetySetting(
      category: switch (json['category']) {
        null => HarmCategory.$default,
        Object $1 => HarmCategory.fromJson($1),
      },
      threshold: switch (json['threshold']) {
        null => SafetySetting_HarmBlockThreshold.$default,
        Object $1 => SafetySetting_HarmBlockThreshold.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    'category': category.toJson(),
    'threshold': threshold.toJson(),
  };

  @override
  String toString() {
    final contents = ['category=$category', 'threshold=$threshold'].join(',');
    return 'SafetySetting($contents)';
  }
}

/// Block at and beyond a specified harm probability.
final class SafetySetting_HarmBlockThreshold extends ProtoEnum {
  /// Threshold is unspecified.
  static const harmBlockThresholdUnspecified = SafetySetting_HarmBlockThreshold(
    'HARM_BLOCK_THRESHOLD_UNSPECIFIED',
  );

  /// Content with NEGLIGIBLE will be allowed.
  static const blockLowAndAbove = SafetySetting_HarmBlockThreshold(
    'BLOCK_LOW_AND_ABOVE',
  );

  /// Content with NEGLIGIBLE and LOW will be allowed.
  static const blockMediumAndAbove = SafetySetting_HarmBlockThreshold(
    'BLOCK_MEDIUM_AND_ABOVE',
  );

  /// Content with NEGLIGIBLE, LOW, and MEDIUM will be allowed.
  static const blockOnlyHigh = SafetySetting_HarmBlockThreshold(
    'BLOCK_ONLY_HIGH',
  );

  /// All content will be allowed.
  static const blockNone = SafetySetting_HarmBlockThreshold('BLOCK_NONE');

  /// Turn off the safety filter.
  static const off = SafetySetting_HarmBlockThreshold('OFF');

  /// The default value for [SafetySetting_HarmBlockThreshold].
  static const $default = harmBlockThresholdUnspecified;

  const SafetySetting_HarmBlockThreshold(super.value);

  factory SafetySetting_HarmBlockThreshold.fromJson(Object? json) =>
      SafetySetting_HarmBlockThreshold(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'HarmBlockThreshold.$value';
}

/// Request to generate a text completion response from the model.
final class GenerateTextRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GenerateTextRequest';

  /// Required. The name of the `Model` or `TunedModel` to use for generating the
  /// completion.
  /// Examples:
  ///  models/text-bison-001
  ///  tunedModels/sentence-translator-u3b7m
  final String model;

  /// Required. The free-form input text given to the model as a prompt.
  ///
  /// Given a prompt, the model will generate a TextCompletion response it
  /// predicts as the completion of the input text.
  final TextPrompt? prompt;

  /// Optional. Controls the randomness of the output.
  /// Note: The default value varies by model, see the `Model.temperature`
  /// attribute of the `Model` returned the `getModel` function.
  ///
  /// Values can range from [0.0,1.0],
  /// inclusive. A value closer to 1.0 will produce responses that are more
  /// varied and creative, while a value closer to 0.0 will typically result in
  /// more straightforward responses from the model.
  final double? temperature;

  /// Optional. Number of generated responses to return.
  ///
  /// This value must be between [1, 8], inclusive. If unset, this will default
  /// to 1.
  final int? candidateCount;

  /// Optional. The maximum number of tokens to include in a candidate.
  ///
  /// If unset, this will default to output_token_limit specified in the `Model`
  /// specification.
  final int? maxOutputTokens;

  /// Optional. The maximum cumulative probability of tokens to consider when
  /// sampling.
  ///
  /// The model uses combined Top-k and nucleus sampling.
  ///
  /// Tokens are sorted based on their assigned probabilities so that only the
  /// most likely tokens are considered. Top-k sampling directly limits the
  /// maximum number of tokens to consider, while Nucleus sampling limits number
  /// of tokens based on the cumulative probability.
  ///
  /// Note: The default value varies by model, see the `Model.top_p`
  /// attribute of the `Model` returned the `getModel` function.
  final double? topP;

  /// Optional. The maximum number of tokens to consider when sampling.
  ///
  /// The model uses combined Top-k and nucleus sampling.
  ///
  /// Top-k sampling considers the set of `top_k` most probable tokens.
  /// Defaults to 40.
  ///
  /// Note: The default value varies by model, see the `Model.top_k`
  /// attribute of the `Model` returned the `getModel` function.
  final int? topK;

  /// Optional. A list of unique `SafetySetting` instances for blocking unsafe
  /// content.
  ///
  /// that will be enforced on the `GenerateTextRequest.prompt` and
  /// `GenerateTextResponse.candidates`. There should not be more than one
  /// setting for each `SafetyCategory` type. The API will block any prompts and
  /// responses that fail to meet the thresholds set by these settings. This list
  /// overrides the default settings for each `SafetyCategory` specified in the
  /// safety_settings. If there is no `SafetySetting` for a given
  /// `SafetyCategory` provided in the list, the API will use the default safety
  /// setting for that category. Harm categories HARM_CATEGORY_DEROGATORY,
  /// HARM_CATEGORY_TOXICITY, HARM_CATEGORY_VIOLENCE, HARM_CATEGORY_SEXUAL,
  /// HARM_CATEGORY_MEDICAL, HARM_CATEGORY_DANGEROUS are supported in text
  /// service.
  final List<SafetySetting> safetySettings;

  /// The set of character sequences (up to 5) that will stop output generation.
  /// If specified, the API will stop at the first appearance of a stop
  /// sequence. The stop sequence will not be included as part of the response.
  final List<String> stopSequences;

  GenerateTextRequest({
    required this.model,
    required this.prompt,
    this.temperature,
    this.candidateCount,
    this.maxOutputTokens,
    this.topP,
    this.topK,
    this.safetySettings = const [],
    this.stopSequences = const [],
  }) : super(fullyQualifiedName);

  factory GenerateTextRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GenerateTextRequest(
      model: switch (json['model']) {
        null => '',
        Object $1 => decodeString($1),
      },
      prompt: switch (json['prompt']) {
        null => null,
        Object $1 => TextPrompt.fromJson($1),
      },
      temperature: switch (json['temperature']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      candidateCount: switch (json['candidateCount']) {
        null => null,
        Object $1 => decodeInt($1),
      },
      maxOutputTokens: switch (json['maxOutputTokens']) {
        null => null,
        Object $1 => decodeInt($1),
      },
      topP: switch (json['topP']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      topK: switch (json['topK']) {
        null => null,
        Object $1 => decodeInt($1),
      },
      safetySettings: switch (json['safetySettings']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) SafetySetting.fromJson(i)],
        _ => throw const FormatException('"safetySettings" is not a list'),
      },
      stopSequences: switch (json['stopSequences']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) decodeString(i)],
        _ => throw const FormatException('"stopSequences" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    'model': model,
    if (prompt != null) 'prompt': prompt!.toJson(),
    if (temperature != null) 'temperature': encodeDouble(temperature),
    if (candidateCount != null) 'candidateCount': candidateCount,
    if (maxOutputTokens != null) 'maxOutputTokens': maxOutputTokens,
    if (topP != null) 'topP': encodeDouble(topP),
    if (topK != null) 'topK': topK,
    if (safetySettings.isNotDefault)
      'safetySettings': encodeList(safetySettings),
    if (stopSequences.isNotDefault) 'stopSequences': stopSequences,
  };

  @override
  String toString() {
    final contents = [
      'model=$model',
      if (temperature != null) 'temperature=$temperature',
      if (candidateCount != null) 'candidateCount=$candidateCount',
      if (maxOutputTokens != null) 'maxOutputTokens=$maxOutputTokens',
      if (topP != null) 'topP=$topP',
      if (topK != null) 'topK=$topK',
    ].join(',');
    return 'GenerateTextRequest($contents)';
  }
}

/// The response from the model, including candidate completions.
final class GenerateTextResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.GenerateTextResponse';

  /// Candidate responses from the model.
  final List<TextCompletion> candidates;

  /// A set of content filtering metadata for the prompt and response
  /// text.
  ///
  /// This indicates which `SafetyCategory`(s) blocked a
  /// candidate from this response, the lowest `HarmProbability`
  /// that triggered a block, and the HarmThreshold setting for that category.
  /// This indicates the smallest change to the `SafetySettings` that would be
  /// necessary to unblock at least 1 response.
  ///
  /// The blocking is configured by the `SafetySettings` in the request (or the
  /// default `SafetySettings` of the API).
  final List<ContentFilter> filters;

  /// Returns any safety feedback related to content filtering.
  final List<SafetyFeedback> safetyFeedback;

  GenerateTextResponse({
    this.candidates = const [],
    this.filters = const [],
    this.safetyFeedback = const [],
  }) : super(fullyQualifiedName);

  factory GenerateTextResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return GenerateTextResponse(
      candidates: switch (json['candidates']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) TextCompletion.fromJson(i)],
        _ => throw const FormatException('"candidates" is not a list'),
      },
      filters: switch (json['filters']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) ContentFilter.fromJson(i)],
        _ => throw const FormatException('"filters" is not a list'),
      },
      safetyFeedback: switch (json['safetyFeedback']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) SafetyFeedback.fromJson(i)],
        _ => throw const FormatException('"safetyFeedback" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (candidates.isNotDefault) 'candidates': encodeList(candidates),
    if (filters.isNotDefault) 'filters': encodeList(filters),
    if (safetyFeedback.isNotDefault)
      'safetyFeedback': encodeList(safetyFeedback),
  };

  @override
  String toString() => 'GenerateTextResponse()';
}

/// Text given to the model as a prompt.
///
/// The Model will use this TextPrompt to Generate a text completion.
final class TextPrompt extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.TextPrompt';

  /// Required. The prompt text.
  final String text;

  TextPrompt({required this.text}) : super(fullyQualifiedName);

  factory TextPrompt.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return TextPrompt(
      text: switch (json['text']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {'text': text};

  @override
  String toString() {
    final contents = ['text=$text'].join(',');
    return 'TextPrompt($contents)';
  }
}

/// Output text returned from a model.
final class TextCompletion extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.TextCompletion';

  /// Output only. The generated text returned from the model.
  final String output;

  /// Ratings for the safety of a response.
  ///
  /// There is at most one rating per category.
  final List<SafetyRating> safetyRatings;

  /// Output only. Citation information for model-generated `output` in this
  /// `TextCompletion`.
  ///
  /// This field may be populated with attribution information for any text
  /// included in the `output`.
  final CitationMetadata? citationMetadata;

  TextCompletion({
    this.output = '',
    this.safetyRatings = const [],
    this.citationMetadata,
  }) : super(fullyQualifiedName);

  factory TextCompletion.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return TextCompletion(
      output: switch (json['output']) {
        null => '',
        Object $1 => decodeString($1),
      },
      safetyRatings: switch (json['safetyRatings']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) SafetyRating.fromJson(i)],
        _ => throw const FormatException('"safetyRatings" is not a list'),
      },
      citationMetadata: switch (json['citationMetadata']) {
        null => null,
        Object $1 => CitationMetadata.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (output.isNotDefault) 'output': output,
    if (safetyRatings.isNotDefault) 'safetyRatings': encodeList(safetyRatings),
    if (citationMetadata != null)
      'citationMetadata': citationMetadata!.toJson(),
  };

  @override
  String toString() {
    final contents = ['output=$output'].join(',');
    return 'TextCompletion($contents)';
  }
}

/// Request to get a text embedding from the model.
final class EmbedTextRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.EmbedTextRequest';

  /// Required. The model name to use with the format model=models/{model}.
  final String model;

  /// Optional. The free-form input text that the model will turn into an
  /// embedding.
  final String text;

  EmbedTextRequest({required this.model, this.text = ''})
    : super(fullyQualifiedName);

  factory EmbedTextRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return EmbedTextRequest(
      model: switch (json['model']) {
        null => '',
        Object $1 => decodeString($1),
      },
      text: switch (json['text']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {'model': model, if (text.isNotDefault) 'text': text};

  @override
  String toString() {
    final contents = ['model=$model', 'text=$text'].join(',');
    return 'EmbedTextRequest($contents)';
  }
}

/// The response to a EmbedTextRequest.
final class EmbedTextResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.EmbedTextResponse';

  /// Output only. The embedding generated from the input text.
  final Embedding? embedding;

  EmbedTextResponse({this.embedding}) : super(fullyQualifiedName);

  factory EmbedTextResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return EmbedTextResponse(
      embedding: switch (json['embedding']) {
        null => null,
        Object $1 => Embedding.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {if (embedding != null) 'embedding': embedding!.toJson()};

  @override
  String toString() => 'EmbedTextResponse()';
}

/// Batch request to get a text embedding from the model.
final class BatchEmbedTextRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BatchEmbedTextRequest';

  /// Required. The name of the `Model` to use for generating the embedding.
  /// Examples:
  ///  models/embedding-gecko-001
  final String model;

  /// Optional. The free-form input texts that the model will turn into an
  /// embedding. The current limit is 100 texts, over which an error will be
  /// thrown.
  final List<String> texts;

  /// Optional. Embed requests for the batch. Only one of `texts` or `requests`
  /// can be set.
  final List<EmbedTextRequest> requests;

  BatchEmbedTextRequest({
    required this.model,
    this.texts = const [],
    this.requests = const [],
  }) : super(fullyQualifiedName);

  factory BatchEmbedTextRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BatchEmbedTextRequest(
      model: switch (json['model']) {
        null => '',
        Object $1 => decodeString($1),
      },
      texts: switch (json['texts']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) decodeString(i)],
        _ => throw const FormatException('"texts" is not a list'),
      },
      requests: switch (json['requests']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) EmbedTextRequest.fromJson(i)],
        _ => throw const FormatException('"requests" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    'model': model,
    if (texts.isNotDefault) 'texts': texts,
    if (requests.isNotDefault) 'requests': encodeList(requests),
  };

  @override
  String toString() {
    final contents = ['model=$model'].join(',');
    return 'BatchEmbedTextRequest($contents)';
  }
}

/// The response to a EmbedTextRequest.
final class BatchEmbedTextResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.BatchEmbedTextResponse';

  /// Output only. The embeddings generated from the input text.
  final List<Embedding> embeddings;

  BatchEmbedTextResponse({this.embeddings = const []})
    : super(fullyQualifiedName);

  factory BatchEmbedTextResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return BatchEmbedTextResponse(
      embeddings: switch (json['embeddings']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) Embedding.fromJson(i)],
        _ => throw const FormatException('"embeddings" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (embeddings.isNotDefault) 'embeddings': encodeList(embeddings),
  };

  @override
  String toString() => 'BatchEmbedTextResponse()';
}

/// A list of floats representing the embedding.
final class Embedding extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Embedding';

  /// The embedding values.
  final List<double> value;

  Embedding({this.value = const []}) : super(fullyQualifiedName);

  factory Embedding.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Embedding(
      value: switch (json['value']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) decodeDouble(i)],
        _ => throw const FormatException('"value" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {if (value.isNotDefault) 'value': value};

  @override
  String toString() => 'Embedding()';
}

/// Counts the number of tokens in the `prompt` sent to a model.
///
/// Models may tokenize text differently, so each model may return a different
/// `token_count`.
final class CountTextTokensRequest extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CountTextTokensRequest';

  /// Required. The model's resource name. This serves as an ID for the Model to
  /// use.
  ///
  /// This name should match a model name returned by the `ListModels` method.
  ///
  /// Format: `models/{model}`
  final String model;

  /// Required. The free-form input text given to the model as a prompt.
  final TextPrompt? prompt;

  CountTextTokensRequest({required this.model, required this.prompt})
    : super(fullyQualifiedName);

  factory CountTextTokensRequest.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CountTextTokensRequest(
      model: switch (json['model']) {
        null => '',
        Object $1 => decodeString($1),
      },
      prompt: switch (json['prompt']) {
        null => null,
        Object $1 => TextPrompt.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    'model': model,
    if (prompt != null) 'prompt': prompt!.toJson(),
  };

  @override
  String toString() {
    final contents = ['model=$model'].join(',');
    return 'CountTextTokensRequest($contents)';
  }
}

/// A response from `CountTextTokens`.
///
/// It returns the model's `token_count` for the `prompt`.
final class CountTextTokensResponse extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.CountTextTokensResponse';

  /// The number of tokens that the `model` tokenizes the `prompt` into.
  ///
  /// Always non-negative.
  final int tokenCount;

  CountTextTokensResponse({this.tokenCount = 0}) : super(fullyQualifiedName);

  factory CountTextTokensResponse.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return CountTextTokensResponse(
      tokenCount: switch (json['tokenCount']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
    );
  }

  @override
  Object toJson() => {if (tokenCount.isNotDefault) 'tokenCount': tokenCount};

  @override
  String toString() {
    final contents = ['tokenCount=$tokenCount'].join(',');
    return 'CountTextTokensResponse($contents)';
  }
}

/// A fine-tuned model created using ModelService.CreateTunedModel.
final class TunedModel extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.TunedModel';

  /// Optional. TunedModel to use as the starting point for training the new
  /// model.
  final TunedModelSource? tunedModelSource;

  /// Immutable. The name of the `Model` to tune.
  /// Example: `models/gemini-1.5-flash-001`
  final String? baseModel;

  /// Output only. The tuned model name. A unique name will be generated on
  /// create. Example: `tunedModels/az2mb0bpw6i` If display_name is set on
  /// create, the id portion of the name will be set by concatenating the words
  /// of the display_name with hyphens and adding a random portion for
  /// uniqueness.
  ///
  /// Example:
  ///
  ///  * display_name = `Sentence Translator`
  ///  * name = `tunedModels/sentence-translator-u3b7m`
  final String name;

  /// Optional. The name to display for this model in user interfaces.
  /// The display name must be up to 40 characters including spaces.
  final String displayName;

  /// Optional. A short description of this model.
  final String description;

  /// Optional. Controls the randomness of the output.
  ///
  /// Values can range over `[0.0,1.0]`, inclusive. A value closer to `1.0` will
  /// produce responses that are more varied, while a value closer to `0.0` will
  /// typically result in less surprising responses from the model.
  ///
  /// This value specifies default to be the one used by the base model while
  /// creating the model.
  final double? temperature;

  /// Optional. For Nucleus sampling.
  ///
  /// Nucleus sampling considers the smallest set of tokens whose probability
  /// sum is at least `top_p`.
  ///
  /// This value specifies default to be the one used by the base model while
  /// creating the model.
  final double? topP;

  /// Optional. For Top-k sampling.
  ///
  /// Top-k sampling considers the set of `top_k` most probable tokens.
  /// This value specifies default to be used by the backend while making the
  /// call to the model.
  ///
  /// This value specifies default to be the one used by the base model while
  /// creating the model.
  final int? topK;

  /// Output only. The state of the tuned model.
  final TunedModel_State state;

  /// Output only. The timestamp when this model was created.
  final Timestamp? createTime;

  /// Output only. The timestamp when this model was updated.
  final Timestamp? updateTime;

  /// Required. The tuning task that creates the tuned model.
  final TuningTask? tuningTask;

  /// Optional. List of project numbers that have read access to the tuned model.
  final List<int> readerProjectNumbers;

  TunedModel({
    this.tunedModelSource,
    this.baseModel,
    this.name = '',
    this.displayName = '',
    this.description = '',
    this.temperature,
    this.topP,
    this.topK,
    this.state = TunedModel_State.$default,
    this.createTime,
    this.updateTime,
    required this.tuningTask,
    this.readerProjectNumbers = const [],
  }) : super(fullyQualifiedName);

  factory TunedModel.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return TunedModel(
      tunedModelSource: switch (json['tunedModelSource']) {
        null => null,
        Object $1 => TunedModelSource.fromJson($1),
      },
      baseModel: switch (json['baseModel']) {
        null => null,
        Object $1 => decodeString($1),
      },
      name: switch (json['name']) {
        null => '',
        Object $1 => decodeString($1),
      },
      displayName: switch (json['displayName']) {
        null => '',
        Object $1 => decodeString($1),
      },
      description: switch (json['description']) {
        null => '',
        Object $1 => decodeString($1),
      },
      temperature: switch (json['temperature']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      topP: switch (json['topP']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      topK: switch (json['topK']) {
        null => null,
        Object $1 => decodeInt($1),
      },
      state: switch (json['state']) {
        null => TunedModel_State.$default,
        Object $1 => TunedModel_State.fromJson($1),
      },
      createTime: switch (json['createTime']) {
        null => null,
        Object $1 => Timestamp.fromJson($1),
      },
      updateTime: switch (json['updateTime']) {
        null => null,
        Object $1 => Timestamp.fromJson($1),
      },
      tuningTask: switch (json['tuningTask']) {
        null => null,
        Object $1 => TuningTask.fromJson($1),
      },
      readerProjectNumbers: switch (json['readerProjectNumbers']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) decodeInt64(i)],
        _ => throw const FormatException(
          '"readerProjectNumbers" is not a list',
        ),
      },
    );
  }

  @override
  Object toJson() => {
    if (tunedModelSource != null)
      'tunedModelSource': tunedModelSource!.toJson(),
    if (baseModel != null) 'baseModel': baseModel,
    if (name.isNotDefault) 'name': name,
    if (displayName.isNotDefault) 'displayName': displayName,
    if (description.isNotDefault) 'description': description,
    if (temperature != null) 'temperature': encodeDouble(temperature),
    if (topP != null) 'topP': encodeDouble(topP),
    if (topK != null) 'topK': topK,
    if (state.isNotDefault) 'state': state.toJson(),
    if (createTime != null) 'createTime': createTime!.toJson(),
    if (updateTime != null) 'updateTime': updateTime!.toJson(),
    if (tuningTask != null) 'tuningTask': tuningTask!.toJson(),
    if (readerProjectNumbers.isNotDefault)
      'readerProjectNumbers': readerProjectNumbers,
  };

  @override
  String toString() {
    final contents = [
      if (baseModel != null) 'baseModel=$baseModel',
      'name=$name',
      'displayName=$displayName',
      'description=$description',
      if (temperature != null) 'temperature=$temperature',
      if (topP != null) 'topP=$topP',
      if (topK != null) 'topK=$topK',
      'state=$state',
    ].join(',');
    return 'TunedModel($contents)';
  }
}

/// The state of the tuned model.
final class TunedModel_State extends ProtoEnum {
  /// The default value. This value is unused.
  static const stateUnspecified = TunedModel_State('STATE_UNSPECIFIED');

  /// The model is being created.
  static const creating = TunedModel_State('CREATING');

  /// The model is ready to be used.
  static const active = TunedModel_State('ACTIVE');

  /// The model failed to be created.
  static const failed = TunedModel_State('FAILED');

  /// The default value for [TunedModel_State].
  static const $default = stateUnspecified;

  const TunedModel_State(super.value);

  factory TunedModel_State.fromJson(Object? json) =>
      TunedModel_State(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'State.$value';
}

/// Tuned model as a source for training a new model.
final class TunedModelSource extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.TunedModelSource';

  /// Immutable. The name of the `TunedModel` to use as the starting point for
  /// training the new model.
  /// Example: `tunedModels/my-tuned-model`
  final String tunedModel;

  /// Output only. The name of the base `Model` this `TunedModel` was tuned from.
  /// Example: `models/gemini-1.5-flash-001`
  final String baseModel;

  TunedModelSource({this.tunedModel = '', this.baseModel = ''})
    : super(fullyQualifiedName);

  factory TunedModelSource.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return TunedModelSource(
      tunedModel: switch (json['tunedModel']) {
        null => '',
        Object $1 => decodeString($1),
      },
      baseModel: switch (json['baseModel']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (tunedModel.isNotDefault) 'tunedModel': tunedModel,
    if (baseModel.isNotDefault) 'baseModel': baseModel,
  };

  @override
  String toString() {
    final contents = [
      'tunedModel=$tunedModel',
      'baseModel=$baseModel',
    ].join(',');
    return 'TunedModelSource($contents)';
  }
}

/// Tuning tasks that create tuned models.
final class TuningTask extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.TuningTask';

  /// Output only. The timestamp when tuning this model started.
  final Timestamp? startTime;

  /// Output only. The timestamp when tuning this model completed.
  final Timestamp? completeTime;

  /// Output only. Metrics collected during tuning.
  final List<TuningSnapshot> snapshots;

  /// Required. Input only. Immutable. The model training data.
  final Dataset? trainingData;

  /// Immutable. Hyperparameters controlling the tuning process. If not provided,
  /// default values will be used.
  final Hyperparameters? hyperparameters;

  TuningTask({
    this.startTime,
    this.completeTime,
    this.snapshots = const [],
    required this.trainingData,
    this.hyperparameters,
  }) : super(fullyQualifiedName);

  factory TuningTask.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return TuningTask(
      startTime: switch (json['startTime']) {
        null => null,
        Object $1 => Timestamp.fromJson($1),
      },
      completeTime: switch (json['completeTime']) {
        null => null,
        Object $1 => Timestamp.fromJson($1),
      },
      snapshots: switch (json['snapshots']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) TuningSnapshot.fromJson(i)],
        _ => throw const FormatException('"snapshots" is not a list'),
      },
      trainingData: switch (json['trainingData']) {
        null => null,
        Object $1 => Dataset.fromJson($1),
      },
      hyperparameters: switch (json['hyperparameters']) {
        null => null,
        Object $1 => Hyperparameters.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (startTime != null) 'startTime': startTime!.toJson(),
    if (completeTime != null) 'completeTime': completeTime!.toJson(),
    if (snapshots.isNotDefault) 'snapshots': encodeList(snapshots),
    if (trainingData != null) 'trainingData': trainingData!.toJson(),
    if (hyperparameters != null) 'hyperparameters': hyperparameters!.toJson(),
  };

  @override
  String toString() => 'TuningTask()';
}

/// Hyperparameters controlling the tuning process. Read more at
/// https://ai.google.dev/docs/model_tuning_guidance
final class Hyperparameters extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Hyperparameters';

  /// Optional. Immutable. The learning rate hyperparameter for tuning.
  /// If not set, a default of 0.001 or 0.0002 will be calculated based on the
  /// number of training examples.
  final double? learningRate;

  /// Optional. Immutable. The learning rate multiplier is used to calculate a
  /// final learning_rate based on the default (recommended) value. Actual
  /// learning rate := learning_rate_multiplier * default learning rate Default
  /// learning rate is dependent on base model and dataset size. If not set, a
  /// default of 1.0 will be used.
  final double? learningRateMultiplier;

  /// Immutable. The number of training epochs. An epoch is one pass through the
  /// training data. If not set, a default of 5 will be used.
  final int? epochCount;

  /// Immutable. The batch size hyperparameter for tuning.
  /// If not set, a default of 4 or 16 will be used based on the number of
  /// training examples.
  final int? batchSize;

  Hyperparameters({
    this.learningRate,
    this.learningRateMultiplier,
    this.epochCount,
    this.batchSize,
  }) : super(fullyQualifiedName);

  factory Hyperparameters.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Hyperparameters(
      learningRate: switch (json['learningRate']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      learningRateMultiplier: switch (json['learningRateMultiplier']) {
        null => null,
        Object $1 => decodeDouble($1),
      },
      epochCount: switch (json['epochCount']) {
        null => null,
        Object $1 => decodeInt($1),
      },
      batchSize: switch (json['batchSize']) {
        null => null,
        Object $1 => decodeInt($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (learningRate != null) 'learningRate': encodeDouble(learningRate),
    if (learningRateMultiplier != null)
      'learningRateMultiplier': encodeDouble(learningRateMultiplier),
    if (epochCount != null) 'epochCount': epochCount,
    if (batchSize != null) 'batchSize': batchSize,
  };

  @override
  String toString() {
    final contents = [
      if (learningRate != null) 'learningRate=$learningRate',
      if (learningRateMultiplier != null)
        'learningRateMultiplier=$learningRateMultiplier',
      if (epochCount != null) 'epochCount=$epochCount',
      if (batchSize != null) 'batchSize=$batchSize',
    ].join(',');
    return 'Hyperparameters($contents)';
  }
}

/// Dataset for training or validation.
final class Dataset extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.Dataset';

  /// Optional. Inline examples with simple input/output text.
  final TuningExamples? examples;

  Dataset({this.examples}) : super(fullyQualifiedName);

  factory Dataset.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return Dataset(
      examples: switch (json['examples']) {
        null => null,
        Object $1 => TuningExamples.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {if (examples != null) 'examples': examples!.toJson()};

  @override
  String toString() => 'Dataset()';
}

/// A set of tuning examples. Can be training or validation data.
final class TuningExamples extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.TuningExamples';

  /// The examples. Example input can be for text or discuss, but all examples
  /// in a set must be of the same type.
  final List<TuningExample> examples;

  TuningExamples({this.examples = const []}) : super(fullyQualifiedName);

  factory TuningExamples.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return TuningExamples(
      examples: switch (json['examples']) {
        null => [],
        List<Object?> $1 => [for (final i in $1) TuningExample.fromJson(i)],
        _ => throw const FormatException('"examples" is not a list'),
      },
    );
  }

  @override
  Object toJson() => {
    if (examples.isNotDefault) 'examples': encodeList(examples),
  };

  @override
  String toString() => 'TuningExamples()';
}

/// A single example for tuning.
final class TuningExample extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.TuningExample';

  /// Optional. Text model input.
  final String? textInput;

  /// Required. The expected model output.
  final String output;

  TuningExample({this.textInput, required this.output})
    : super(fullyQualifiedName);

  factory TuningExample.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return TuningExample(
      textInput: switch (json['textInput']) {
        null => null,
        Object $1 => decodeString($1),
      },
      output: switch (json['output']) {
        null => '',
        Object $1 => decodeString($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (textInput != null) 'textInput': textInput,
    'output': output,
  };

  @override
  String toString() {
    final contents = [
      if (textInput != null) 'textInput=$textInput',
      'output=$output',
    ].join(',');
    return 'TuningExample($contents)';
  }
}

/// Record for a single tuning step.
final class TuningSnapshot extends ProtoMessage {
  static const String fullyQualifiedName =
      'google.ai.generativelanguage.v1beta.TuningSnapshot';

  /// Output only. The tuning step.
  final int step;

  /// Output only. The epoch this step was part of.
  final int epoch;

  /// Output only. The mean loss of the training examples for this step.
  final double meanLoss;

  /// Output only. The timestamp when this metric was computed.
  final Timestamp? computeTime;

  TuningSnapshot({
    this.step = 0,
    this.epoch = 0,
    this.meanLoss = 0,
    this.computeTime,
  }) : super(fullyQualifiedName);

  factory TuningSnapshot.fromJson(Object? j) {
    final json = j as Map<String, Object?>;
    return TuningSnapshot(
      step: switch (json['step']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      epoch: switch (json['epoch']) {
        null => 0,
        Object $1 => decodeInt($1),
      },
      meanLoss: switch (json['meanLoss']) {
        null => 0,
        Object $1 => decodeDouble($1),
      },
      computeTime: switch (json['computeTime']) {
        null => null,
        Object $1 => Timestamp.fromJson($1),
      },
    );
  }

  @override
  Object toJson() => {
    if (step.isNotDefault) 'step': step,
    if (epoch.isNotDefault) 'epoch': epoch,
    if (meanLoss.isNotDefault) 'meanLoss': encodeDouble(meanLoss),
    if (computeTime != null) 'computeTime': computeTime!.toJson(),
  };

  @override
  String toString() {
    final contents = [
      'step=$step',
      'epoch=$epoch',
      'meanLoss=$meanLoss',
    ].join(',');
    return 'TuningSnapshot($contents)';
  }
}

/// Type contains the list of OpenAPI data types as defined by
/// https://spec.openapis.org/oas/v3.0.3#data-types
final class Type extends ProtoEnum {
  /// Not specified, should not be used.
  static const typeUnspecified = Type('TYPE_UNSPECIFIED');

  /// String type.
  static const string = Type('STRING');

  /// Number type.
  static const number = Type('NUMBER');

  /// Integer type.
  static const integer = Type('INTEGER');

  /// Boolean type.
  static const boolean = Type('BOOLEAN');

  /// Array type.
  static const array = Type('ARRAY');

  /// Object type.
  static const object = Type('OBJECT');

  /// Null type.
  static const null$ = Type('NULL');

  /// The default value for [Type].
  static const $default = typeUnspecified;

  const Type(super.value);

  factory Type.fromJson(Object? json) => Type(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'Type.$value';
}

/// Content Part modality
final class Modality extends ProtoEnum {
  /// Unspecified modality.
  static const modalityUnspecified = Modality('MODALITY_UNSPECIFIED');

  /// Plain text.
  static const text = Modality('TEXT');

  /// Image.
  static const image = Modality('IMAGE');

  /// Video.
  static const video = Modality('VIDEO');

  /// Audio.
  static const audio = Modality('AUDIO');

  /// Document, e.g. PDF.
  static const document = Modality('DOCUMENT');

  /// The default value for [Modality].
  static const $default = modalityUnspecified;

  const Modality(super.value);

  factory Modality.fromJson(Object? json) => Modality(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'Modality.$value';
}

/// Type of task for which the embedding will be used.
final class TaskType extends ProtoEnum {
  /// Unset value, which will default to one of the other enum values.
  static const taskTypeUnspecified = TaskType('TASK_TYPE_UNSPECIFIED');

  /// Specifies the given text is a query in a search/retrieval setting.
  static const retrievalQuery = TaskType('RETRIEVAL_QUERY');

  /// Specifies the given text is a document from the corpus being searched.
  static const retrievalDocument = TaskType('RETRIEVAL_DOCUMENT');

  /// Specifies the given text will be used for STS.
  static const semanticSimilarity = TaskType('SEMANTIC_SIMILARITY');

  /// Specifies that the given text will be classified.
  static const classification = TaskType('CLASSIFICATION');

  /// Specifies that the embeddings will be used for clustering.
  static const clustering = TaskType('CLUSTERING');

  /// Specifies that the given text will be used for question answering.
  static const questionAnswering = TaskType('QUESTION_ANSWERING');

  /// Specifies that the given text will be used for fact verification.
  static const factVerification = TaskType('FACT_VERIFICATION');

  /// Specifies that the given text will be used for code retrieval.
  static const codeRetrievalQuery = TaskType('CODE_RETRIEVAL_QUERY');

  /// The default value for [TaskType].
  static const $default = taskTypeUnspecified;

  const TaskType(super.value);

  factory TaskType.fromJson(Object? json) => TaskType(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'TaskType.$value';
}

/// The category of a rating.
///
/// These categories cover various kinds of harms that developers
/// may wish to adjust.
final class HarmCategory extends ProtoEnum {
  /// Category is unspecified.
  static const harmCategoryUnspecified = HarmCategory(
    'HARM_CATEGORY_UNSPECIFIED',
  );

  /// **PaLM** - Negative or harmful comments targeting identity and/or protected
  /// attribute.
  static const harmCategoryDerogatory = HarmCategory(
    'HARM_CATEGORY_DEROGATORY',
  );

  /// **PaLM** - Content that is rude, disrespectful, or profane.
  static const harmCategoryToxicity = HarmCategory('HARM_CATEGORY_TOXICITY');

  /// **PaLM** - Describes scenarios depicting violence against an individual or
  /// group, or general descriptions of gore.
  static const harmCategoryViolence = HarmCategory('HARM_CATEGORY_VIOLENCE');

  /// **PaLM** - Contains references to sexual acts or other lewd content.
  static const harmCategorySexual = HarmCategory('HARM_CATEGORY_SEXUAL');

  /// **PaLM** - Promotes unchecked medical advice.
  static const harmCategoryMedical = HarmCategory('HARM_CATEGORY_MEDICAL');

  /// **PaLM** - Dangerous content that promotes, facilitates, or encourages
  /// harmful acts.
  static const harmCategoryDangerous = HarmCategory('HARM_CATEGORY_DANGEROUS');

  /// **Gemini** - Harassment content.
  static const harmCategoryHarassment = HarmCategory(
    'HARM_CATEGORY_HARASSMENT',
  );

  /// **Gemini** - Hate speech and content.
  static const harmCategoryHateSpeech = HarmCategory(
    'HARM_CATEGORY_HATE_SPEECH',
  );

  /// **Gemini** - Sexually explicit content.
  static const harmCategorySexuallyExplicit = HarmCategory(
    'HARM_CATEGORY_SEXUALLY_EXPLICIT',
  );

  /// **Gemini** - Dangerous content.
  static const harmCategoryDangerousContent = HarmCategory(
    'HARM_CATEGORY_DANGEROUS_CONTENT',
  );

  /// **Gemini** - Content that may be used to harm civic integrity.
  /// DEPRECATED: use enable_enhanced_civic_answers instead.
  static const harmCategoryCivicIntegrity = HarmCategory(
    'HARM_CATEGORY_CIVIC_INTEGRITY',
  );

  /// The default value for [HarmCategory].
  static const $default = harmCategoryUnspecified;

  const HarmCategory(super.value);

  factory HarmCategory.fromJson(Object? json) => HarmCategory(json as String);

  bool get isNotDefault => this != $default;

  @override
  String toString() => 'HarmCategory.$value';
}
